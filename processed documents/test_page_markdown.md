First page structure: {'pageNumber': 1, 'angle': 0, 'width': 8.2639, 'height': 11.6806, 'unit': 'inch', 'words': [{'content': 'Dense', 'polygon': [1.3549, 1.1198, 1.8739, 1.1261, 1.8702, 1.3297, 1.3503, 1.3244], 'confidence': 0.993, 'span': {'offset': 0, 'length': 5}}, {'content': 'XX', 'polygon': [1.9305, 1.1268, 2.1334, 1.1292, 2.1302, 1.3324, 1.9269, 1.3303], 'confidence': 0.239, 'span': {'offset': 6, 'length': 2}}, {'content': 'Retrieval:', 'polygon': [2.1867, 1.1297, 3.0152, 1.1373, 3.0134, 1.3393, 2.1835, 1.3328], 'confidence': 0.971, 'span': {'offset': 9, 'length': 10}}, {'content': 'What', 'polygon': [3.0917, 1.1378, 3.5675, 1.1409, 3.5666, 1.3426, 3.09, 1.3398], 'confidence': 0.993, 'span': {'offset': 20, 'length': 4}}, {'content': 'Retrieval', 'polygon': [3.6208, 1.1413, 4.3894, 1.1445, 4.3899, 1.3462, 3.6199, 1.343], 'confidence': 0.993, 'span': {'offset': 25, 'length': 9}}, {'content': 'Granularity', 'polygon': [4.4493, 1.1447, 5.4709, 1.1456, 5.4731, 1.3481, 4.4498, 1.3464], 'confidence': 0.993, 'span': {'offset': 35, 'length': 11}}, {'content': 'Should', 'polygon': [5.5274, 1.1455, 6.1164, 1.1445, 6.1197, 1.3479, 5.5297, 1.3481], 'confidence': 0.996, 'span': {'offset': 47, 'length': 6}}, {'content': 'We', 'polygon': [6.173, 1.1442, 6.4425, 1.143, 6.4463, 1.3472, 6.1763, 1.3478], 'confidence': 0.996, 'span': {'offset': 54, 'length': 2}}, {'content': 'Use?', 'polygon': [6.4957, 1.1428, 6.9153, 1.1409, 6.9153, 1.346, 6.4996, 1.347], 'confidence': 0.993, 'span': {'offset': 57, 'length': 4}}, {'content': 'Tong', 'polygon': [2.0483, 1.6598, 2.4219, 1.6623, 2.4182, 1.85, 2.0438, 1.8508], 'confidence': 0.991, 'span': {'offset': 78, 'length': 4}}, {'content': 'Chen', 'polygon': [2.4671, 1.6627, 2.8075, 1.6652, 2.8046, 1.8499, 2.4635, 1.8499], 'confidence': 0.993, 'span': {'offset': 83, 'length': 4}}, {'content': '\\*\\*', 'polygon': [2.8437, 1.6655, 2.9913, 1.6666, 2.9888, 1.8501, 2.8409, 1.8499], 'confidence': 0.6, 'span': {'offset': 88, 'length': 4}}, {'content': 'Hongwei', 'polygon': [3.0486, 1.6671, 3.6722, 1.6724, 3.6712, 1.8527, 3.0462, 1.8502], 'confidence': 0.991, 'span': {'offset': 93, 'length': 7}}, {'content': 'Wang', 'polygon': [3.7174, 1.6728, 4.1522, 1.6769, 4.1522, 1.8555, 3.7165, 1.853], 'confidence': 0.993, 'span': {'offset': 101, 'length': 4}}, {'content': 'Sihao', 'polygon': [4.3134, 1.6624, 4.7331, 1.6781, 4.7313, 1.8332, 4.3102, 1.8386], 'confidence': 0.995, 'span': {'offset': 106, 'length': 5}}, {'content': 'Chen', 'polygon': [4.784, 1.6791, 5.2473, 1.6696, 5.2473, 1.8085, 4.7824, 1.8318], 'confidence': 0.968, 'span': {'offset': 112, 'length': 4}}, {'content': 'Wenhao', 'polygon': [5.3594, 1.6723, 5.9417, 1.6745, 5.9417, 1.8279, 5.3656, 1.8264], 'confidence': 0.99, 'span': {'offset': 117, 'length': 6}}, {'content': 'Yu"', 'polygon': [5.9843, 1.6735, 6.2613, 1.6633, 6.2613, 1.8121, 5.984, 1.8264], 'confidence': 0.255, 'span': {'offset': 124, 'length': 3}}, {'content': 'Kaixin', 'polygon': [1.9765, 1.8583, 2.4763, 1.8635, 2.4718, 2.038, 1.972, 2.0353], 'confidence': 0.993, 'span': {'offset': 128, 'length': 6}}, {'content': 'Ma', 'polygon': [2.5194, 1.8639, 2.7922, 1.8663, 2.7877, 2.038, 2.5148, 2.038], 'confidence': 0.996, 'span': {'offset': 135, 'length': 2}}, {'content': 'Xinran', 'polygon': [2.963, 1.8626, 3.4718, 1.8645, 3.4728, 2.0423, 2.9632, 2.0396], 'confidence': 0.995, 'span': {'offset': 151, 'length': 6}}, {'content': 'Zhao^', 'polygon': [3.5156, 1.8647, 3.9688, 1.8664, 3.9708, 2.0448, 3.5168, 2.0425], 'confidence': 0.615, 'span': {'offset': 158, 'length': 5}}, {'content': 'Hongming', 'polygon': [4.0916, 1.8669, 4.8372, 1.8699, 4.8407, 2.049, 4.0938, 2.0454], 'confidence': 0.995, 'span': {'offset': 164, 'length': 8}}, {'content': 'Zhang', 'polygon': [4.8781, 1.87, 5.3956, 1.8721, 5.3994, 2.0515, 4.8817, 2.0492], 'confidence': 0.993, 'span': {'offset': 173, 'length': 5}}, {'content': 'Dong', 'polygon': [5.5274, 1.8674, 5.9256, 1.8699, 5.9256, 2.041, 5.5274, 2.0411], 'confidence': 0.993, 'span': {'offset': 192, 'length': 4}}, {'content': 'Yu', 'polygon': [5.9705, 1.8701, 6.2453, 1.8696, 6.2453, 2.037, 5.9705, 2.0408], 'confidence': 0.762, 'span': {'offset': 197, 'length': 2}}, {'content': '\\*University', 'polygon': [2.6206, 2.1175, 3.4189, 2.1265, 3.4157, 2.3081, 2.6174, 2.2916], 'confidence': 0.907, 'span': {'offset': 201, 'length': 12}}, {'content': 'of', 'polygon': [3.4623, 2.127, 3.604, 2.1287, 3.6008, 2.3095, 3.459, 2.3084], 'confidence': 0.998, 'span': {'offset': 214, 'length': 2}}, {'content': 'Washington', 'polygon': [3.6387, 2.1291, 4.4513, 2.139, 4.4513, 2.305, 3.6355, 2.3097], 'confidence': 0.993, 'span': {'offset': 217, 'length': 10}}, {'content': 'Tencent', 'polygon': [4.5468, 2.1199, 5.1607, 2.1224, 5.1603, 2.2985, 4.5513, 2.3067], 'confidence': 0.993, 'span': {'offset': 229, 'length': 7}}, {'content': 'AI', 'polygon': [5.2018, 2.1225, 5.3751, 2.1233, 5.373, 2.298, 5.2011, 2.2982], 'confidence': 0.97, 'span': {'offset': 237, 'length': 2}}, {'content': 'Lab', 'polygon': [5.4192, 2.1235, 5.6833, 2.1247, 5.6819, 2.2999, 5.4167, 2.298], 'confidence': 0.999, 'span': {'offset': 240, 'length': 3}}, {'content': 'University', 'polygon': [2.1259, 2.3211, 2.9289, 2.3238, 2.9231, 2.5024, 2.1194, 2.5018], 'confidence': 0.957, 'span': {'offset': 258, 'length': 10}}, {'content': 'of', 'polygon': [2.9755, 2.3239, 3.1035, 2.3243, 3.0978, 2.5026, 2.9697, 2.5025], 'confidence': 0.998, 'span': {'offset': 269, 'length': 2}}, {'content': 'Pennsylvania', 'polygon': [3.1384, 2.3244, 4.0404, 2.3271, 4.0355, 2.5039, 3.1328, 2.5026], 'confidence': 0.993, 'span': {'offset': 272, 'length': 12}}, {'content': '^Carnegie', 'polygon': [4.148, 2.3274, 4.8405, 2.3292, 4.8362, 2.5054, 4.1432, 2.5041], 'confidence': 0.865, 'span': {'offset': 285, 'length': 9}}, {'content': 'Mellon', 'polygon': [4.8812, 2.3294, 5.3613, 2.3305, 5.3575, 2.5066, 4.877, 2.5055], 'confidence': 0.994, 'span': {'offset': 295, 'length': 6}}, {'content': 'University', 'polygon': [5.3992, 2.3306, 6.0991, 2.3322, 6.0991, 2.5085, 5.3953, 2.5067], 'confidence': 0.995, 'span': {'offset': 302, 'length': 10}}, {'content': 'Abstract', 'polygon': [2.1814, 3.0734, 2.8138, 3.0776, 2.8138, 3.2232, 2.1782, 3.2236], 'confidence': 0.995, 'span': {'offset': 328, 'length': 8}}, {'content': 'Dense', 'polygon': [1.1985, 3.4136, 1.5706, 3.4157, 1.5678, 3.5547, 1.1952, 3.5548], 'confidence': 0.997, 'span': {'offset': 338, 'length': 5}}, {'content': 'retrieval', 'polygon': [1.6659, 3.4162, 2.1288, 3.4181, 2.1267, 3.5551, 1.6633, 3.5547], 'confidence': 0.995, 'span': {'offset': 344, 'length': 9}}, {'content': 'has', 'polygon': [2.2218, 3.4184, 2.4102, 3.4189, 2.4084, 3.5556, 2.2199, 3.5552], 'confidence': 0.999, 'span': {'offset': 354, 'length': 3}}, {'content': 'become', 'polygon': [2.5032, 3.4191, 2.9434, 3.4199, 2.9423, 3.5569, 2.5016, 3.5558], 'confidence': 0.998, 'span': {'offset': 358, 'length': 6}}, {'content': 'a', 'polygon': [3.041, 3.42, 3.1022, 3.42, 3.1014, 3.5575, 3.04, 3.5572], 'confidence': 0.996, 'span': {'offset': 365, 'length': 1}}, {'content': 'prominent', 'polygon': [3.1975, 3.42, 3.7923, 3.4197, 3.7923, 3.5589, 3.1968, 3.5578], 'confidence': 0.995, 'span': {'offset': 367, 'length': 9}}, {'content': 'method', 'polygon': [1.2017, 3.5807, 1.6413, 3.5838, 1.6413, 3.7262, 1.2017, 3.7262], 'confidence': 0.996, 'span': {'offset': 377, 'length': 6}}, {'content': 'to', 'polygon': [1.701, 3.5842, 1.8092, 3.5848, 1.8092, 3.7262, 1.701, 3.7262], 'confidence': 0.997, 'span': {'offset': 384, 'length': 2}}, {'content': 'obtain', 'polygon': [1.8733, 3.5852, 2.2245, 3.5872, 2.2245, 3.7249, 1.8733, 3.7262], 'confidence': 0.996, 'span': {'offset': 387, 'length': 6}}, {'content': 'relevant', 'polygon': [2.2842, 3.5875, 2.7349, 3.5894, 2.7349, 3.7219, 2.2842, 3.7246], 'confidence': 0.996, 'span': {'offset': 394, 'length': 8}}, {'content': 'context', 'polygon': [2.7945, 3.5896, 3.2099, 3.5908, 3.2099, 3.7184, 2.7945, 3.7215], 'confidence': 0.995, 'span': {'offset': 403, 'length': 7}}, {'content': 'or', 'polygon': [3.2695, 3.591, 3.391, 3.5913, 3.391, 3.7169, 3.2695, 3.7179], 'confidence': 0.996, 'span': {'offset': 411, 'length': 2}}, {'content': 'world', 'polygon': [3.4507, 3.5914, 3.7872, 3.5918, 3.7872, 3.7131, 3.4507, 3.7163], 'confidence': 0.996, 'span': {'offset': 414, 'length': 5}}, {'content': 'knowledge', 'polygon': [1.2017, 3.752, 1.8306, 3.7534, 1.8274, 3.9018, 1.1984, 3.9017], 'confidence': 0.996, 'span': {'offset': 420, 'length': 9}}, {'content': 'in', 'polygon': [1.8683, 3.7534, 1.9743, 3.7535, 1.971, 3.9014, 1.865, 3.9017], 'confidence': 0.998, 'span': {'offset': 430, 'length': 2}}, {'content': 'open-domain', 'polygon': [2.012, 3.7535, 2.7492, 3.7528, 2.746, 3.8962, 2.0087, 3.9012], 'confidence': 0.993, 'span': {'offset': 433, 'length': 11}}, {'content': 'NLP', 'polygon': [2.7822, 3.7527, 3.0484, 3.752, 3.0452, 3.893, 2.779, 3.8959], 'confidence': 0.997, 'span': {'offset': 445, 'length': 3}}, {'content': 'tasks.', 'polygon': [3.0861, 3.7519, 3.3947, 3.7506, 3.3914, 3.8883, 3.0828, 3.8925], 'confidence': 0.994, 'span': {'offset': 449, 'length': 6}}, {'content': 'When', 'polygon': [3.4394, 3.7504, 3.7872, 3.7483, 3.7872, 3.8817, 3.4362, 3.8876], 'confidence': 0.993, 'span': {'offset': 456, 'length': 4}}, {'content': 'we', 'polygon': [1.2049, 3.9218, 1.3753, 3.9208, 1.3751, 4.0545, 1.2049, 4.0532], 'confidence': 0.996, 'span': {'offset': 461, 'length': 2}}, {'content': 'use', 'polygon': [1.4122, 3.9206, 1.5965, 3.9195, 1.596, 4.0561, 1.4119, 4.0548], 'confidence': 0.999, 'span': {'offset': 464, 'length': 3}}, {'content': 'a', 'polygon': [1.6356, 3.9193, 1.684, 3.9191, 1.6834, 4.0566, 1.6351, 4.0563], 'confidence': 0.996, 'span': {'offset': 468, 'length': 1}}, {'content': 'learned', 'polygon': [1.7117, 3.9189, 2.147, 3.9168, 2.1458, 4.0584, 1.711, 4.0567], 'confidence': 0.995, 'span': {'offset': 470, 'length': 7}}, {'content': 'dense', 'polygon': [2.1816, 3.9166, 2.4995, 3.9153, 2.4979, 4.0589, 2.1804, 4.0585], 'confidence': 0.996, 'span': {'offset': 478, 'length': 5}}, {'content': 'retriever', 'polygon': [2.534, 3.9152, 3.0109, 3.9135, 3.0086, 4.058, 2.5324, 4.0589], 'confidence': 0.995, 'span': {'offset': 484, 'length': 9}}, {'content': 'on', 'polygon': [3.0431, 3.9134, 3.1814, 3.9131, 3.1789, 4.0572, 3.0408, 4.0579], 'confidence': 0.997, 'span': {'offset': 494, 'length': 2}}, {'content': 'a', 'polygon': [3.2205, 3.913, 3.2804, 3.9128, 3.2778, 4.0567, 3.218, 4.057], 'confidence': 0.996, 'span': {'offset': 497, 'length': 1}}, {'content': 'retrieval', 'polygon': [3.315, 3.9127, 3.7923, 3.9117, 3.7909, 4.0531, 3.3123, 4.0566], 'confidence': 0.993, 'span': {'offset': 499, 'length': 9}}, {'content': 'corpus', 'polygon': [1.2017, 4.087, 1.5917, 4.0857, 1.5912, 4.2325, 1.2017, 4.2332], 'confidence': 0.995, 'span': {'offset': 509, 'length': 6}}, {'content': 'at', 'polygon': [1.6321, 4.0856, 1.7344, 4.0853, 1.7337, 4.2319, 1.6316, 4.2323], 'confidence': 0.995, 'span': {'offset': 516, 'length': 2}}, {'content': 'inference', 'polygon': [1.7725, 4.0852, 2.2838, 4.0837, 2.2825, 4.2296, 1.7718, 4.2318], 'confidence': 0.993, 'span': {'offset': 519, 'length': 9}}, {'content': 'time,', 'polygon': [2.3219, 4.0836, 2.6025, 4.0829, 2.6008, 4.2282, 2.3205, 4.2295], 'confidence': 0.993, 'span': {'offset': 529, 'length': 5}}, {'content': 'an', 'polygon': [2.6453, 4.0827, 2.7738, 4.0824, 2.7718, 4.2274, 2.6435, 4.228], 'confidence': 0.997, 'span': {'offset': 535, 'length': 2}}, {'content': 'often-overlooked', 'polygon': [2.8118, 4.0823, 3.7923, 4.0801, 3.7923, 4.2221, 2.8098, 4.2272], 'confidence': 0.989, 'span': {'offset': 538, 'length': 16}}, {'content': 'design', 'polygon': [1.1985, 4.2477, 1.5764, 4.2473, 1.5764, 4.3967, 1.1985, 4.3996], 'confidence': 0.995, 'span': {'offset': 555, 'length': 6}}, {'content': 'choice', 'polygon': [1.6125, 4.2472, 1.9591, 4.2468, 1.9591, 4.3942, 1.6125, 4.3964], 'confidence': 0.996, 'span': {'offset': 562, 'length': 6}}, {'content': 'is', 'polygon': [1.988, 4.2468, 2.0987, 4.2467, 2.0987, 4.3935, 1.988, 4.3941], 'confidence': 0.993, 'span': {'offset': 569, 'length': 2}}, {'content': 'the', 'polygon': [2.1348, 4.2466, 2.3009, 4.2464, 2.3009, 4.3925, 2.1348, 4.3933], 'confidence': 0.999, 'span': {'offset': 572, 'length': 3}}, {'content': 'retrieval', 'polygon': [2.3418, 4.2463, 2.8087, 4.2457, 2.8087, 4.3906, 2.3418, 4.3923], 'confidence': 0.994, 'span': {'offset': 576, 'length': 9}}, {'content': 'unit', 'polygon': [2.8424, 4.2456, 3.059, 4.2453, 3.059, 4.39, 2.8424, 4.3906], 'confidence': 0.993, 'span': {'offset': 586, 'length': 4}}, {'content': 'in', 'polygon': [3.0951, 4.2453, 3.1986, 4.2451, 3.1986, 4.3898, 3.0951, 4.39], 'confidence': 0.993, 'span': {'offset': 591, 'length': 2}}, {'content': 'which', 'polygon': [3.2371, 4.245, 3.5717, 4.2445, 3.5717, 4.3896, 3.2371, 4.3898], 'confidence': 0.997, 'span': {'offset': 594, 'length': 5}}, {'content': 'the', 'polygon': [3.6078, 4.2444, 3.7923, 4.2441, 3.7923, 4.3896, 3.6078, 4.3896], 'confidence': 0.999, 'span': {'offset': 600, 'length': 3}}, {'content': 'corpus', 'polygon': [1.2017, 4.4211, 1.5904, 4.4149, 1.5899, 4.5624, 1.2017, 4.5627], 'confidence': 0.994, 'span': {'offset': 604, 'length': 6}}, {'content': 'is', 'polygon': [1.6312, 4.4145, 1.72, 4.4135, 1.7193, 4.5619, 1.6307, 4.5622], 'confidence': 0.996, 'span': {'offset': 611, 'length': 2}}, {'content': 'indexed,', 'polygon': [1.7608, 4.4131, 2.2336, 4.4095, 2.2323, 4.5603, 1.7601, 4.5618], 'confidence': 0.992, 'span': {'offset': 614, 'length': 8}}, {'content': 'e.g.', 'polygon': [2.2744, 4.4093, 2.4712, 4.4091, 2.4696, 4.5597, 2.273, 4.5602], 'confidence': 0.989, 'span': {'offset': 623, 'length': 4}}, {'content': 'document,', 'polygon': [2.524, 4.4091, 3.1119, 4.4115, 3.1096, 4.5588, 2.5223, 4.5596], 'confidence': 0.993, 'span': {'offset': 628, 'length': 9}}, {'content': 'passage,', 'polygon': [3.1527, 4.4119, 3.6231, 4.4179, 3.6201, 4.5587, 3.1503, 4.5588], 'confidence': 0.993, 'span': {'offset': 638, 'length': 8}}, {'content': 'or', 'polygon': [3.6639, 4.4186, 3.8024, 4.4207, 3.7998, 4.5588, 3.6608, 4.5588], 'confidence': 0.995, 'span': {'offset': 647, 'length': 2}}, {'content': 'sentence.', 'polygon': [1.2017, 4.5812, 1.7335, 4.5762, 1.7328, 4.7199, 1.2017, 4.7199], 'confidence': 0.995, 'span': {'offset': 650, 'length': 9}}, {'content': 'We', 'polygon': [1.8154, 4.5756, 2.0029, 4.5743, 2.0019, 4.7192, 1.8147, 4.7197], 'confidence': 0.997, 'span': {'offset': 660, 'length': 2}}, {'content': 'discover', 'polygon': [2.0497, 4.5741, 2.5253, 4.5723, 2.5236, 4.7176, 2.0487, 4.719], 'confidence': 0.996, 'span': {'offset': 663, 'length': 8}}, {'content': 'that', 'polygon': [2.5698, 4.5722, 2.783, 4.572, 2.781, 4.7169, 2.5681, 4.7175], 'confidence': 0.993, 'span': {'offset': 672, 'length': 4}}, {'content': 'the', 'polygon': [2.8299, 4.572, 3.0009, 4.5721, 2.9986, 4.7163, 2.8278, 4.7167], 'confidence': 0.995, 'span': {'offset': 677, 'length': 3}}, {'content': 'retrieval', 'polygon': [3.0454, 4.5721, 3.5093, 4.5738, 3.5064, 4.7148, 3.0431, 4.7161], 'confidence': 0.994, 'span': {'offset': 681, 'length': 9}}, {'content': 'unit', 'polygon': [3.5584, 4.5741, 3.7923, 4.5754, 3.7923, 4.7141, 3.5555, 4.7147], 'confidence': 0.993, 'span': {'offset': 691, 'length': 4}}, {'content': 'choice', 'polygon': [1.2017, 4.7427, 1.586, 4.7432, 1.5828, 4.8972, 1.1984, 4.8965], 'confidence': 0.995, 'span': {'offset': 696, 'length': 6}}, {'content': 'significantly', 'polygon': [1.6337, 4.7432, 2.3246, 4.7436, 2.3213, 4.8973, 1.6305, 4.8972], 'confidence': 0.993, 'span': {'offset': 703, 'length': 13}}, {'content': 'impacts', 'polygon': [2.3723, 4.7436, 2.8069, 4.7434, 2.8036, 4.8968, 2.3691, 4.8973], 'confidence': 0.994, 'span': {'offset': 717, 'length': 7}}, {'content': 'the', 'polygon': [2.8496, 4.7434, 3.0204, 4.7433, 3.0172, 4.8963, 2.8464, 4.8967], 'confidence': 0.999, 'span': {'offset': 725, 'length': 3}}, {'content': 'performance', 'polygon': [3.0631, 4.7432, 3.7923, 4.7421, 3.7909, 4.8935, 3.0599, 4.8962], 'confidence': 0.993, 'span': {'offset': 729, 'length': 11}}, {'content': 'of', 'polygon': [1.2049, 4.9103, 1.345, 4.9101, 1.3448, 5.0567, 1.2049, 5.0572], 'confidence': 0.99, 'span': {'offset': 741, 'length': 2}}, {'content': 'both', 'polygon': [1.3806, 4.91, 1.63, 4.9098, 1.6295, 5.0558, 1.3804, 5.0566], 'confidence': 0.993, 'span': {'offset': 744, 'length': 4}}, {'content': 'retrieval', 'polygon': [1.6752, 4.9097, 2.1384, 4.9097, 2.1372, 5.0548, 1.6746, 5.0557], 'confidence': 0.995, 'span': {'offset': 749, 'length': 9}}, {'content': 'and', 'polygon': [2.1883, 4.9097, 2.3878, 4.9099, 2.3863, 5.0546, 2.187, 5.0547], 'confidence': 0.996, 'span': {'offset': 759, 'length': 3}}, {'content': 'downstream', 'polygon': [2.4329, 4.9099, 3.1218, 4.9112, 3.1194, 5.0552, 2.4314, 5.0546], 'confidence': 0.996, 'span': {'offset': 763, 'length': 10}}, {'content': 'tasks.', 'polygon': [3.1669, 4.9114, 3.4757, 4.9123, 3.4728, 5.0561, 3.1644, 5.0553], 'confidence': 0.997, 'span': {'offset': 774, 'length': 6}}, {'content': 'Dis-', 'polygon': [3.5565, 4.9126, 3.8024, 4.9136, 3.8024, 5.0574, 3.5535, 5.0564], 'confidence': 0.993, 'span': {'offset': 781, 'length': 4}}, {'content': 'tinct', 'polygon': [1.2017, 5.0779, 1.4715, 5.0761, 1.4686, 5.2215, 1.1984, 5.2194], 'confidence': 0.996, 'span': {'offset': 786, 'length': 5}}, {'content': 'from', 'polygon': [1.5181, 5.0758, 1.7904, 5.0745, 1.7879, 5.2237, 1.5153, 5.2219], 'confidence': 0.993, 'span': {'offset': 792, 'length': 4}}, {'content': 'the', 'polygon': [1.837, 5.0743, 2.0087, 5.0738, 2.0065, 5.225, 1.8346, 5.224], 'confidence': 0.999, 'span': {'offset': 797, 'length': 3}}, {'content': 'typical', 'polygon': [2.0553, 5.0737, 2.438, 5.0734, 2.4363, 5.2267, 2.0532, 5.2252], 'confidence': 0.996, 'span': {'offset': 801, 'length': 7}}, {'content': 'approach', 'polygon': [2.4871, 5.0734, 3.0022, 5.0746, 3.0012, 5.2268, 2.4854, 5.2268], 'confidence': 0.997, 'span': {'offset': 809, 'length': 8}}, {'content': 'of', 'polygon': [3.0488, 5.0748, 3.1739, 5.0755, 3.1732, 5.2268, 3.0479, 5.2268], 'confidence': 0.997, 'span': {'offset': 818, 'length': 2}}, {'content': 'using', 'polygon': [3.2132, 5.0757, 3.5149, 5.0776, 3.5146, 5.2268, 3.2125, 5.2268], 'confidence': 0.996, 'span': {'offset': 821, 'length': 5}}, {'content': 'pas-', 'polygon': [3.5615, 5.078, 3.8075, 5.08, 3.8075, 5.2268, 3.5612, 5.2268], 'confidence': 0.993, 'span': {'offset': 827, 'length': 4}}, {'content': 'sages', 'polygon': [1.2017, 5.2602, 1.5206, 5.2536, 1.5206, 5.3886, 1.2017, 5.3915], 'confidence': 0.996, 'span': {'offset': 832, 'length': 5}}, {'content': 'or', 'polygon': [1.5745, 5.2525, 1.6935, 5.2507, 1.6935, 5.3872, 1.5745, 5.3881], 'confidence': 0.997, 'span': {'offset': 838, 'length': 2}}, {'content': 'sentences,', 'polygon': [1.7452, 5.2499, 2.3134, 5.2427, 2.3134, 5.3828, 1.7452, 5.3868], 'confidence': 0.994, 'span': {'offset': 841, 'length': 10}}, {'content': 'we', 'polygon': [2.3741, 5.2423, 2.5336, 5.2413, 2.5336, 5.3817, 2.3741, 5.3825], 'confidence': 0.996, 'span': {'offset': 852, 'length': 2}}, {'content': 'introduce', 'polygon': [2.5875, 5.241, 3.1176, 5.2401, 3.1176, 5.3792, 2.5875, 5.3814], 'confidence': 0.995, 'span': {'offset': 855, 'length': 9}}, {'content': 'a', 'polygon': [3.1715, 5.2403, 3.2344, 5.2405, 3.2344, 5.3789, 3.1715, 5.3791], 'confidence': 0.997, 'span': {'offset': 865, 'length': 1}}, {'content': 'novel', 'polygon': [3.2838, 5.2406, 3.5893, 5.2424, 3.5893, 5.3781, 3.2838, 5.3788], 'confidence': 0.995, 'span': {'offset': 867, 'length': 5}}, {'content': 're-', 'polygon': [3.641, 5.2428, 3.8024, 5.2441, 3.8024, 5.3778, 3.641, 5.378], 'confidence': 0.995, 'span': {'offset': 873, 'length': 3}}, {'content': 'trieval', 'polygon': [1.2049, 5.4092, 1.5702, 5.4084, 1.5674, 5.5564, 1.2016, 5.5512], 'confidence': 0.995, 'span': {'offset': 877, 'length': 7}}, {'content': 'unit,', 'polygon': [1.6331, 5.4083, 1.8823, 5.4078, 1.8799, 5.5591, 1.6304, 5.557], 'confidence': 0.995, 'span': {'offset': 885, 'length': 5}}, {'content': 'proposition,', 'polygon': [1.9403, 5.4077, 2.6274, 5.4066, 2.626, 5.5597, 1.938, 5.5596], 'confidence': 0.993, 'span': {'offset': 891, 'length': 12}}, {'content': 'for', 'polygon': [2.6952, 5.4066, 2.8645, 5.4064, 2.8634, 5.5579, 2.6938, 5.5594], 'confidence': 0.999, 'span': {'offset': 904, 'length': 3}}, {'content': 'dense', 'polygon': [2.9226, 5.4063, 3.2395, 5.406, 3.2388, 5.5535, 2.9215, 5.5574], 'confidence': 0.995, 'span': {'offset': 908, 'length': 5}}, {'content': 'retrieval.', 'polygon': [3.3, 5.406, 3.8024, 5.4055, 3.8024, 5.5429, 3.2994, 5.5526], 'confidence': 0.993, 'span': {'offset': 914, 'length': 10}}, {'content': 'Propositions', 'polygon': [1.2017, 5.5705, 1.9016, 5.5689, 1.9016, 5.7145, 1.2017, 5.7233], 'confidence': 0.993, 'span': {'offset': 925, 'length': 12}}, {'content': 'are', 'polygon': [1.9398, 5.5689, 2.1046, 5.569, 2.1046, 5.7133, 1.9398, 5.7141], 'confidence': 0.998, 'span': {'offset': 938, 'length': 3}}, {'content': 'defined', 'polygon': [2.1428, 5.569, 2.5442, 5.5696, 2.5442, 5.7123, 2.1428, 5.7131], 'confidence': 0.993, 'span': {'offset': 942, 'length': 7}}, {'content': 'as', 'polygon': [2.5824, 5.5697, 2.6923, 5.57, 2.6923, 5.7124, 2.5824, 5.7123], 'confidence': 0.996, 'span': {'offset': 950, 'length': 2}}, {'content': 'atomic', 'polygon': [2.7305, 5.5702, 3.0983, 5.5718, 3.0983, 5.7147, 2.7305, 5.7126], 'confidence': 0.995, 'span': {'offset': 953, 'length': 6}}, {'content': 'expressions', 'polygon': [3.1366, 5.572, 3.7872, 5.5765, 3.7872, 5.7234, 3.1366, 5.7151], 'confidence': 0.983, 'span': {'offset': 960, 'length': 11}}, {'content': 'within', 'polygon': [1.208, 5.7377, 1.5715, 5.7384, 1.5711, 5.8859, 1.208, 5.8839], 'confidence': 0.995, 'span': {'offset': 972, 'length': 6}}, {'content': 'text,', 'polygon': [1.6152, 5.7385, 1.8551, 5.7388, 1.8543, 5.8859, 1.6146, 5.8859], 'confidence': 0.993, 'span': {'offset': 979, 'length': 5}}, {'content': 'each', 'polygon': [1.9035, 5.7388, 2.1604, 5.7389, 2.1592, 5.8859, 1.9027, 5.8859], 'confidence': 0.993, 'span': {'offset': 985, 'length': 4}}, {'content': 'encapsulating', 'polygon': [2.204, 5.739, 2.977, 5.7383, 2.9748, 5.8859, 2.2028, 5.8859], 'confidence': 0.995, 'span': {'offset': 990, 'length': 13}}, {'content': 'a', 'polygon': [3.0255, 5.7382, 3.0861, 5.7381, 3.0837, 5.8859, 3.0232, 5.8859], 'confidence': 0.996, 'span': {'offset': 1004, 'length': 1}}, {'content': 'distinct', 'polygon': [3.1249, 5.738, 3.5441, 5.7369, 3.5412, 5.8844, 3.1225, 5.8859], 'confidence': 0.994, 'span': {'offset': 1006, 'length': 8}}, {'content': 'fac-', 'polygon': [3.5829, 5.7367, 3.8125, 5.7359, 3.8122, 5.8827, 3.5799, 5.8841], 'confidence': 0.993, 'span': {'offset': 1015, 'length': 4}}, {'content': 'toid', 'polygon': [1.2049, 5.909, 1.4337, 5.9092, 1.4337, 6.0522, 1.2049, 6.0525], 'confidence': 0.993, 'span': {'offset': 1021, 'length': 4}}, {'content': 'and', 'polygon': [1.4695, 5.9092, 1.6531, 5.9092, 1.6531, 6.0519, 1.4695, 6.0521], 'confidence': 0.999, 'span': {'offset': 1026, 'length': 3}}, {'content': 'presented', 'polygon': [1.6817, 5.9091, 2.2324, 5.9081, 2.2324, 6.0512, 1.6817, 6.0519], 'confidence': 0.993, 'span': {'offset': 1030, 'length': 9}}, {'content': 'in', 'polygon': [2.2705, 5.908, 2.373, 5.9077, 2.373, 6.0511, 2.2705, 6.0512], 'confidence': 0.997, 'span': {'offset': 1040, 'length': 2}}, {'content': 'a', 'polygon': [2.4112, 5.9075, 2.4732, 5.9072, 2.4732, 6.051, 2.4112, 6.0511], 'confidence': 0.996, 'span': {'offset': 1043, 'length': 1}}, {'content': 'concise,', 'polygon': [2.5066, 5.9071, 2.95, 5.9048, 2.95, 6.0506, 2.5066, 6.051], 'confidence': 0.988, 'span': {'offset': 1045, 'length': 8}}, {'content': 'self-contained', 'polygon': [2.9929, 5.9045, 3.7923, 5.898, 3.7923, 6.05, 2.9929, 6.0505], 'confidence': 0.986, 'span': {'offset': 1054, 'length': 14}}, {'content': 'natural', 'polygon': [1.2017, 6.0768, 1.5956, 6.0751, 1.5951, 6.2178, 1.2017, 6.2178], 'confidence': 0.996, 'span': {'offset': 1069, 'length': 7}}, {'content': 'language', 'polygon': [1.631, 6.0749, 2.1216, 6.0735, 2.1204, 6.2179, 1.6304, 6.2178], 'confidence': 0.996, 'span': {'offset': 1077, 'length': 8}}, {'content': 'format.', 'polygon': [2.157, 6.0735, 2.5509, 6.0729, 2.5492, 6.218, 2.1558, 6.2179], 'confidence': 0.996, 'span': {'offset': 1086, 'length': 7}}, {'content': 'We', 'polygon': [2.5933, 6.0729, 2.775, 6.0728, 2.773, 6.218, 2.5916, 6.218], 'confidence': 0.994, 'span': {'offset': 1094, 'length': 2}}, {'content': 'conduct', 'polygon': [2.8103, 6.0728, 3.2444, 6.0732, 3.2418, 6.2181, 2.8083, 6.218], 'confidence': 0.997, 'span': {'offset': 1097, 'length': 7}}, {'content': 'an', 'polygon': [3.2797, 6.0732, 3.4048, 6.0734, 3.402, 6.2181, 3.2771, 6.2181], 'confidence': 0.996, 'span': {'offset': 1105, 'length': 2}}, {'content': 'empir-', 'polygon': [3.4401, 6.0735, 3.8075, 6.0745, 3.8075, 6.2182, 3.4373, 6.2181], 'confidence': 0.995, 'span': {'offset': 1108, 'length': 6}}, {'content': 'ical', 'polygon': [1.208, 6.2377, 1.4168, 6.2368, 1.4139, 6.3824, 1.2048, 6.3827], 'confidence': 0.993, 'span': {'offset': 1115, 'length': 4}}, {'content': 'comparison', 'polygon': [1.4548, 6.2366, 2.1049, 6.2348, 2.1028, 6.3814, 1.4519, 6.3824], 'confidence': 0.996, 'span': {'offset': 1120, 'length': 10}}, {'content': 'of', 'polygon': [2.1405, 6.2347, 2.252, 6.2346, 2.2501, 6.3811, 2.1384, 6.3813], 'confidence': 0.998, 'span': {'offset': 1131, 'length': 2}}, {'content': 'different', 'polygon': [2.2805, 6.2345, 2.7669, 6.2345, 2.7656, 6.3801, 2.2786, 6.3811], 'confidence': 0.996, 'span': {'offset': 1134, 'length': 9}}, {'content': 'retrieval', 'polygon': [2.8001, 6.2346, 3.2556, 6.2354, 3.2549, 6.3791, 2.7988, 6.3801], 'confidence': 0.996, 'span': {'offset': 1144, 'length': 9}}, {'content': 'granular-', 'polygon': [3.2936, 6.2355, 3.8024, 6.2374, 3.8024, 6.3778, 3.293, 6.379], 'confidence': 0.994, 'span': {'offset': 1154, 'length': 9}}, {'content': 'ity.', 'polygon': [1.2049, 6.4041, 1.3851, 6.4039, 1.3849, 6.5513, 1.2049, 6.5506], 'confidence': 0.993, 'span': {'offset': 1164, 'length': 4}}, {'content': 'Our', 'polygon': [1.4606, 6.4037, 1.6823, 6.4034, 1.6817, 6.5522, 1.4603, 6.5516], 'confidence': 0.998, 'span': {'offset': 1169, 'length': 3}}, {'content': 'results', 'polygon': [1.7188, 6.4034, 2.0842, 6.403, 2.0831, 6.5529, 1.7182, 6.5523], 'confidence': 0.994, 'span': {'offset': 1173, 'length': 7}}, {'content': 'reveal', 'polygon': [2.1305, 6.4029, 2.469, 6.4026, 2.4674, 6.5529, 2.1293, 6.5529], 'confidence': 0.995, 'span': {'offset': 1181, 'length': 6}}, {'content': 'that', 'polygon': [2.5129, 6.4025, 2.7248, 6.4023, 2.7229, 6.5525, 2.5112, 6.5528], 'confidence': 0.993, 'span': {'offset': 1188, 'length': 4}}, {'content': 'proposition-based', 'polygon': [2.7662, 6.4023, 3.7923, 6.4016, 3.7923, 6.5476, 2.7642, 6.5524], 'confidence': 0.988, 'span': {'offset': 1193, 'length': 17}}, {'content': 'retrieval', 'polygon': [1.2017, 6.5675, 1.678, 6.5699, 1.678, 6.7196, 1.2017, 6.7159], 'confidence': 0.994, 'span': {'offset': 1211, 'length': 9}}, {'content': 'significantly', 'polygon': [1.7271, 6.5701, 2.4196, 6.5713, 2.4196, 6.7219, 1.7271, 6.7199], 'confidence': 0.993, 'span': {'offset': 1221, 'length': 13}}, {'content': 'outperforms', 'polygon': [2.4662, 6.5713, 3.1488, 6.5697, 3.1488, 6.72, 2.4662, 6.7219], 'confidence': 0.993, 'span': {'offset': 1235, 'length': 11}}, {'content': 'traditional', 'polygon': [3.1979, 6.5695, 3.7821, 6.5659, 3.7821, 6.7149, 3.1979, 6.7197], 'confidence': 0.994, 'span': {'offset': 1247, 'length': 11}}, {'content': 'passage', 'polygon': [1.2017, 6.7547, 1.6541, 6.749, 1.6541, 6.8849, 1.2017, 6.8876], 'confidence': 0.996, 'span': {'offset': 1259, 'length': 7}}, {'content': 'or', 'polygon': [1.7054, 6.7484, 1.8243, 6.7469, 1.8243, 6.8841, 1.7054, 6.8847], 'confidence': 0.997, 'span': {'offset': 1267, 'length': 2}}, {'content': 'sentence-based', 'polygon': [1.8733, 6.7464, 2.7244, 6.7374, 2.7244, 6.8813, 1.8733, 6.8839], 'confidence': 0.994, 'span': {'offset': 1270, 'length': 14}}, {'content': 'methods', 'polygon': [2.7734, 6.737, 3.2468, 6.7329, 3.2468, 6.8809, 2.7734, 6.8812], 'confidence': 0.996, 'span': {'offset': 1285, 'length': 7}}, {'content': 'in', 'polygon': [3.2981, 6.7325, 3.4054, 6.7317, 3.4054, 6.8809, 3.2981, 6.8809], 'confidence': 0.997, 'span': {'offset': 1293, 'length': 2}}, {'content': 'dense', 'polygon': [3.4543, 6.7313, 3.7872, 6.729, 3.7872, 6.8814, 3.4543, 6.881], 'confidence': 0.996, 'span': {'offset': 1296, 'length': 5}}, {'content': 'retrieval.', 'polygon': [1.2028, 6.9019, 1.7112, 6.9029, 1.708, 7.0418, 1.1995, 7.0356], 'confidence': 0.993, 'span': {'offset': 1302, 'length': 10}}, {'content': 'Moreover,', 'polygon': [1.8007, 6.903, 2.3797, 6.903, 2.3764, 7.0473, 1.7974, 7.0427], 'confidence': 0.993, 'span': {'offset': 1313, 'length': 9}}, {'content': 'retrieval', 'polygon': [2.4338, 6.9029, 2.8952, 6.902, 2.8919, 7.0492, 2.4306, 7.0475], 'confidence': 0.993, 'span': {'offset': 1323, 'length': 9}}, {'content': 'by', 'polygon': [2.9446, 6.9019, 3.0835, 6.9015, 3.0802, 7.0495, 2.9413, 7.0493], 'confidence': 0.995, 'span': {'offset': 1333, 'length': 2}}, {'content': 'proposition', 'polygon': [3.1352, 6.9013, 3.7821, 6.8985, 3.7821, 7.0481, 3.132, 7.0495], 'confidence': 0.993, 'span': {'offset': 1336, 'length': 11}}, {'content': 'also', 'polygon': [1.2028, 7.0649, 1.4405, 7.0652, 1.4405, 7.2092, 1.2028, 7.207], 'confidence': 0.993, 'span': {'offset': 1348, 'length': 4}}, {'content': 'enhances', 'polygon': [1.4758, 7.0652, 1.9795, 7.0657, 1.9795, 7.2126, 1.4758, 7.2095], 'confidence': 0.995, 'span': {'offset': 1353, 'length': 8}}, {'content': 'the', 'polygon': [2.0172, 7.0658, 2.1843, 7.0659, 2.1843, 7.213, 2.0172, 7.2127], 'confidence': 0.999, 'span': {'offset': 1362, 'length': 3}}, {'content': 'performance', 'polygon': [2.2196, 7.0659, 2.9187, 7.0663, 2.9187, 7.2113, 2.2196, 7.2131], 'confidence': 0.993, 'span': {'offset': 1366, 'length': 11}}, {'content': 'of', 'polygon': [2.954, 7.0664, 3.0764, 7.0664, 3.0764, 7.2103, 2.954, 7.2111], 'confidence': 0.998, 'span': {'offset': 1378, 'length': 2}}, {'content': 'downstream', 'polygon': [3.1046, 7.0664, 3.7821, 7.0664, 3.7821, 7.2026, 3.1046, 7.2101], 'confidence': 0.994, 'span': {'offset': 1381, 'length': 10}}, {'content': 'QA', 'polygon': [1.2124, 7.2318, 1.4231, 7.2324, 1.4201, 7.3797, 1.2091, 7.3805], 'confidence': 0.993, 'span': {'offset': 1392, 'length': 2}}, {'content': 'tasks,', 'polygon': [1.4722, 7.2326, 1.786, 7.2334, 1.7835, 7.3784, 1.4693, 7.3795], 'confidence': 0.993, 'span': {'offset': 1395, 'length': 6}}, {'content': 'since', 'polygon': [1.8469, 7.2336, 2.1302, 7.2341, 2.1281, 7.3773, 1.8444, 7.3782], 'confidence': 0.996, 'span': {'offset': 1402, 'length': 5}}, {'content': 'the', 'polygon': [2.184, 7.2342, 2.3526, 7.2345, 2.3508, 7.3767, 2.182, 7.3772], 'confidence': 0.999, 'span': {'offset': 1408, 'length': 3}}, {'content': 'retrieved', 'polygon': [2.4041, 7.2345, 2.9005, 7.235, 2.8994, 7.3756, 2.4024, 7.3766], 'confidence': 0.995, 'span': {'offset': 1412, 'length': 9}}, {'content': 'texts', 'polygon': [2.9496, 7.235, 3.2119, 7.235, 3.2112, 7.3752, 2.9486, 7.3755], 'confidence': 0.995, 'span': {'offset': 1422, 'length': 5}}, {'content': 'are', 'polygon': [3.2657, 7.235, 3.4343, 7.2349, 3.4339, 7.3749, 3.2651, 7.3751], 'confidence': 0.997, 'span': {'offset': 1428, 'length': 3}}, {'content': 'more', 'polygon': [3.4858, 7.2349, 3.7872, 7.2347, 3.7872, 7.3746, 3.4854, 7.3748], 'confidence': 0.993, 'span': {'offset': 1432, 'length': 4}}, {'content': 'condensed', 'polygon': [1.1985, 7.3967, 1.8012, 7.3966, 1.8004, 7.5411, 1.1985, 7.5351], 'confidence': 0.995, 'span': {'offset': 1437, 'length': 9}}, {'content': 'with', 'polygon': [1.8393, 7.3966, 2.0822, 7.3964, 2.0811, 7.5429, 1.8385, 7.5414], 'confidence': 0.993, 'span': {'offset': 1447, 'length': 4}}, {'content': 'question-relevant', 'polygon': [2.118, 7.3964, 3.0732, 7.3955, 3.0709, 7.5437, 2.1168, 7.543], 'confidence': 0.99, 'span': {'offset': 1452, 'length': 17}}, {'content': 'information,', 'polygon': [3.1089, 7.3955, 3.8125, 7.3943, 3.8125, 7.5389, 3.1066, 7.5436], 'confidence': 0.989, 'span': {'offset': 1470, 'length': 12}}, {'content': 'reducing', 'polygon': [1.2049, 7.5669, 1.6989, 7.5652, 1.6956, 7.7123, 1.2016, 7.7065], 'confidence': 0.99, 'span': {'offset': 1483, 'length': 8}}, {'content': 'the', 'polygon': [1.735, 7.5651, 1.9013, 7.5646, 1.898, 7.714, 1.7318, 7.7126], 'confidence': 0.999, 'span': {'offset': 1492, 'length': 3}}, {'content': 'need', 'polygon': [1.9374, 7.5645, 2.1953, 7.564, 2.192, 7.7154, 1.9342, 7.7143], 'confidence': 0.993, 'span': {'offset': 1496, 'length': 4}}, {'content': 'for', 'polygon': [2.2314, 7.564, 2.3929, 7.5638, 2.3896, 7.7158, 2.2282, 7.7155], 'confidence': 0.999, 'span': {'offset': 1501, 'length': 3}}, {'content': 'lengthy', 'polygon': [2.4242, 7.5638, 2.8315, 7.5637, 2.8282, 7.715, 2.421, 7.7158], 'confidence': 0.996, 'span': {'offset': 1505, 'length': 7}}, {'content': 'input', 'polygon': [2.87, 7.5637, 3.1544, 7.564, 3.1511, 7.7131, 2.8668, 7.7149], 'confidence': 0.996, 'span': {'offset': 1513, 'length': 5}}, {'content': 'tokens', 'polygon': [3.1881, 7.564, 3.5424, 7.5647, 3.5391, 7.709, 3.1849, 7.7128], 'confidence': 0.996, 'span': {'offset': 1519, 'length': 6}}, {'content': 'and', 'polygon': [3.5833, 7.5648, 3.7973, 7.5653, 3.7946, 7.7056, 3.5801, 7.7085], 'confidence': 0.999, 'span': {'offset': 1526, 'length': 3}}, {'content': 'minimizing', 'polygon': [1.2049, 7.7299, 1.8569, 7.7295, 1.8544, 7.8765, 1.2016, 7.8693], 'confidence': 0.994, 'span': {'offset': 1530, 'length': 10}}, {'content': 'the', 'polygon': [1.8951, 7.7295, 2.0647, 7.7294, 2.0625, 7.8778, 1.8927, 7.8768], 'confidence': 0.999, 'span': {'offset': 1541, 'length': 3}}, {'content': 'inclusion', 'polygon': [2.1005, 7.7293, 2.6068, 7.7288, 2.6053, 7.8783, 2.0984, 7.8779], 'confidence': 0.995, 'span': {'offset': 1545, 'length': 9}}, {'content': 'of', 'polygon': [2.6402, 7.7288, 2.7525, 7.7286, 2.7512, 7.8783, 2.6388, 7.8783], 'confidence': 0.998, 'span': {'offset': 1555, 'length': 2}}, {'content': 'extraneous,', 'polygon': [2.7811, 7.7286, 3.4212, 7.7277, 3.4207, 7.8733, 2.7799, 7.8781], 'confidence': 0.993, 'span': {'offset': 1558, 'length': 11}}, {'content': 'irrele-', 'polygon': [3.4618, 7.7276, 3.8125, 7.7269, 3.8125, 7.8677, 3.4614, 7.8727], 'confidence': 0.993, 'span': {'offset': 1570, 'length': 7}}, {'content': 'vant', 'polygon': [1.208, 7.9023, 1.4511, 7.9017, 1.4487, 8.0305, 1.2048, 8.031], 'confidence': 0.993, 'span': {'offset': 1578, 'length': 4}}, {'content': 'information.', 'polygon': [1.4871, 7.9016, 2.1699, 7.9052, 2.1699, 8.0344, 1.4848, 8.0305], 'confidence': 0.994, 'span': {'offset': 1583, 'length': 12}}, {'content': '1', 'polygon': [0.9773, 8.4529, 1.0628, 8.4543, 1.0582, 8.6333, 0.9728, 8.6337], 'confidence': 0.994, 'span': {'offset': 1600, 'length': 1}}, {'content': 'Introduction', 'polygon': [1.2307, 8.4569, 2.1446, 8.4597, 2.1446, 8.6319, 1.2262, 8.6321], 'confidence': 0.978, 'span': {'offset': 1602, 'length': 12}}, {'content': 'Dense', 'polygon': [0.9693, 8.7839, 1.3513, 8.7838, 1.3489, 8.936, 0.9661, 8.9307], 'confidence': 0.995, 'span': {'offset': 1616, 'length': 5}}, {'content': 'retrievers', 'polygon': [1.3823, 8.7838, 1.9734, 8.7835, 1.9723, 8.942, 1.38, 8.9364], 'confidence': 0.988, 'span': {'offset': 1622, 'length': 10}}, {'content': 'are', 'polygon': [2.0173, 8.7835, 2.2006, 8.7833, 2.2, 8.9429, 2.0163, 8.9423], 'confidence': 0.998, 'span': {'offset': 1633, 'length': 3}}, {'content': 'a', 'polygon': [2.2419, 8.7833, 2.309, 8.7832, 2.3086, 8.9429, 2.2414, 8.9429], 'confidence': 0.995, 'span': {'offset': 1637, 'length': 1}}, {'content': 'popular', 'polygon': [2.3477, 8.7832, 2.8176, 8.7827, 2.8182, 8.9429, 2.3474, 8.9429], 'confidence': 0.994, 'span': {'offset': 1639, 'length': 7}}, {'content': 'class', 'polygon': [2.8537, 8.7827, 3.148, 8.7824, 3.1493, 8.9429, 2.8545, 8.9429], 'confidence': 0.993, 'span': {'offset': 1647, 'length': 5}}, {'content': 'of', 'polygon': [3.1893, 8.7823, 3.3132, 8.7821, 3.3149, 8.9429, 3.1908, 8.9429], 'confidence': 0.993, 'span': {'offset': 1653, 'length': 2}}, {'content': 'techniques', 'polygon': [3.3441, 8.7821, 4.0305, 8.781, 4.0305, 8.9383, 3.346, 8.9429], 'confidence': 0.993, 'span': {'offset': 1656, 'length': 10}}, {'content': 'for', 'polygon': [0.965, 8.97, 1.1666, 8.9698, 1.1636, 9.1281, 0.9618, 9.128], 'confidence': 0.995, 'span': {'offset': 1667, 'length': 3}}, {'content': 'accessing', 'polygon': [1.2448, 8.9697, 1.832, 8.9692, 1.8297, 9.1277, 1.2418, 9.1281], 'confidence': 0.995, 'span': {'offset': 1671, 'length': 9}}, {'content': 'external', 'polygon': [1.9101, 8.9692, 2.4067, 8.969, 2.4049, 9.1256, 1.9079, 9.1274], 'confidence': 0.991, 'span': {'offset': 1681, 'length': 8}}, {'content': 'information', 'polygon': [2.4873, 8.969, 3.2157, 8.9691, 3.2149, 9.1205, 2.4857, 9.1252], 'confidence': 0.994, 'span': {'offset': 1690, 'length': 11}}, {'content': 'sources', 'polygon': [3.2989, 8.9692, 3.7525, 8.9695, 3.7523, 9.1155, 3.2981, 9.1197], 'confidence': 0.993, 'span': {'offset': 1702, 'length': 7}}, {'content': 'for', 'polygon': [3.8357, 8.9695, 4.0298, 8.9696, 4.0298, 9.1126, 3.8355, 9.1146], 'confidence': 0.999, 'span': {'offset': 1710, 'length': 3}}, {'content': 'knowledge-intensive', 'polygon': [0.9693, 9.1546, 2.2092, 9.1544, 2.2079, 9.3181, 0.9693, 9.3148], 'confidence': 0.993, 'span': {'offset': 1714, 'length': 19}}, {'content': 'tasks', 'polygon': [2.2441, 9.1544, 2.5393, 9.1541, 2.5377, 9.3181, 2.2428, 9.3181], 'confidence': 0.995, 'span': {'offset': 1734, 'length': 5}}, {'content': '(Karpukhin', 'polygon': [2.5796, 9.154, 3.2613, 9.1529, 3.2589, 9.3179, 2.5779, 9.3181], 'confidence': 0.995, 'span': {'offset': 1740, 'length': 10}}, {'content': 'et', 'polygon': [3.2989, 9.1528, 3.4062, 9.1525, 3.4036, 9.3173, 3.2964, 9.3178], 'confidence': 0.997, 'span': {'offset': 1751, 'length': 2}}, {'content': 'al.,', 'polygon': [3.4411, 9.1525, 3.6182, 9.1521, 3.6154, 9.3164, 3.4385, 9.3172], 'confidence': 0.989, 'span': {'offset': 1754, 'length': 4}}, {'content': '2020).', 'polygon': [3.6585, 9.1519, 4.0407, 9.1508, 4.0407, 9.3138, 3.6556, 9.3162], 'confidence': 0.994, 'span': {'offset': 1759, 'length': 6}}, {'content': 'Before', 'polygon': [0.9777, 9.3403, 1.3914, 9.3412, 1.3886, 9.4987, 0.9745, 9.497], 'confidence': 0.995, 'span': {'offset': 1766, 'length': 6}}, {'content': 'we', 'polygon': [1.43, 9.3412, 1.6047, 9.3415, 1.6021, 9.4993, 1.4272, 9.4989], 'confidence': 0.997, 'span': {'offset': 1773, 'length': 2}}, {'content': 'use', 'polygon': [1.6406, 9.3415, 1.8411, 9.3418, 1.8387, 9.5, 1.6381, 9.4994], 'confidence': 0.998, 'span': {'offset': 1776, 'length': 3}}, {'content': 'a', 'polygon': [1.8847, 9.3419, 1.949, 9.3419, 1.9468, 9.5001, 1.8825, 9.5], 'confidence': 0.996, 'span': {'offset': 1780, 'length': 1}}, {'content': 'learned', 'polygon': [1.9875, 9.342, 2.45, 9.3423, 2.4483, 9.5003, 1.9854, 9.5001], 'confidence': 0.994, 'span': {'offset': 1782, 'length': 7}}, {'content': 'dense', 'polygon': [2.4885, 9.3423, 2.8251, 9.3423, 2.8239, 9.4998, 2.4869, 9.5002], 'confidence': 0.995, 'span': {'offset': 1790, 'length': 5}}, {'content': 'retriever', 'polygon': [2.8637, 9.3423, 3.3698, 9.3421, 3.3691, 9.4982, 2.8624, 9.4997], 'confidence': 0.994, 'span': {'offset': 1796, 'length': 9}}, {'content': 'to', 'polygon': [3.4058, 9.342, 3.5189, 9.342, 3.5183, 9.4976, 3.4052, 9.498], 'confidence': 0.998, 'span': {'offset': 1806, 'length': 2}}, {'content': 'retrieve', 'polygon': [3.56, 9.3419, 4.0255, 9.3413, 4.0255, 9.4948, 3.5595, 9.4974], 'confidence': 0.995, 'span': {'offset': 1809, 'length': 8}}, {'content': 'from', 'polygon': [0.9714, 9.5355, 1.2822, 9.5337, 1.2819, 9.6927, 0.9714, 9.6896], 'confidence': 0.993, 'span': {'offset': 1818, 'length': 4}}, {'content': 'a', 'polygon': [1.33, 9.5334, 1.3991, 9.533, 1.3987, 9.6938, 1.3297, 9.6931], 'confidence': 0.997, 'span': {'offset': 1823, 'length': 1}}, {'content': 'corpus,', 'polygon': [1.439, 9.5328, 1.8853, 9.5309, 1.8843, 9.6969, 1.4385, 9.6942], 'confidence': 0.994, 'span': {'offset': 1825, 'length': 7}}, {'content': 'an', 'polygon': [1.9384, 9.5308, 2.0819, 9.5304, 2.0807, 9.6975, 1.9374, 9.6971], 'confidence': 0.997, 'span': {'offset': 1833, 'length': 2}}, {'content': 'imperative', 'polygon': [2.1271, 9.5303, 2.7806, 9.5297, 2.7787, 9.6973, 2.1258, 9.6976], 'confidence': 0.995, 'span': {'offset': 1836, 'length': 10}}, {'content': 'design', 'polygon': [2.8258, 9.5297, 3.227, 9.5302, 3.2246, 9.6949, 2.8238, 9.6971], 'confidence': 0.994, 'span': {'offset': 1847, 'length': 6}}, {'content': 'decision', 'polygon': [3.2748, 9.5303, 3.7876, 9.532, 3.7846, 9.6895, 3.2724, 9.6945], 'confidence': 0.993, 'span': {'offset': 1854, 'length': 8}}, {'content': 'we', 'polygon': [3.8354, 9.5323, 4.0305, 9.533, 4.0288, 9.6866, 3.8324, 9.6889], 'confidence': 0.997, 'span': {'offset': 1863, 'length': 2}}, {'content': 'have', 'polygon': [0.9725, 9.7225, 1.2665, 9.7204, 1.2661, 9.8703, 0.9725, 9.8684], 'confidence': 0.993, 'span': {'offset': 1866, 'length': 4}}, {'content': 'to', 'polygon': [1.307, 9.7201, 1.4236, 9.7193, 1.4231, 9.8714, 1.3067, 9.8706], 'confidence': 0.997, 'span': {'offset': 1871, 'length': 2}}, {'content': 'make', 'polygon': [1.4667, 9.7191, 1.7987, 9.7177, 1.7978, 9.8733, 1.4661, 9.8716], 'confidence': 0.993, 'span': {'offset': 1874, 'length': 4}}, {'content': 'is', 'polygon': [1.8392, 9.7175, 1.938, 9.7173, 1.937, 9.8739, 1.8383, 9.8735], 'confidence': 0.997, 'span': {'offset': 1879, 'length': 2}}, {'content': 'the', 'polygon': [1.9811, 9.7172, 2.1661, 9.7168, 2.1649, 9.8748, 1.9801, 9.8741], 'confidence': 0.999, 'span': {'offset': 1882, 'length': 3}}, {'content': 'retrieval', 'polygon': [2.2143, 9.7167, 2.7338, 9.7169, 2.7319, 9.8757, 2.213, 9.875], 'confidence': 0.996, 'span': {'offset': 1886, 'length': 9}}, {'content': 'unit', 'polygon': [2.7769, 9.717, 3.0176, 9.7178, 3.0155, 9.8757, 2.775, 9.8757], 'confidence': 0.993, 'span': {'offset': 1896, 'length': 4}}, {'content': '-', 'polygon': [3.0506, 9.7179, 3.1342, 9.7182, 3.1319, 9.8757, 3.0484, 9.8757], 'confidence': 0.995, 'span': {'offset': 1901, 'length': 1}}, {'content': 'i.e.', 'polygon': [3.1697, 9.7183, 3.3522, 9.7194, 3.3497, 9.8757, 3.1674, 9.8757], 'confidence': 0.993, 'span': {'offset': 1903, 'length': 4}}, {'content': 'the', 'polygon': [3.4079, 9.7197, 3.5955, 9.7208, 3.5927, 9.8757, 3.4053, 9.8757], 'confidence': 0.999, 'span': {'offset': 1908, 'length': 3}}, {'content': 'granu-', 'polygon': [3.6385, 9.7211, 4.044, 9.7245, 4.0408, 9.8755, 3.6357, 9.8757], 'confidence': 0.996, 'span': {'offset': 1912, 'length': 6}}, {'content': 'larity', 'polygon': [0.9682, 9.9099, 1.3022, 9.9099, 1.3018, 10.0684, 0.9682, 10.0684], 'confidence': 0.993, 'span': {'offset': 1919, 'length': 6}}, {'content': 'at', 'polygon': [1.3433, 9.9099, 1.4512, 9.9098, 1.4507, 10.0684, 1.3429, 10.0684], 'confidence': 0.996, 'span': {'offset': 1926, 'length': 2}}, {'content': 'which', 'polygon': [1.4923, 9.9098, 1.8622, 9.9094, 1.8613, 10.0684, 1.4917, 10.0684], 'confidence': 0.997, 'span': {'offset': 1929, 'length': 5}}, {'content': 'we', 'polygon': [1.9008, 9.9093, 2.0755, 9.9089, 2.0743, 10.0679, 1.8998, 10.0684], 'confidence': 0.998, 'span': {'offset': 1935, 'length': 2}}, {'content': 'segment', 'polygon': [2.1192, 9.9088, 2.6201, 9.9074, 2.6184, 10.0651, 2.1179, 10.0677], 'confidence': 0.995, 'span': {'offset': 1938, 'length': 7}}, {'content': 'and', 'polygon': [2.6612, 9.9072, 2.8771, 9.9064, 2.875, 10.0633, 2.6595, 10.0649], 'confidence': 0.999, 'span': {'offset': 1946, 'length': 3}}, {'content': 'index', 'polygon': [2.9182, 9.9062, 3.2496, 9.9047, 3.2472, 10.06, 2.9161, 10.0629], 'confidence': 0.995, 'span': {'offset': 1950, 'length': 5}}, {'content': 'the', 'polygon': [3.2907, 9.9045, 3.4757, 9.9036, 3.473, 10.0577, 3.2882, 10.0596], 'confidence': 0.999, 'span': {'offset': 1956, 'length': 3}}, {'content': 'retrieval', 'polygon': [3.5142, 9.9034, 4.0255, 9.9001, 4.0255, 10.0509, 3.5115, 10.0573], 'confidence': 0.993, 'span': {'offset': 1960, 'length': 9}}, {'content': 'Question:', 'polygon': [4.3443, 3.1983, 4.8238, 3.1963, 4.8212, 3.3359, 4.3411, 3.3354], 'confidence': 0.986, 'span': {'offset': 1987, 'length': 9}}, {'content': 'What', 'polygon': [4.8668, 3.1962, 5.1224, 3.1956, 5.1201, 3.3359, 4.8642, 3.3359], 'confidence': 0.993, 'span': {'offset': 1997, 'length': 4}}, {'content': 'is', 'polygon': [5.1495, 3.1956, 5.249, 3.1955, 5.247, 3.3359, 5.1473, 3.3359], 'confidence': 0.995, 'span': {'offset': 2002, 'length': 2}}, {'content': 'the', 'polygon': [5.283, 3.1954, 5.4345, 3.1953, 5.4327, 3.3359, 5.2809, 3.3359], 'confidence': 0.999, 'span': {'offset': 2005, 'length': 3}}, {'content': 'angle', 'polygon': [5.4662, 3.1954, 5.7353, 3.1956, 5.7339, 3.3359, 5.4644, 3.3359], 'confidence': 0.996, 'span': {'offset': 2009, 'length': 5}}, {'content': 'of', 'polygon': [5.767, 3.1956, 5.8642, 3.1958, 5.863, 3.3359, 5.7656, 3.3359], 'confidence': 0.997, 'span': {'offset': 2015, 'length': 2}}, {'content': 'the', 'polygon': [5.8914, 3.1959, 6.052, 3.1963, 6.0509, 3.3353, 5.8902, 3.3359], 'confidence': 0.995, 'span': {'offset': 2018, 'length': 3}}, {'content': 'Tower', 'polygon': [6.0836, 3.1964, 6.398, 3.1977, 6.3974, 3.3326, 6.0827, 3.3351], 'confidence': 0.998, 'span': {'offset': 2022, 'length': 5}}, {'content': 'of', 'polygon': [6.4252, 3.1978, 6.5247, 3.1983, 6.5243, 3.3313, 6.4246, 3.3324], 'confidence': 0.998, 'span': {'offset': 2028, 'length': 2}}, {'content': 'Pisa?', 'polygon': [6.5518, 3.1985, 6.8393, 3.2002, 6.8393, 3.3279, 6.5514, 3.3311], 'confidence': 0.996, 'span': {'offset': 2031, 'length': 5}}, {'content': 'Passage', 'polygon': [4.3348, 3.4253, 4.7606, 3.432, 4.7606, 3.548, 4.3315, 3.5476], 'confidence': 0.996, 'span': {'offset': 2042, 'length': 7}}, {'content': 'Retrieval', 'polygon': [4.3368, 3.5603, 4.8062, 3.5645, 4.8062, 3.68, 4.3368, 3.6766], 'confidence': 0.993, 'span': {'offset': 2050, 'length': 9}}, {'content': 'Prior', 'polygon': [5.0437, 3.4213, 5.3182, 3.4218, 5.3178, 3.5482, 5.0437, 3.5455], 'confidence': 0.994, 'span': {'offset': 2062, 'length': 5}}, {'content': 'to', 'polygon': [5.3731, 3.4219, 5.4703, 3.422, 5.4697, 3.5494, 5.3726, 3.5487], 'confidence': 0.998, 'span': {'offset': 2068, 'length': 2}}, {'content': 'restoration', 'polygon': [5.5294, 3.422, 6.0764, 3.4222, 6.0749, 3.5524, 5.5287, 3.5498], 'confidence': 0.994, 'span': {'offset': 2071, 'length': 11}}, {'content': 'work', 'polygon': [6.1335, 3.4221, 6.3953, 3.4219, 6.3933, 3.5527, 6.1318, 3.5525], 'confidence': 0.993, 'span': {'offset': 2083, 'length': 4}}, {'content': 'performed', 'polygon': [6.4524, 3.4218, 6.9825, 3.4208, 6.9796, 3.5509, 6.4503, 3.5526], 'confidence': 0.995, 'span': {'offset': 2088, 'length': 9}}, {'content': 'be-', 'polygon': [7.0395, 3.4206, 7.2144, 3.4201, 7.2116, 3.5495, 7.0365, 3.5505], 'confidence': 0.999, 'span': {'offset': 2098, 'length': 3}}, {'content': 'tween', 'polygon': [5.05, 3.5594, 5.3652, 3.5602, 5.3619, 3.6856, 5.0468, 3.6831], 'confidence': 0.996, 'span': {'offset': 2102, 'length': 5}}, {'content': '1990', 'polygon': [5.4105, 3.5602, 5.6515, 3.5606, 5.6483, 3.6872, 5.4073, 3.6859], 'confidence': 0.993, 'span': {'offset': 2108, 'length': 4}}, {'content': 'and', 'polygon': [5.6886, 3.5606, 5.8678, 3.5608, 5.8645, 3.6878, 5.6853, 3.6873], 'confidence': 0.998, 'span': {'offset': 2113, 'length': 3}}, {'content': '2001,', 'polygon': [5.9028, 3.5608, 6.1829, 3.5609, 6.1797, 3.688, 5.8995, 3.6879], 'confidence': 0.993, 'span': {'offset': 2117, 'length': 5}}, {'content': 'the', 'polygon': [6.22, 3.5609, 6.3745, 3.5608, 6.3712, 3.6877, 6.2167, 3.688], 'confidence': 0.999, 'span': {'offset': 2123, 'length': 3}}, {'content': 'tower', 'polygon': [6.4074, 3.5608, 6.6958, 3.5605, 6.6925, 3.6865, 6.4042, 3.6876], 'confidence': 0.996, 'span': {'offset': 2127, 'length': 5}}, {'content': 'leaned', 'polygon': [6.7267, 3.5604, 7.0583, 3.5598, 7.0551, 3.684, 6.7234, 3.6863], 'confidence': 0.994, 'span': {'offset': 2133, 'length': 6}}, {'content': 'at', 'polygon': [7.0933, 3.5597, 7.2043, 3.5594, 7.2043, 3.6828, 7.0901, 3.6837], 'confidence': 0.997, 'span': {'offset': 2140, 'length': 2}}, {'content': 'an', 'polygon': [5.05, 3.7036, 5.1771, 3.7026, 5.1769, 3.8323, 5.05, 3.8322], 'confidence': 0.997, 'span': {'offset': 2143, 'length': 2}}, {'content': 'angle', 'polygon': [5.2104, 3.7024, 5.4749, 3.7007, 5.4743, 3.8324, 5.2102, 3.8324], 'confidence': 0.995, 'span': {'offset': 2146, 'length': 5}}, {'content': 'of', 'polygon': [5.5082, 3.7005, 5.6082, 3.7, 5.6074, 3.8323, 5.5075, 3.8324], 'confidence': 0.997, 'span': {'offset': 2152, 'length': 2}}, {'content': '5.5', 'polygon': [5.6332, 3.6999, 5.7915, 3.6995, 5.7904, 3.8318, 5.6323, 3.8323], 'confidence': 0.993, 'span': {'offset': 2155, 'length': 3}}, {'content': 'degrees,', 'polygon': [5.829, 3.6994, 6.233, 3.6993, 6.2312, 3.8296, 5.8278, 3.8317], 'confidence': 0.993, 'span': {'offset': 2159, 'length': 8}}, {'content': 'but', 'polygon': [6.2642, 3.6994, 6.4246, 3.6999, 6.4225, 3.8281, 6.2624, 3.8295], 'confidence': 0.999, 'span': {'offset': 2168, 'length': 3}}, {'content': 'the', 'polygon': [6.4829, 3.7001, 6.637, 3.7008, 6.6346, 3.8262, 6.4808, 3.8277], 'confidence': 0.995, 'span': {'offset': 2172, 'length': 3}}, {'content': 'tower', 'polygon': [6.6683, 3.701, 6.9474, 3.703, 6.9445, 3.8227, 6.6658, 3.8259], 'confidence': 0.996, 'span': {'offset': 2176, 'length': 5}}, {'content': 'now', 'polygon': [6.9786, 3.7033, 7.1941, 3.7051, 7.1941, 3.8195, 6.9757, 3.8223], 'confidence': 0.995, 'span': {'offset': 2182, 'length': 3}}, {'content': 'leans', 'polygon': [5.0382, 3.8508, 5.3175, 3.8499, 5.3203, 3.9746, 5.0415, 3.971], 'confidence': 0.992, 'span': {'offset': 2191, 'length': 5}}, {'content': 'at', 'polygon': [5.3661, 3.8498, 5.4571, 3.8495, 5.4597, 3.976, 5.3689, 3.9752], 'confidence': 0.995, 'span': {'offset': 2197, 'length': 2}}, {'content': 'about', 'polygon': [5.5015, 3.8494, 5.7828, 3.8486, 5.785, 3.9786, 5.504, 3.9764], 'confidence': 0.993, 'span': {'offset': 2200, 'length': 5}}, {'content': '3.99', 'polygon': [5.8273, 3.8485, 6.0472, 3.848, 6.049, 3.9797, 5.8293, 3.9788], 'confidence': 0.993, 'span': {'offset': 2206, 'length': 4}}, {'content': 'degrees.', 'polygon': [6.0917, 3.8478, 6.5042, 3.8469, 6.5052, 3.9797, 6.0933, 3.9797], 'confidence': 0.985, 'span': {'offset': 2211, 'length': 8}}, {'content': 'This', 'polygon': [6.5866, 3.8468, 6.8088, 3.8463, 6.8093, 3.9784, 6.5876, 3.9796], 'confidence': 0.993, 'span': {'offset': 2220, 'length': 4}}, {'content': 'means', 'polygon': [6.8595, 3.8462, 7.1941, 3.8457, 7.1941, 3.9752, 6.86, 3.9781], 'confidence': 0.995, 'span': {'offset': 2225, 'length': 5}}, {'content': 'the', 'polygon': [5.05, 3.9925, 5.2115, 3.9921, 5.211, 4.1217, 5.05, 4.1217], 'confidence': 0.995, 'span': {'offset': 2231, 'length': 3}}, {'content': 'top', 'polygon': [5.2433, 3.9921, 5.4026, 3.9918, 5.4016, 4.1217, 5.2428, 4.1217], 'confidence': 0.999, 'span': {'offset': 2235, 'length': 3}}, {'content': 'of', 'polygon': [5.4345, 3.9917, 5.5343, 3.9915, 5.5329, 4.1217, 5.4333, 4.1217], 'confidence': 0.998, 'span': {'offset': 2239, 'length': 2}}, {'content': 'the', 'polygon': [5.5598, 3.9914, 5.7191, 3.9911, 5.7171, 4.1214, 5.5583, 4.1217], 'confidence': 0.999, 'span': {'offset': 2242, 'length': 3}}, {'content': 'Leaning', 'polygon': [5.751, 3.991, 6.1567, 3.9901, 6.1534, 4.1201, 5.7489, 4.1213], 'confidence': 0.993, 'span': {'offset': 2246, 'length': 7}}, {'content': 'Tower', 'polygon': [6.1886, 3.9901, 6.5029, 3.9893, 6.4986, 4.1188, 6.1851, 4.12], 'confidence': 0.995, 'span': {'offset': 2254, 'length': 5}}, {'content': 'of', 'polygon': [6.5327, 3.9893, 6.6304, 3.989, 6.6256, 4.1182, 6.5282, 4.1186], 'confidence': 0.998, 'span': {'offset': 2260, 'length': 2}}, {'content': 'Pisa', 'polygon': [6.6559, 3.989, 6.8768, 3.9884, 6.8713, 4.1171, 6.651, 4.1181], 'confidence': 0.993, 'span': {'offset': 2263, 'length': 4}}, {'content': 'is', 'polygon': [6.9065, 3.9884, 6.9872, 3.9882, 6.9814, 4.1165, 6.9009, 4.1169], 'confidence': 0.997, 'span': {'offset': 2268, 'length': 2}}, {'content': 'dis-', 'polygon': [7.0212, 3.9881, 7.2144, 3.9876, 7.2101, 4.1153, 7.0153, 4.1163], 'confidence': 0.993, 'span': {'offset': 2271, 'length': 4}}, {'content': 'placed', 'polygon': [5.0564, 4.1283, 5.3824, 4.1288, 5.3824, 4.2592, 5.0564, 4.26], 'confidence': 0.996, 'span': {'offset': 2276, 'length': 6}}, {'content': 'horizontally', 'polygon': [5.4153, 4.1288, 6.0126, 4.128, 6.0126, 4.2587, 5.4153, 4.2591], 'confidence': 0.993, 'span': {'offset': 2283, 'length': 12}}, {'content': '3.9', 'polygon': [6.0476, 4.1279, 6.1986, 4.1273, 6.1986, 4.2588, 6.0476, 4.2587], 'confidence': 0.994, 'span': {'offset': 2296, 'length': 3}}, {'content': 'meters', 'polygon': [6.2336, 4.1272, 6.5618, 4.1253, 6.5618, 4.2593, 6.2336, 4.2588], 'confidence': 0.993, 'span': {'offset': 2300, 'length': 6}}, {'content': '(12', 'polygon': [6.5968, 4.1251, 6.7565, 4.1238, 6.7565, 4.2599, 6.5968, 4.2594], 'confidence': 0.997, 'span': {'offset': 2307, 'length': 3}}, {'content': 'ft', 'polygon': [6.7915, 4.1235, 6.8659, 4.1229, 6.8659, 4.2601, 6.7915, 4.26], 'confidence': 0.994, 'span': {'offset': 2311, 'length': 2}}, {'content': '10', 'polygon': [6.9097, 4.1225, 7.0212, 4.1214, 7.0212, 4.2607, 6.9097, 4.2603], 'confidence': 0.996, 'span': {'offset': 2314, 'length': 2}}, {'content': 'in)', 'polygon': [7.0541, 4.1211, 7.2043, 4.1196, 7.2043, 4.2614, 7.0541, 4.2609], 'confidence': 0.993, 'span': {'offset': 2317, 'length': 3}}, {'content': 'from', 'polygon': [5.0564, 4.2716, 5.3047, 4.271, 5.3015, 4.3846, 5.0532, 4.3806], 'confidence': 0.993, 'span': {'offset': 2321, 'length': 4}}, {'content': 'the', 'polygon': [5.3362, 4.271, 5.4863, 4.2716, 5.483, 4.3865, 5.333, 4.385], 'confidence': 0.999, 'span': {'offset': 2326, 'length': 3}}, {'content': 'center.', 'polygon': [5.5178, 4.2718, 5.8456, 4.2754, 5.8456, 4.3879, 5.5145, 4.3867], 'confidence': 0.993, 'span': {'offset': 2330, 'length': 7}}, {'content': 'Sentence', 'polygon': [4.3432, 4.4858, 4.8164, 4.4911, 4.8164, 4.5975, 4.3367, 4.5965], 'confidence': 0.994, 'span': {'offset': 2342, 'length': 8}}, {'content': 'Retrieval', 'polygon': [4.3411, 4.6211, 4.8093, 4.6208, 4.8061, 4.738, 4.3379, 4.7388], 'confidence': 0.995, 'span': {'offset': 2351, 'length': 9}}, {'content': 'Prior', 'polygon': [5.0469, 4.4859, 5.3175, 4.4854, 5.3175, 4.6043, 5.0469, 4.6006], 'confidence': 0.996, 'span': {'offset': 2363, 'length': 5}}, {'content': 'to', 'polygon': [5.374, 4.4853, 5.4709, 4.4851, 5.4709, 4.606, 5.374, 4.6051], 'confidence': 0.998, 'span': {'offset': 2369, 'length': 2}}, {'content': 'restoration', 'polygon': [5.5295, 4.485, 6.0747, 4.4837, 6.0747, 4.6083, 5.5295, 4.6065], 'confidence': 0.996, 'span': {'offset': 2372, 'length': 11}}, {'content': 'work', 'polygon': [6.1333, 4.4836, 6.3958, 4.4829, 6.3958, 4.6083, 6.1333, 4.6083], 'confidence': 0.993, 'span': {'offset': 2384, 'length': 4}}, {'content': 'performed', 'polygon': [6.4524, 4.4827, 6.9794, 4.4812, 6.9794, 4.6063, 6.4524, 4.6083], 'confidence': 0.995, 'span': {'offset': 2389, 'length': 9}}, {'content': 'be-', 'polygon': [7.038, 4.481, 7.2094, 4.4804, 7.2094, 4.6038, 7.038, 4.6057], 'confidence': 0.996, 'span': {'offset': 2399, 'length': 3}}, {'content': 'tween', 'polygon': [5.05, 4.6223, 5.3633, 4.6201, 5.3628, 4.7434, 5.05, 4.7398], 'confidence': 0.996, 'span': {'offset': 2403, 'length': 5}}, {'content': '1990', 'polygon': [5.4095, 4.6199, 5.6504, 4.6189, 5.6495, 4.7452, 5.4089, 4.7437], 'confidence': 0.993, 'span': {'offset': 2409, 'length': 4}}, {'content': 'and', 'polygon': [5.6866, 4.6188, 5.8673, 4.6187, 5.866, 4.7452, 5.6856, 4.7452], 'confidence': 0.993, 'span': {'offset': 2414, 'length': 3}}, {'content': '2001,', 'polygon': [5.9034, 4.6186, 6.1825, 4.6191, 6.1808, 4.7452, 5.9021, 4.7452], 'confidence': 0.993, 'span': {'offset': 2418, 'length': 5}}, {'content': 'the', 'polygon': [6.2187, 4.6191, 6.3733, 4.6198, 6.3713, 4.7452, 6.2169, 4.7452], 'confidence': 0.999, 'span': {'offset': 2424, 'length': 3}}, {'content': 'tower', 'polygon': [6.4074, 4.62, 6.6965, 4.6219, 6.694, 4.7447, 6.4054, 4.7452], 'confidence': 0.996, 'span': {'offset': 2428, 'length': 5}}, {'content': 'leaned', 'polygon': [6.7267, 4.6222, 7.058, 4.6255, 7.0549, 4.7412, 6.7241, 4.7444], 'confidence': 0.995, 'span': {'offset': 2434, 'length': 6}}, {'content': 'at', 'polygon': [7.0921, 4.6259, 7.1941, 4.627, 7.1941, 4.7396, 7.089, 4.7409], 'confidence': 0.997, 'span': {'offset': 2441, 'length': 2}}, {'content': 'an', 'polygon': [5.0532, 4.7673, 5.1779, 4.7661, 5.1777, 4.8907, 5.0532, 4.8893], 'confidence': 0.997, 'span': {'offset': 2444, 'length': 2}}, {'content': 'angle', 'polygon': [5.2126, 4.7657, 5.4803, 4.7635, 5.4796, 4.8934, 5.2124, 4.8911], 'confidence': 0.993, 'span': {'offset': 2447, 'length': 5}}, {'content': 'of', 'polygon': [5.515, 4.7633, 5.6274, 4.7626, 5.6265, 4.8942, 5.5143, 4.8936], 'confidence': 0.998, 'span': {'offset': 2453, 'length': 2}}, {'content': '5.5', 'polygon': [5.6519, 4.7625, 5.8011, 4.7621, 5.7999, 4.8945, 5.651, 4.8943], 'confidence': 0.998, 'span': {'offset': 2456, 'length': 3}}, {'content': 'degrees,', 'polygon': [5.8399, 4.762, 6.2506, 4.7622, 6.2488, 4.8931, 5.8387, 4.8945], 'confidence': 0.993, 'span': {'offset': 2460, 'length': 8}}, {'content': 'but', 'polygon': [6.2833, 4.7623, 6.4427, 4.7632, 6.4406, 4.8913, 6.2814, 4.8929], 'confidence': 0.996, 'span': {'offset': 2469, 'length': 3}}, {'content': 'the', 'polygon': [6.4733, 4.7634, 6.6266, 4.7645, 6.6242, 4.889, 6.4712, 4.8909], 'confidence': 0.999, 'span': {'offset': 2473, 'length': 3}}, {'content': 'tower', 'polygon': [6.6593, 4.7648, 6.9412, 4.7677, 6.9384, 4.8837, 6.6568, 4.8885], 'confidence': 0.996, 'span': {'offset': 2477, 'length': 5}}, {'content': 'now', 'polygon': [6.976, 4.7682, 7.1941, 4.771, 7.1913, 4.8785, 6.973, 4.883], 'confidence': 0.999, 'span': {'offset': 2483, 'length': 3}}, {'content': 'leans', 'polygon': [5.0639, 4.9193, 5.3151, 4.9186, 5.3119, 5.0358, 5.0607, 5.0314], 'confidence': 0.993, 'span': {'offset': 2492, 'length': 5}}, {'content': 'at', 'polygon': [5.3507, 4.9185, 5.4417, 4.9183, 5.4384, 5.0376, 5.3475, 5.0363], 'confidence': 0.995, 'span': {'offset': 2498, 'length': 2}}, {'content': 'about', 'polygon': [5.4733, 4.9183, 5.7462, 4.918, 5.743, 5.0408, 5.4701, 5.038], 'confidence': 0.996, 'span': {'offset': 2501, 'length': 5}}, {'content': '3.99', 'polygon': [5.7818, 4.918, 5.9935, 4.9181, 5.9902, 5.0422, 5.7786, 5.041], 'confidence': 0.993, 'span': {'offset': 2507, 'length': 4}}, {'content': 'degrees.', 'polygon': [6.0271, 4.9181, 6.4438, 4.9189, 6.4438, 5.042, 6.0238, 5.0423], 'confidence': 0.989, 'span': {'offset': 2512, 'length': 8}}, {'content': 'Proposition', 'polygon': [4.3336, 5.1344, 4.9178, 5.1322, 4.9178, 5.2543, 4.3304, 5.255], 'confidence': 0.995, 'span': {'offset': 2525, 'length': 11}}, {'content': 'Retrieval', 'polygon': [4.3411, 5.2698, 4.8214, 5.2712, 4.8214, 5.3839, 4.3379, 5.384], 'confidence': 0.995, 'span': {'offset': 2537, 'length': 9}}, {'content': 'The', 'polygon': [5.0639, 5.1314, 5.2566, 5.1314, 5.2566, 5.2596, 5.0639, 5.2572], 'confidence': 0.999, 'span': {'offset': 2549, 'length': 3}}, {'content': 'Leaning', 'polygon': [5.3001, 5.1314, 5.7166, 5.1315, 5.7166, 5.2623, 5.3001, 5.2601], 'confidence': 0.996, 'span': {'offset': 2553, 'length': 7}}, {'content': 'Tower', 'polygon': [5.7601, 5.1316, 6.0791, 5.1319, 6.0791, 5.2623, 5.7601, 5.2623], 'confidence': 0.995, 'span': {'offset': 2561, 'length': 5}}, {'content': 'of', 'polygon': [6.1226, 5.132, 6.2304, 5.1321, 6.2304, 5.2623, 6.1226, 5.2623], 'confidence': 0.998, 'span': {'offset': 2567, 'length': 2}}, {'content': 'Pisa', 'polygon': [6.2718, 5.1322, 6.4831, 5.1326, 6.4831, 5.2604, 6.2718, 5.2623], 'confidence': 0.993, 'span': {'offset': 2570, 'length': 4}}, {'content': 'now', 'polygon': [6.5287, 5.1327, 6.74, 5.1332, 6.74, 5.2571, 6.5287, 5.26], 'confidence': 0.999, 'span': {'offset': 2575, 'length': 3}}, {'content': 'leans', 'polygon': [6.7856, 5.1333, 7.0425, 5.1341, 7.0425, 5.2517, 6.7856, 5.2565], 'confidence': 0.994, 'span': {'offset': 2579, 'length': 5}}, {'content': 'at', 'polygon': [7.0902, 5.1342, 7.1938, 5.1345, 7.1938, 5.2487, 7.0902, 5.2508], 'confidence': 0.996, 'span': {'offset': 2585, 'length': 2}}, {'content': 'about', 'polygon': [5.0553, 5.2941, 5.3395, 5.2901, 5.3373, 5.416, 5.0521, 5.416], 'confidence': 0.994, 'span': {'offset': 2593, 'length': 5}}, {'content': '3.99', 'polygon': [5.3718, 5.29, 5.5855, 5.2907, 5.584, 5.4163, 5.3696, 5.416], 'confidence': 0.993, 'span': {'offset': 2599, 'length': 4}}, {'content': 'degrees.', 'polygon': [5.6198, 5.291, 6.0281, 5.301, 6.0281, 5.4172, 5.6184, 5.4163], 'confidence': 0.994, 'span': {'offset': 2604, 'length': 8}}, {'content': 'Passage', 'polygon': [4.8304, 5.6087, 5.2111, 5.6115, 5.2046, 5.7013, 4.8304, 5.6985], 'confidence': 0.998, 'span': {'offset': 2662, 'length': 7}}, {'content': 'Sentence', 'polygon': [5.6083, 5.6065, 6.0281, 5.6117, 6.0265, 5.7004, 5.6082, 5.6931], 'confidence': 0.995, 'span': {'offset': 2670, 'length': 8}}, {'content': 'Proposition', 'polygon': [6.4306, 5.6044, 6.9407, 5.6094, 6.9406, 5.7029, 6.4305, 5.7001], 'confidence': 0.996, 'span': {'offset': 2679, 'length': 11}}, {'content': '70', 'polygon': [4.6643, 5.8656, 4.7677, 5.8656, 4.7677, 5.9518, 4.6643, 5.9518], 'confidence': 0.995, 'span': {'offset': 2691, 'length': 2}}, {'content': 'Passage', 'polygon': [4.8859, 5.7888, 5.2424, 5.7893, 5.2392, 5.891, 4.8827, 5.8886], 'confidence': 0.996, 'span': {'offset': 2694, 'length': 7}}, {'content': 'Retrieval', 'polygon': [5.2773, 5.7895, 5.6732, 5.7925, 5.672, 5.891, 5.274, 5.891], 'confidence': 0.994, 'span': {'offset': 2702, 'length': 9}}, {'content': 'Question', 'polygon': [6.1012, 5.788, 6.4864, 5.7909, 6.4846, 5.89, 6.098, 5.8892], 'confidence': 0.996, 'span': {'offset': 2712, 'length': 8}}, {'content': 'Answering', 'polygon': [6.5193, 5.7911, 6.9863, 5.7919, 6.9863, 5.8949, 6.5176, 5.8902], 'confidence': 0.996, 'span': {'offset': 2721, 'length': 9}}, {'content': '40', 'polygon': [5.9267, 5.8707, 6.0382, 5.8707, 6.0382, 5.9467, 5.9267, 5.9467], 'confidence': 0.998, 'span': {'offset': 2731, 'length': 2}}, {'content': 'Recall@5', 'polygon': [4.5379, 6.6342, 4.5378, 6.2317, 4.6346, 6.2333, 4.6361, 6.6343], 'confidence': 0.993, 'span': {'offset': 2734, 'length': 8}}, {'content': '(%)', 'polygon': [4.5369, 6.193, 4.5325, 6.0177, 4.6347, 6.02, 4.6345, 6.1948], 'confidence': 0.995, 'span': {'offset': 2743, 'length': 3}}, {'content': '60', 'polygon': [4.6541, 6.0836, 4.7633, 6.0836, 4.7633, 6.1698, 4.6541, 6.1698], 'confidence': 0.996, 'span': {'offset': 2747, 'length': 2}}, {'content': 'EM@100', 'polygon': [5.8017, 6.6268, 5.7957, 6.241, 5.9082, 6.2385, 5.9021, 6.6244], 'confidence': 0.996, 'span': {'offset': 2750, 'length': 6}}, {'content': '(%)', 'polygon': [5.7944, 6.2033, 5.7856, 6.0126, 5.9032, 6.0126, 5.9079, 6.2008], 'confidence': 0.993, 'span': {'offset': 2757, 'length': 3}}, {'content': '30', 'polygon': [5.9317, 6.0735, 6.0288, 6.0782, 6.0241, 6.1693, 5.9317, 6.1642], 'confidence': 0.998, 'span': {'offset': 2761, 'length': 2}}, {'content': '50', 'polygon': [4.6592, 6.2915, 4.7626, 6.2915, 4.7626, 6.3777, 4.6592, 6.3777], 'confidence': 0.996, 'span': {'offset': 2764, 'length': 2}}, {'content': '20', 'polygon': [5.9216, 6.2915, 6.0308, 6.2915, 6.0286, 6.3775, 5.9216, 6.3748], 'confidence': 0.995, 'span': {'offset': 2767, 'length': 2}}, {'content': '40', 'polygon': [4.6541, 6.5095, 4.7748, 6.5095, 4.7729, 6.5956, 4.6541, 6.5929], 'confidence': 0.998, 'span': {'offset': 2770, 'length': 2}}, {'content': '10', 'polygon': [5.9267, 6.5095, 6.0358, 6.5095, 6.0358, 6.5957, 5.9267, 6.5957], 'confidence': 0.998, 'span': {'offset': 2773, 'length': 2}}, {'content': '30', 'polygon': [4.6694, 6.7224, 4.7721, 6.7224, 4.7721, 6.8035, 4.6694, 6.8035], 'confidence': 0.997, 'span': {'offset': 2776, 'length': 2}}, {'content': '0', 'polygon': [5.9774, 6.728, 6.03, 6.7275, 6.0372, 6.7979, 5.9774, 6.8037], 'confidence': 0.865, 'span': {'offset': 2779, 'length': 1}}, {'content': 'Contriever', 'polygon': [4.8368, 6.823, 5.2828, 6.8287, 5.2828, 6.9117, 4.8368, 6.9188], 'confidence': 0.996, 'span': {'offset': 2781, 'length': 10}}, {'content': 'GTR', 'polygon': [5.4045, 6.8238, 5.5775, 6.8238, 5.5775, 6.9049, 5.4045, 6.9049], 'confidence': 0.998, 'span': {'offset': 2792, 'length': 3}}, {'content': 'Contriever', 'polygon': [6.114, 6.8247, 6.5553, 6.8276, 6.5533, 6.9172, 6.1107, 6.9127], 'confidence': 0.996, 'span': {'offset': 2796, 'length': 10}}, {'content': 'GTR', 'polygon': [6.6709, 6.8187, 6.8474, 6.8187, 6.8474, 6.91, 6.6709, 6.91], 'confidence': 0.997, 'span': {'offset': 2807, 'length': 3}}, {'content': 'Figure', 'polygon': [4.2335, 7.054, 4.6168, 7.0548, 4.6135, 7.204, 4.2302, 7.204], 'confidence': 0.995, 'span': {'offset': 2831, 'length': 6}}, {'content': '1:', 'polygon': [4.6774, 7.0549, 4.7647, 7.0551, 4.7615, 7.204, 4.6742, 7.204], 'confidence': 0.994, 'span': {'offset': 2838, 'length': 2}}, {'content': '(Top)', 'polygon': [4.8399, 7.0552, 5.1334, 7.0557, 5.1302, 7.204, 4.8367, 7.204], 'confidence': 0.993, 'span': {'offset': 2841, 'length': 5}}, {'content': 'An', 'polygon': [5.1844, 7.0557, 5.3566, 7.056, 5.3534, 7.204, 5.1811, 7.204], 'confidence': 0.996, 'span': {'offset': 2847, 'length': 2}}, {'content': 'example', 'polygon': [5.4003, 7.056, 5.8757, 7.0566, 5.8725, 7.204, 5.397, 7.204], 'confidence': 0.995, 'span': {'offset': 2850, 'length': 7}}, {'content': 'of', 'polygon': [5.9218, 7.0567, 6.048, 7.0568, 6.0447, 7.204, 5.9186, 7.204], 'confidence': 0.998, 'span': {'offset': 2858, 'length': 2}}, {'content': 'three', 'polygon': [6.0843, 7.0568, 6.3657, 7.0571, 6.3625, 7.2033, 6.0811, 7.2039], 'confidence': 0.996, 'span': {'offset': 2861, 'length': 5}}, {'content': 'granularities', 'polygon': [6.4143, 7.0571, 7.1153, 7.0574, 7.1121, 7.2014, 6.411, 7.2032], 'confidence': 0.993, 'span': {'offset': 2867, 'length': 13}}, {'content': 'of', 'polygon': [7.1638, 7.0574, 7.3057, 7.0574, 7.3057, 7.2009, 7.1606, 7.2013], 'confidence': 0.998, 'span': {'offset': 2881, 'length': 2}}, {'content': 'retrieval', 'polygon': [4.2314, 7.2277, 4.7094, 7.2256, 4.7089, 7.3651, 4.2314, 7.3607], 'confidence': 0.996, 'span': {'offset': 2884, 'length': 9}}, {'content': 'units', 'polygon': [4.7535, 7.2255, 5.0226, 7.2248, 5.0218, 7.3673, 4.7529, 7.3654], 'confidence': 0.998, 'span': {'offset': 2894, 'length': 5}}, {'content': 'of', 'polygon': [5.0667, 7.2247, 5.192, 7.2244, 5.191, 7.3683, 5.0659, 7.3676], 'confidence': 0.998, 'span': {'offset': 2900, 'length': 2}}, {'content': 'Wikipedia', 'polygon': [5.2222, 7.2244, 5.8046, 7.2242, 5.8029, 7.3705, 5.2211, 7.3685], 'confidence': 0.996, 'span': {'offset': 2903, 'length': 9}}, {'content': 'text', 'polygon': [5.844, 7.2243, 6.0552, 7.2245, 6.0533, 7.3708, 5.8423, 7.3706], 'confidence': 0.995, 'span': {'offset': 2913, 'length': 4}}, {'content': 'when', 'polygon': [6.097, 7.2246, 6.3986, 7.2253, 6.3963, 7.3707, 6.095, 7.3708], 'confidence': 0.993, 'span': {'offset': 2918, 'length': 4}}, {'content': 'using', 'polygon': [6.4404, 7.2254, 6.7443, 7.2267, 6.7417, 7.3698, 6.438, 7.3706], 'confidence': 0.997, 'span': {'offset': 2923, 'length': 5}}, {'content': 'dense', 'polygon': [6.7861, 7.2268, 7.104, 7.2285, 7.1009, 7.3681, 6.7834, 7.3696], 'confidence': 0.997, 'span': {'offset': 2929, 'length': 5}}, {'content': 're-', 'polygon': [7.1457, 7.2288, 7.3108, 7.2297, 7.3096, 7.367, 7.1427, 7.3679], 'confidence': 0.999, 'span': {'offset': 2935, 'length': 3}}, {'content': 'trieval.', 'polygon': [4.2367, 7.3883, 4.6198, 7.3889, 4.6198, 7.5344, 4.2367, 7.5349], 'confidence': 0.996, 'span': {'offset': 2939, 'length': 8}}, {'content': '(Bottom)', 'polygon': [4.6728, 7.389, 5.1523, 7.3897, 5.1523, 7.5345, 4.6728, 7.5343], 'confidence': 0.995, 'span': {'offset': 2948, 'length': 8}}, {'content': 'We', 'polygon': [5.1908, 7.3898, 5.3691, 7.3901, 5.3691, 7.5349, 5.1908, 7.5346], 'confidence': 0.997, 'span': {'offset': 2957, 'length': 2}}, {'content': 'observe', 'polygon': [5.4077, 7.3902, 5.8293, 7.3909, 5.8293, 7.5362, 5.4077, 7.535], 'confidence': 0.997, 'span': {'offset': 2960, 'length': 7}}, {'content': 'that', 'polygon': [5.8679, 7.3909, 6.0727, 7.3913, 6.0727, 7.5372, 5.8679, 7.5364], 'confidence': 0.993, 'span': {'offset': 2968, 'length': 4}}, {'content': 'retrieving', 'polygon': [6.1064, 7.3913, 6.6341, 7.3922, 6.6341, 7.5405, 6.1064, 7.5374], 'confidence': 0.996, 'span': {'offset': 2973, 'length': 10}}, {'content': 'by', 'polygon': [6.6702, 7.3923, 6.8051, 7.3925, 6.8051, 7.5416, 6.6702, 7.5407], 'confidence': 0.997, 'span': {'offset': 2984, 'length': 2}}, {'content': 'proposi-', 'polygon': [6.8413, 7.3926, 7.3006, 7.3935, 7.3006, 7.5437, 6.8413, 7.5419], 'confidence': 0.995, 'span': {'offset': 2987, 'length': 8}}, {'content': 'tions', 'polygon': [4.2367, 7.5526, 4.5181, 7.5529, 4.5178, 7.6986, 4.2367, 7.6997], 'confidence': 0.996, 'span': {'offset': 2996, 'length': 5}}, {'content': 'yields', 'polygon': [4.5566, 7.5529, 4.8886, 7.5532, 4.8879, 7.6975, 4.5563, 7.6984], 'confidence': 0.995, 'span': {'offset': 3002, 'length': 6}}, {'content': 'the', 'polygon': [4.9271, 7.5532, 5.0979, 7.5533, 5.097, 7.6971, 4.9264, 7.6974], 'confidence': 0.999, 'span': {'offset': 3009, 'length': 3}}, {'content': 'best', 'polygon': [5.1316, 7.5533, 5.3625, 7.5533, 5.3613, 7.697, 5.1306, 7.697], 'confidence': 0.993, 'span': {'offset': 3013, 'length': 4}}, {'content': 'retrieval', 'polygon': [5.3938, 7.5533, 5.8557, 7.5532, 5.854, 7.6975, 5.3926, 7.697], 'confidence': 0.995, 'span': {'offset': 3018, 'length': 9}}, {'content': 'performance', 'polygon': [5.8942, 7.5532, 6.6014, 7.5524, 6.5989, 7.7003, 5.8924, 7.6975], 'confidence': 0.995, 'span': {'offset': 3028, 'length': 11}}, {'content': 'in', 'polygon': [6.6399, 7.5524, 6.7482, 7.5522, 6.7455, 7.7011, 6.6374, 7.7005], 'confidence': 0.997, 'span': {'offset': 3040, 'length': 2}}, {'content': 'both', 'polygon': [6.7818, 7.5522, 7.032, 7.5517, 7.0291, 7.703, 6.7792, 7.7012], 'confidence': 0.993, 'span': {'offset': 3043, 'length': 4}}, {'content': 'pas-', 'polygon': [7.0681, 7.5516, 7.3108, 7.5512, 7.3108, 7.7051, 7.0651, 7.7032], 'confidence': 0.993, 'span': {'offset': 3048, 'length': 4}}, {'content': 'sage', 'polygon': [4.2356, 7.7288, 4.4956, 7.7282, 4.4983, 7.8665, 4.2388, 7.8669], 'confidence': 0.993, 'span': {'offset': 3053, 'length': 4}}, {'content': 'retrieval', 'polygon': [4.5385, 7.7281, 5.0013, 7.7268, 5.0029, 7.8662, 4.5411, 7.8664], 'confidence': 0.996, 'span': {'offset': 3058, 'length': 9}}, {'content': 'task', 'polygon': [5.0466, 7.7267, 5.2756, 7.7259, 5.2766, 7.8665, 5.0481, 7.8662], 'confidence': 0.993, 'span': {'offset': 3068, 'length': 4}}, {'content': 'and', 'polygon': [5.3186, 7.7257, 5.5213, 7.725, 5.5218, 7.8668, 5.3195, 7.8665], 'confidence': 0.995, 'span': {'offset': 3073, 'length': 3}}, {'content': 'downstream', 'polygon': [5.5643, 7.7249, 6.2513, 7.7219, 6.2502, 7.8692, 5.5647, 7.8669], 'confidence': 0.995, 'span': {'offset': 3077, 'length': 10}}, {'content': 'open-domain', 'polygon': [6.2942, 7.7217, 7.0385, 7.7178, 7.0358, 7.8732, 6.2931, 7.8694], 'confidence': 0.995, 'span': {'offset': 3088, 'length': 11}}, {'content': 'QA', 'polygon': [7.0814, 7.7175, 7.2854, 7.7164, 7.2854, 7.8732, 7.0786, 7.8732], 'confidence': 0.993, 'span': {'offset': 3100, 'length': 2}}, {'content': 'task,', 'polygon': [4.2399, 7.8937, 4.5011, 7.8931, 4.5011, 8.0355, 4.2399, 8.0355], 'confidence': 0.993, 'span': {'offset': 3103, 'length': 5}}, {'content': 'e.g.', 'polygon': [4.5395, 7.893, 4.7312, 7.8926, 4.7312, 8.0355, 4.5395, 8.0355], 'confidence': 0.993, 'span': {'offset': 3109, 'length': 4}}, {'content': 'with', 'polygon': [4.7815, 7.8925, 5.0236, 7.8918, 5.0236, 8.0355, 4.7815, 8.0355], 'confidence': 0.993, 'span': {'offset': 3114, 'length': 4}}, {'content': 'Contriever', 'polygon': [5.0595, 7.8918, 5.6443, 7.89, 5.6443, 8.0355, 5.0595, 8.0355], 'confidence': 0.994, 'span': {'offset': 3119, 'length': 10}}, {'content': '(Izacard', 'polygon': [5.6803, 7.8899, 6.1285, 7.8883, 6.1285, 8.0355, 5.6803, 8.0355], 'confidence': 0.993, 'span': {'offset': 3130, 'length': 8}}, {'content': 'et', 'polygon': [6.162, 7.8882, 6.2627, 7.8878, 6.2627, 8.035, 6.162, 8.0354], 'confidence': 0.996, 'span': {'offset': 3139, 'length': 2}}, {'content': 'al.,', 'polygon': [6.2986, 7.8876, 6.4592, 7.887, 6.4592, 8.0344, 6.2986, 8.0349], 'confidence': 0.99, 'span': {'offset': 3142, 'length': 4}}, {'content': '2022)', 'polygon': [6.4999, 7.8868, 6.8163, 7.8855, 6.8163, 8.033, 6.4999, 8.0343], 'confidence': 0.995, 'span': {'offset': 3147, 'length': 5}}, {'content': 'or', 'polygon': [6.8546, 7.8853, 6.9721, 7.8848, 6.9721, 8.0323, 6.8546, 8.0329], 'confidence': 0.996, 'span': {'offset': 3153, 'length': 2}}, {'content': 'GTR', 'polygon': [7.0056, 7.8846, 7.2905, 7.8832, 7.2905, 8.0307, 7.0056, 8.0321], 'confidence': 0.995, 'span': {'offset': 3156, 'length': 3}}, {'content': '(Ni', 'polygon': [4.2474, 8.0538, 4.4308, 8.0545, 4.4308, 8.2032, 4.2474, 8.2038], 'confidence': 0.993, 'span': {'offset': 3160, 'length': 3}}, {'content': 'et', 'polygon': [4.4743, 8.0547, 4.5781, 8.055, 4.5781, 8.2027, 4.4743, 8.203], 'confidence': 0.997, 'span': {'offset': 3164, 'length': 2}}, {'content': 'al.,', 'polygon': [4.6216, 8.0552, 4.7906, 8.0557, 4.7906, 8.202, 4.6216, 8.2025], 'confidence': 0.989, 'span': {'offset': 3167, 'length': 4}}, {'content': '2022)', 'polygon': [4.8364, 8.0558, 5.16, 8.0565, 5.16, 8.2013, 4.8364, 8.202], 'confidence': 0.996, 'span': {'offset': 3172, 'length': 5}}, {'content': 'as', 'polygon': [5.2082, 8.0566, 5.3217, 8.0567, 5.3217, 8.2012, 5.2082, 8.2013], 'confidence': 0.997, 'span': {'offset': 3178, 'length': 2}}, {'content': 'the', 'polygon': [5.37, 8.0568, 5.5366, 8.0569, 5.5366, 8.2011, 5.37, 8.2012], 'confidence': 0.999, 'span': {'offset': 3181, 'length': 3}}, {'content': 'backbone', 'polygon': [5.5825, 8.057, 6.1232, 8.0568, 6.1232, 8.2018, 5.5825, 8.2011], 'confidence': 0.996, 'span': {'offset': 3185, 'length': 8}}, {'content': 'retriever.', 'polygon': [6.1691, 8.0567, 6.6616, 8.0558, 6.6616, 8.2034, 6.1691, 8.2019], 'confidence': 0.995, 'span': {'offset': 3194, 'length': 10}}, {'content': 'Highlight', 'polygon': [6.7365, 8.0556, 7.2955, 8.0537, 7.2955, 8.2065, 6.7365, 8.2037], 'confidence': 0.996, 'span': {'offset': 3205, 'length': 9}}, {'content': 'indicates', 'polygon': [4.2367, 8.2176, 4.7363, 8.2208, 4.7358, 8.368, 4.2367, 8.3663], 'confidence': 0.994, 'span': {'offset': 3215, 'length': 9}}, {'content': 'the', 'polygon': [4.7746, 8.221, 4.9395, 8.2217, 4.9387, 8.3684, 4.774, 8.3681], 'confidence': 0.999, 'span': {'offset': 3225, 'length': 3}}, {'content': 'part', 'polygon': [4.9778, 8.2219, 5.1953, 8.2228, 5.1943, 8.3689, 4.977, 8.3685], 'confidence': 0.993, 'span': {'offset': 3229, 'length': 4}}, {'content': 'that', 'polygon': [5.2288, 8.2229, 5.4368, 8.2235, 5.4355, 8.3691, 5.2277, 8.3689], 'confidence': 0.989, 'span': {'offset': 3234, 'length': 4}}, {'content': 'contains', 'polygon': [5.4702, 8.2235, 5.9292, 8.224, 5.9274, 8.369, 5.4689, 8.3691], 'confidence': 0.995, 'span': {'offset': 3239, 'length': 8}}, {'content': 'answer', 'polygon': [5.9675, 8.2241, 6.3595, 8.2236, 6.3573, 8.3683, 5.9656, 8.369], 'confidence': 0.996, 'span': {'offset': 3248, 'length': 6}}, {'content': 'to', 'polygon': [6.393, 8.2236, 6.4982, 8.2233, 6.4958, 8.3679, 6.3907, 8.3682], 'confidence': 0.997, 'span': {'offset': 3255, 'length': 2}}, {'content': 'the', 'polygon': [6.534, 8.2232, 6.7014, 8.2226, 6.6987, 8.3672, 6.5316, 8.3678], 'confidence': 0.999, 'span': {'offset': 3258, 'length': 3}}, {'content': 'question.', 'polygon': [6.7373, 8.2225, 7.2601, 8.22, 7.2601, 8.3649, 6.7346, 8.3671], 'confidence': 0.993, 'span': {'offset': 3262, 'length': 9}}, {'content': 'corpus', 'polygon': [4.2399, 8.526, 4.6541, 8.5221, 4.6541, 8.6793, 4.2399, 8.6793], 'confidence': 0.995, 'span': {'offset': 3300, 'length': 6}}, {'content': 'for', 'polygon': [4.6984, 8.5217, 4.8781, 8.5204, 4.8781, 8.6793, 4.6984, 8.6793], 'confidence': 0.998, 'span': {'offset': 3307, 'length': 3}}, {'content': 'inference.', 'polygon': [4.9146, 8.5201, 5.5164, 8.5166, 5.5164, 8.6786, 4.9146, 8.6793], 'confidence': 0.993, 'span': {'offset': 3311, 'length': 10}}, {'content': 'In', 'polygon': [5.5711, 8.5163, 5.6988, 8.5159, 5.6988, 8.6781, 5.5711, 8.6785], 'confidence': 0.995, 'span': {'offset': 3322, 'length': 2}}, {'content': 'practice,', 'polygon': [5.7379, 8.5158, 6.2563, 8.5147, 6.2563, 8.6762, 5.7379, 8.678], 'confidence': 0.993, 'span': {'offset': 3325, 'length': 9}}, {'content': 'the', 'polygon': [6.3006, 8.5146, 6.4882, 8.5146, 6.4882, 8.6753, 6.3006, 8.6761], 'confidence': 0.999, 'span': {'offset': 3335, 'length': 3}}, {'content': 'choice', 'polygon': [6.5273, 8.5146, 6.9259, 8.5151, 6.9259, 8.6731, 6.5273, 8.6751], 'confidence': 0.996, 'span': {'offset': 3339, 'length': 6}}, {'content': 'of', 'polygon': [6.9676, 8.5152, 7.0952, 8.5156, 7.0952, 8.6722, 6.9676, 8.6729], 'confidence': 0.996, 'span': {'offset': 3346, 'length': 2}}, {'content': 're-', 'polygon': [7.1265, 8.5157, 7.3057, 8.5161, 7.3057, 8.671, 7.1265, 8.672], 'confidence': 0.997, 'span': {'offset': 3349, 'length': 3}}, {'content': 'trieval', 'polygon': [4.2367, 8.7096, 4.6334, 8.7068, 4.633, 8.8651, 4.2367, 8.8634], 'confidence': 0.993, 'span': {'offset': 3353, 'length': 7}}, {'content': 'unit,', 'polygon': [4.6752, 8.7066, 4.9492, 8.7055, 4.9485, 8.8664, 4.6747, 8.8654], 'confidence': 0.991, 'span': {'offset': 3361, 'length': 5}}, {'content': 'e.g.', 'polygon': [4.9936, 8.7053, 5.2076, 8.7048, 5.2066, 8.8669, 4.9928, 8.8665], 'confidence': 0.993, 'span': {'offset': 3367, 'length': 4}}, {'content': 'documents,', 'polygon': [5.2676, 8.7047, 5.9698, 8.7054, 5.9679, 8.8669, 5.2665, 8.8669], 'confidence': 0.993, 'span': {'offset': 3372, 'length': 10}}, {'content': 'fixed-length', 'polygon': [6.0167, 8.7056, 6.758, 8.7105, 6.7553, 8.8669, 6.0148, 8.8669], 'confidence': 0.994, 'span': {'offset': 3383, 'length': 12}}, {'content': 'passage', 'polygon': [6.7997, 8.7109, 7.2955, 8.7162, 7.2924, 8.8669, 6.797, 8.8669], 'confidence': 0.995, 'span': {'offset': 3396, 'length': 7}}, {'content': 'chunks', 'polygon': [4.2367, 8.8937, 4.6693, 8.8943, 4.6665, 9.0473, 4.2334, 9.0431], 'confidence': 0.995, 'span': {'offset': 3404, 'length': 6}}, {'content': 'or', 'polygon': [4.7081, 8.8944, 4.8351, 8.8945, 4.8325, 9.0487, 4.7054, 9.0477], 'confidence': 0.995, 'span': {'offset': 3411, 'length': 2}}, {'content': 'sentences,', 'polygon': [4.8765, 8.8945, 5.4749, 8.8948, 5.473, 9.053, 4.8739, 9.049], 'confidence': 0.993, 'span': {'offset': 3414, 'length': 10}}, {'content': 'etc,', 'polygon': [5.5163, 8.8948, 5.7236, 8.8947, 5.7219, 9.0541, 5.5145, 9.0532], 'confidence': 0.977, 'span': {'offset': 3425, 'length': 4}}, {'content': 'is', 'polygon': [5.765, 8.8947, 5.8609, 8.8946, 5.8594, 9.0545, 5.7634, 9.0543], 'confidence': 0.996, 'span': {'offset': 3430, 'length': 2}}, {'content': 'usually', 'polygon': [5.8997, 8.8946, 6.3297, 8.8941, 6.3287, 9.0545, 5.8982, 9.0545], 'confidence': 0.995, 'span': {'offset': 3433, 'length': 7}}, {'content': 'pre-determined', 'polygon': [6.3686, 8.8941, 7.2854, 8.892, 7.2854, 9.0545, 6.3676, 9.0545], 'confidence': 0.989, 'span': {'offset': 3441, 'length': 14}}, {'content': 'based', 'polygon': [4.2335, 9.081, 4.596, 9.0805, 4.5928, 9.2316, 4.2302, 9.2305], 'confidence': 0.998, 'span': {'offset': 3456, 'length': 5}}, {'content': 'on', 'polygon': [4.6335, 9.0805, 4.7835, 9.0803, 4.7803, 9.2319, 4.6303, 9.2317], 'confidence': 0.998, 'span': {'offset': 3462, 'length': 2}}, {'content': 'how', 'polygon': [4.8186, 9.0802, 5.0736, 9.0798, 5.0703, 9.2319, 4.8153, 9.2319], 'confidence': 0.999, 'span': {'offset': 3465, 'length': 3}}, {'content': 'the', 'polygon': [5.1161, 9.0798, 5.2986, 9.0795, 5.2954, 9.2319, 5.1128, 9.2319], 'confidence': 0.999, 'span': {'offset': 3469, 'length': 3}}, {'content': 'dense', 'polygon': [5.3386, 9.0795, 5.6811, 9.079, 5.6779, 9.2319, 5.3354, 9.2319], 'confidence': 0.998, 'span': {'offset': 3473, 'length': 5}}, {'content': 'retrieval', 'polygon': [5.7187, 9.0789, 6.2137, 9.0781, 6.2105, 9.2319, 5.7154, 9.2319], 'confidence': 0.995, 'span': {'offset': 3479, 'length': 9}}, {'content': 'model', 'polygon': [6.2562, 9.0781, 6.6313, 9.0775, 6.628, 9.231, 6.253, 9.2319], 'confidence': 0.995, 'span': {'offset': 3489, 'length': 5}}, {'content': 'is', 'polygon': [6.6738, 9.0774, 6.7688, 9.0773, 6.7655, 9.2306, 6.6705, 9.2309], 'confidence': 0.997, 'span': {'offset': 3495, 'length': 2}}, {'content': 'instanti-', 'polygon': [6.8113, 9.0772, 7.3209, 9.0763, 7.3206, 9.2283, 6.808, 9.2304], 'confidence': 0.995, 'span': {'offset': 3498, 'length': 9}}, {'content': 'ated', 'polygon': [4.2367, 9.2745, 4.4794, 9.2723, 4.4791, 9.4217, 4.2367, 9.4207], 'confidence': 0.993, 'span': {'offset': 3508, 'length': 4}}, {'content': 'or', 'polygon': [4.5097, 9.272, 4.6311, 9.2709, 4.6306, 9.4224, 4.5094, 9.4219], 'confidence': 0.995, 'span': {'offset': 3513, 'length': 2}}, {'content': 'trained', 'polygon': [4.6614, 9.2706, 5.0937, 9.2676, 5.0928, 9.4238, 4.661, 9.4225], 'confidence': 0.993, 'span': {'offset': 3516, 'length': 7}}, {'content': '(Lewis', 'polygon': [5.1291, 9.2674, 5.5311, 9.2657, 5.5297, 9.4243, 5.1282, 9.4239], 'confidence': 0.991, 'span': {'offset': 3524, 'length': 6}}, {'content': 'et', 'polygon': [5.564, 9.2656, 5.6727, 9.2654, 5.6712, 9.4243, 5.5626, 9.4243], 'confidence': 0.995, 'span': {'offset': 3531, 'length': 2}}, {'content': 'al.,', 'polygon': [5.703, 9.2654, 5.88, 9.2651, 5.8783, 9.4241, 5.7015, 9.4242], 'confidence': 0.989, 'span': {'offset': 3534, 'length': 4}}, {'content': '2020;', 'polygon': [5.9154, 9.265, 6.2491, 9.2651, 6.247, 9.4234, 5.9136, 9.4241], 'confidence': 0.994, 'span': {'offset': 3539, 'length': 5}}, {'content': 'Lee', 'polygon': [6.282, 9.2651, 6.5045, 9.2655, 6.5021, 9.4227, 6.2798, 9.4234], 'confidence': 0.996, 'span': {'offset': 3545, 'length': 3}}, {'content': 'et', 'polygon': [6.5373, 9.2656, 6.6461, 9.266, 6.6435, 9.4221, 6.5349, 9.4225], 'confidence': 0.995, 'span': {'offset': 3549, 'length': 2}}, {'content': 'al.,', 'polygon': [6.6789, 9.2661, 6.8534, 9.2667, 6.8506, 9.4213, 6.6763, 9.422], 'confidence': 0.988, 'span': {'offset': 3552, 'length': 4}}, {'content': '2021a;', 'polygon': [6.8888, 9.2669, 7.3006, 9.2693, 7.2976, 9.4187, 6.8859, 9.421], 'confidence': 0.993, 'span': {'offset': 3557, 'length': 6}}, {'content': 'Santhanam', 'polygon': [4.2399, 9.4557, 4.9226, 9.453, 4.9217, 9.6088, 4.2399, 9.6051], 'confidence': 0.996, 'span': {'offset': 3564, 'length': 9}}, {'content': 'et', 'polygon': [4.9608, 9.4529, 5.0703, 9.4527, 5.0692, 9.6093, 4.9598, 9.609], 'confidence': 0.996, 'span': {'offset': 3574, 'length': 2}}, {'content': 'al.,', 'polygon': [5.1085, 9.4527, 5.2894, 9.4523, 5.288, 9.6099, 5.1074, 9.6094], 'confidence': 0.987, 'span': {'offset': 3577, 'length': 4}}, {'content': '2022;', 'polygon': [5.3327, 9.4523, 5.669, 9.4521, 5.667, 9.6104, 5.3312, 9.61], 'confidence': 0.993, 'span': {'offset': 3582, 'length': 5}}, {'content': 'Ni', 'polygon': [5.7123, 9.4521, 5.8626, 9.4522, 5.8604, 9.6103, 5.7103, 9.6103], 'confidence': 0.993, 'span': {'offset': 3588, 'length': 2}}, {'content': 'et', 'polygon': [5.9033, 9.4522, 6.0129, 9.4523, 6.0105, 9.6101, 5.9011, 9.6103], 'confidence': 0.996, 'span': {'offset': 3591, 'length': 2}}, {'content': 'al.,', 'polygon': [6.0536, 9.4524, 6.2319, 9.4527, 6.2293, 9.6097, 6.0512, 9.6101], 'confidence': 0.991, 'span': {'offset': 3594, 'length': 4}}, {'content': '2022).', 'polygon': [6.2752, 9.4527, 6.6669, 9.4538, 6.6669, 9.6082, 6.2725, 9.6096], 'confidence': 0.993, 'span': {'offset': 3599, 'length': 6}}, {'content': 'In', 'polygon': [4.3923, 9.6635, 4.5277, 9.6635, 4.5245, 9.8242, 4.3891, 9.8233], 'confidence': 0.995, 'span': {'offset': 3607, 'length': 2}}, {'content': 'this', 'polygon': [4.5772, 9.6635, 4.7959, 9.6634, 4.7927, 9.8251, 4.574, 9.8246], 'confidence': 0.993, 'span': {'offset': 3610, 'length': 4}}, {'content': 'paper,', 'polygon': [4.848, 9.6634, 5.2177, 9.6634, 5.2145, 9.8251, 4.8447, 9.8251], 'confidence': 0.993, 'span': {'offset': 3615, 'length': 6}}, {'content': 'we', 'polygon': [5.2724, 9.6634, 5.4521, 9.6635, 5.4488, 9.8251, 5.2692, 9.8251], 'confidence': 0.998, 'span': {'offset': 3622, 'length': 2}}, {'content': 'investigate', 'polygon': [5.4989, 9.6635, 6.1577, 9.6639, 6.1544, 9.825, 5.4957, 9.8251], 'confidence': 0.996, 'span': {'offset': 3625, 'length': 11}}, {'content': 'an', 'polygon': [6.2071, 9.6639, 6.3504, 9.6641, 6.3471, 9.8235, 6.2039, 9.8246], 'confidence': 0.998, 'span': {'offset': 3637, 'length': 2}}, {'content': 'overlooked', 'polygon': [6.3972, 9.6641, 7.0872, 9.665, 7.084, 9.8151, 6.394, 9.8232], 'confidence': 0.995, 'span': {'offset': 3640, 'length': 10}}, {'content': 're-', 'polygon': [7.1341, 9.6651, 7.3108, 9.6652, 7.3108, 9.8118, 7.1308, 9.8144], 'confidence': 0.999, 'span': {'offset': 3651, 'length': 3}}, {'content': 'search', 'polygon': [4.2367, 9.8583, 4.6382, 9.855, 4.635, 10.0107, 4.2334, 10.009], 'confidence': 0.997, 'span': {'offset': 3655, 'length': 6}}, {'content': 'question', 'polygon': [4.6768, 9.8546, 5.1891, 9.8519, 5.1858, 10.0122, 4.6736, 10.011], 'confidence': 0.997, 'span': {'offset': 3662, 'length': 8}}, {'content': 'with', 'polygon': [5.2303, 9.8517, 5.5005, 9.8508, 5.4973, 10.0124, 5.227, 10.0122], 'confidence': 0.993, 'span': {'offset': 3671, 'length': 4}}, {'content': 'dense', 'polygon': [5.5366, 9.8507, 5.8815, 9.8505, 5.8783, 10.0122, 5.5333, 10.0125], 'confidence': 0.996, 'span': {'offset': 3676, 'length': 5}}, {'content': 'retrieval', 'polygon': [5.9201, 9.8504, 6.4195, 9.8512, 6.4163, 10.011, 5.9169, 10.0122], 'confidence': 0.995, 'span': {'offset': 3682, 'length': 9}}, {'content': 'inference', 'polygon': [6.4633, 9.8514, 7.0167, 9.8543, 7.0135, 10.008, 6.46, 10.0107], 'confidence': 0.993, 'span': {'offset': 3692, 'length': 9}}, {'content': '-', 'polygon': [7.0527, 9.8545, 7.1351, 9.8551, 7.1319, 10.0073, 7.0495, 10.0078], 'confidence': 0.996, 'span': {'offset': 3702, 'length': 1}}, {'content': 'at', 'polygon': [7.1737, 9.8554, 7.2955, 9.8561, 7.2955, 10.0063, 7.1705, 10.0071], 'confidence': 0.996, 'span': {'offset': 3704, 'length': 2}}, {'content': 'what', 'polygon': [4.2367, 10.0361, 4.5429, 10.0376, 4.5426, 10.1979, 4.2367, 10.1959], 'confidence': 0.993, 'span': {'offset': 3707, 'length': 4}}, {'content': 'retrieval', 'polygon': [4.5799, 10.0378, 5.0841, 10.0396, 5.0832, 10.2002, 4.5795, 10.1982], 'confidence': 0.993, 'span': {'offset': 3712, 'length': 9}}, {'content': 'granularity', 'polygon': [5.1264, 10.0397, 5.7996, 10.0402, 5.7979, 10.2002, 5.1254, 10.2002], 'confidence': 0.995, 'span': {'offset': 3722, 'length': 11}}, {'content': 'should', 'polygon': [5.8445, 10.0402, 6.251, 10.0397, 6.2489, 10.2002, 5.8428, 10.2002], 'confidence': 0.997, 'span': {'offset': 3734, 'length': 6}}, {'content': 'we', 'polygon': [6.2906, 10.0396, 6.4675, 10.0391, 6.4651, 10.2002, 6.2885, 10.2002], 'confidence': 0.993, 'span': {'offset': 3741, 'length': 2}}, {'content': 'segment', 'polygon': [6.5098, 10.039, 7.0219, 10.0368, 7.019, 10.198, 6.5073, 10.2002], 'confidence': 0.995, 'span': {'offset': 3744, 'length': 7}}, {'content': 'and', 'polygon': [7.0615, 10.0366, 7.2955, 10.0353, 7.2955, 10.1962, 7.0585, 10.1978], 'confidence': 0.999, 'span': {'offset': 3752, 'length': 3}}, {'content': 'index', 'polygon': [4.2378, 10.2211, 4.5902, 10.2211, 4.5873, 10.3796, 4.2346, 10.3726], 'confidence': 0.993, 'span': {'offset': 3756, 'length': 5}}, {'content': 'the', 'polygon': [4.6424, 10.2212, 4.8303, 10.2213, 4.8277, 10.3834, 4.6396, 10.3807], 'confidence': 0.999, 'span': {'offset': 3762, 'length': 3}}, {'content': 'retrieval', 'polygon': [4.8825, 10.2213, 5.3889, 10.2217, 5.3868, 10.3878, 4.8799, 10.3841], 'confidence': 0.995, 'span': {'offset': 3766, 'length': 9}}, {'content': 'corpus?', 'polygon': [5.4437, 10.2218, 5.9213, 10.2224, 5.9199, 10.3878, 5.4417, 10.3878], 'confidence': 0.993, 'span': {'offset': 3776, 'length': 7}}, {'content': 'We', 'polygon': [6.0127, 10.2226, 6.2163, 10.223, 6.2151, 10.3876, 6.0113, 10.3878], 'confidence': 0.997, 'span': {'offset': 3784, 'length': 2}}, {'content': 'discover', 'polygon': [6.2711, 10.2231, 6.7879, 10.2241, 6.7874, 10.3806, 6.27, 10.3872], 'confidence': 0.996, 'span': {'offset': 3787, 'length': 8}}, {'content': 'that', 'polygon': [6.8401, 10.2243, 7.0724, 10.2249, 7.0722, 10.3749, 6.8396, 10.3798], 'confidence': 0.993, 'span': {'offset': 3796, 'length': 4}}, {'content': 'se-', 'polygon': [7.1272, 10.225, 7.3158, 10.2253, 7.3158, 10.3696, 7.127, 10.3737], 'confidence': 0.997, 'span': {'offset': 3801, 'length': 3}}, {'content': 'lecting', 'polygon': [4.2367, 10.4147, 4.6545, 10.4136, 4.6517, 10.5809, 4.2334, 10.5781], 'confidence': 0.993, 'span': {'offset': 3805, 'length': 7}}, {'content': 'the', 'polygon': [4.6932, 10.4135, 4.8565, 10.4131, 4.8539, 10.5818, 4.6905, 10.5811], 'confidence': 0.996, 'span': {'offset': 3813, 'length': 3}}, {'content': 'proper', 'polygon': [4.8897, 10.413, 5.2992, 10.412, 5.2971, 10.583, 4.8871, 10.5819], 'confidence': 0.995, 'span': {'offset': 3817, 'length': 6}}, {'content': 'retrieval', 'polygon': [5.3324, 10.4119, 5.8443, 10.4107, 5.8427, 10.5828, 5.3303, 10.5831], 'confidence': 0.989, 'span': {'offset': 3824, 'length': 9}}, {'content': 'granularity', 'polygon': [5.8858, 10.4106, 6.5388, 10.4091, 6.538, 10.5797, 5.8843, 10.5827], 'confidence': 0.993, 'span': {'offset': 3834, 'length': 11}}, {'content': 'at', 'polygon': [6.5803, 10.4091, 6.6854, 10.4088, 6.6848, 10.5786, 6.5795, 10.5794], 'confidence': 0.996, 'span': {'offset': 3846, 'length': 2}}, {'content': 'inference', 'polygon': [6.7242, 10.4088, 7.2955, 10.4076, 7.2955, 10.5727, 6.7236, 10.5783], 'confidence': 0.993, 'span': {'offset': 3849, 'length': 9}}, {'content': 'time', 'polygon': [4.2387, 10.5977, 4.5224, 10.5971, 4.5221, 10.7577, 4.2387, 10.7528], 'confidence': 0.993, 'span': {'offset': 3859, 'length': 4}}, {'content': 'can', 'polygon': [4.5609, 10.5971, 4.773, 10.5967, 4.7724, 10.7616, 4.5606, 10.7583], 'confidence': 0.999, 'span': {'offset': 3864, 'length': 3}}, {'content': 'be', 'polygon': [4.8115, 10.5966, 4.9575, 10.5964, 4.9567, 10.7638, 4.8109, 10.7621], 'confidence': 0.996, 'span': {'offset': 3868, 'length': 2}}, {'content': 'a', 'polygon': [4.9988, 10.5963, 5.0676, 10.5962, 5.0667, 10.7652, 4.998, 10.7643], 'confidence': 0.996, 'span': {'offset': 3871, 'length': 1}}, {'content': 'simple', 'polygon': [5.1089, 10.5961, 5.5137, 10.5956, 5.5124, 10.768, 5.108, 10.7657], 'confidence': 0.996, 'span': {'offset': 3873, 'length': 6}}, {'content': 'yet', 'polygon': [5.5523, 10.5955, 5.7423, 10.5953, 5.7407, 10.768, 5.5509, 10.768], 'confidence': 0.999, 'span': {'offset': 3880, 'length': 3}}, {'content': 'effective', 'polygon': [5.7781, 10.5953, 6.3068, 10.5949, 6.3046, 10.768, 5.7765, 10.768], 'confidence': 0.995, 'span': {'offset': 3884, 'length': 9}}, {'content': 'strategy', 'polygon': [6.3509, 10.5949, 6.83, 10.5947, 6.8273, 10.7651, 6.3486, 10.768], 'confidence': 0.993, 'span': {'offset': 3894, 'length': 8}}, {'content': 'for', 'polygon': [6.8741, 10.5947, 7.0365, 10.5947, 7.0336, 10.7625, 6.8713, 10.7648], 'confidence': 0.999, 'span': {'offset': 3903, 'length': 3}}, {'content': 'im-', 'polygon': [7.0696, 10.5947, 7.3209, 10.5946, 7.3197, 10.7587, 7.0666, 10.7621], 'confidence': 0.989, 'span': {'offset': 3907, 'length': 3}}, {'content': '\\*', 'polygon': [1.2101, 10.1951, 1.2518, 10.1949, 1.255, 10.3109, 1.2134, 10.31], 'confidence': 0.838, 'span': {'offset': 3927, 'length': 2}}, {'content': 'Work', 'polygon': [1.2869, 10.1948, 1.5656, 10.1933, 1.5684, 10.3174, 1.2901, 10.3116], 'confidence': 0.993, 'span': {'offset': 3930, 'length': 4}}, {'content': 'was', 'polygon': [1.6051, 10.1931, 1.7982, 10.1924, 1.8007, 10.3215, 1.6079, 10.3182], 'confidence': 0.999, 'span': {'offset': 3935, 'length': 3}}, {'content': 'done', 'polygon': [1.8421, 10.1922, 2.0878, 10.1913, 2.09, 10.3258, 1.8446, 10.3221], 'confidence': 0.993, 'span': {'offset': 3939, 'length': 4}}, {'content': 'during', 'polygon': [2.1295, 10.1912, 2.4586, 10.1903, 2.4604, 10.3296, 2.1317, 10.3262], 'confidence': 0.995, 'span': {'offset': 3944, 'length': 6}}, {'content': 'internship', 'polygon': [2.5025, 10.1903, 3.0072, 10.1897, 3.0084, 10.3316, 2.5043, 10.3298], 'confidence': 0.993, 'span': {'offset': 3951, 'length': 10}}, {'content': 'at', 'polygon': [3.0511, 10.1897, 3.1432, 10.1897, 3.1443, 10.3317, 3.0522, 10.3316], 'confidence': 0.996, 'span': {'offset': 3962, 'length': 2}}, {'content': 'Tencent', 'polygon': [3.1806, 10.1896, 3.5843, 10.1899, 3.5848, 10.33, 3.1815, 10.3317], 'confidence': 0.993, 'span': {'offset': 3965, 'length': 7}}, {'content': 'AI', 'polygon': [3.6238, 10.1899, 3.7554, 10.1902, 3.7558, 10.3285, 3.6243, 10.3298], 'confidence': 0.99, 'span': {'offset': 3973, 'length': 2}}, {'content': 'Lab,', 'polygon': [3.7971, 10.1903, 4.0407, 10.1907, 4.0407, 10.3256, 3.7974, 10.3281], 'confidence': 0.989, 'span': {'offset': 3976, 'length': 4}}, {'content': 'Bellevue.', 'polygon': [0.9725, 10.3343, 1.45, 10.343, 1.4491, 10.4573, 0.9725, 10.4513], 'confidence': 0.993, 'span': {'offset': 3981, 'length': 9}}, {'content': 'https://github.com/ct123098/', 'polygon': [1.4383, 10.4939, 3.5388, 10.4823, 3.5388, 10.6073, 1.4351, 10.6109], 'confidence': 0.93, 'span': {'offset': 4023, 'length': 28}}, {'content': 'factoid-wiki', 'polygon': [0.9821, 10.6308, 1.8961, 10.6272, 1.8939, 10.7403, 0.982, 10.739], 'confidence': 0.995, 'span': {'offset': 4052, 'length': 12}}, {'content': 'arXiv:2312.06648v2', 'polygon': [0.23, 8.4738, 0.2192, 6.1623, 0.4883, 6.164, 0.4634, 8.477], 'confidence': 0.958, 'span': {'offset': 4075, 'length': 18}}, {'content': '[cs.CL]', 'polygon': [0.219, 5.9945, 0.2184, 5.1974, 0.4855, 5.1984, 0.4883, 5.9961], 'confidence': 0.975, 'span': {'offset': 4094, 'length': 7}}, {'content': '12', 'polygon': [0.2185, 5.0044, 0.2188, 4.7695, 0.4816, 4.7702, 0.4843, 5.0053], 'confidence': 0.995, 'span': {'offset': 4102, 'length': 2}}, {'content': 'Dec', 'polygon': [0.2189, 4.6856, 0.2198, 4.2535, 0.4752, 4.2539, 0.4806, 4.6863], 'confidence': 0.995, 'span': {'offset': 4105, 'length': 3}}, {'content': '2023', 'polygon': [0.2201, 4.1654, 0.2217, 3.6198, 0.4639, 3.6198, 0.4736, 4.1658], 'confidence': 0.993, 'span': {'offset': 4109, 'length': 4}}], 'selectionMarks': [{'state': 'selected', 'polygon': [1.9386, 1.0996, 2.1396, 1.0996, 2.1396, 1.3262, 1.9386, 1.3262], 'confidence': 0.353, 'span': {'offset': 67, 'length': 10}}, {'state': 'unselected', 'polygon': [2.7603, 1.8707, 2.8418, 1.8707, 2.8418, 1.9524, 2.7603, 1.9524], 'confidence': 0.488, 'span': {'offset': 138, 'length': 12}}, {'state': 'unselected', 'polygon': [5.3453, 1.8637, 5.4296, 1.8637, 5.4296, 1.958, 5.3453, 1.958], 'confidence': 0.353, 'span': {'offset': 179, 'length': 12}}, {'state': 'unselected', 'polygon': [4.5463, 2.1301, 4.6251, 2.1301, 4.6251, 2.2174, 4.5463, 2.2174], 'confidence': 0.38, 'span': {'offset': 245, 'length': 12}}, {'state': 'selected', 'polygon': [4.1446, 2.3335, 4.2304, 2.3335, 4.2304, 2.4208, 4.1446, 2.4208], 'confidence': 0.38, 'span': {'offset': 314, 'length': 10}}, {'state': 'selected', 'polygon': [1.2007, 10.4658, 1.3356, 10.4658, 1.3356, 10.5996, 1.2007, 10.5996], 'confidence': 0.353, 'span': {'offset': 3997, 'length': 10}}, {'state': 'selected', 'polygon': [0.2461, 3.7654, 0.454, 3.7654, 0.454, 3.9066, 0.2461, 3.9066], 'confidence': 0.1, 'span': {'offset': 4114, 'length': 10}}, {'state': 'unselected', 'polygon': [0.2431, 3.9034, 0.4582, 3.9034, 0.4582, 4.0335, 0.2431, 4.0335], 'confidence': 0.1, 'span': {'offset': 4125, 'length': 12}}, {'state': 'unselected', 'polygon': [0.3001, 4.2423, 0.4543, 4.2423, 0.4543, 4.3696, 0.3001, 4.3696], 'confidence': 0.1, 'span': {'offset': 4138, 'length': 12}}, {'state': 'unselected', 'polygon': [0.2563, 4.5061, 0.4526, 4.5061, 0.4526, 4.6813, 0.2563, 4.6813], 'confidence': 0.1, 'span': {'offset': 4151, 'length': 12}}, {'state': 'selected', 'polygon': [0.2466, 4.7681, 0.4576, 4.7681, 0.4576, 4.9175, 0.2466, 4.9175], 'confidence': 0.1, 'span': {'offset': 4164, 'length': 10}}, {'state': 'unselected', 'polygon': [0.2527, 5.2587, 0.4564, 5.2587, 0.4564, 5.408, 0.2527, 5.408], 'confidence': 0.1, 'span': {'offset': 4175, 'length': 12}}, {'state': 'unselected', 'polygon': [0.2465, 5.4307, 0.4576, 5.4307, 0.4576, 5.6098, 0.2465, 5.6098], 'confidence': 0.1, 'span': {'offset': 4188, 'length': 12}}, {'state': 'selected', 'polygon': [0.2805, 5.6866, 0.4673, 5.6866, 0.4673, 5.8031, 0.2805, 5.8031], 'confidence': 0.1, 'span': {'offset': 4201, 'length': 10}}, {'state': 'unselected', 'polygon': [5.3627, 5.6041, 5.5544, 5.6041, 5.5544, 5.69, 5.3627, 5.69], 'confidence': 0.1, 'span': {'offset': 4212, 'length': 12}}, {'state': 'selected', 'polygon': [6.192, 5.6096, 6.3866, 5.6096, 6.3866, 5.6907, 6.192, 5.6907], 'confidence': 0.1, 'span': {'offset': 4225, 'length': 10}}, {'state': 'unselected', 'polygon': [0.2803, 5.8085, 0.4754, 5.8085, 0.4754, 5.9192, 0.2803, 5.9192], 'confidence': 0.1, 'span': {'offset': 4236, 'length': 12}}, {'state': 'unselected', 'polygon': [0.2619, 5.9036, 0.4781, 5.9036, 0.4781, 6.0009, 0.2619, 6.0009], 'confidence': 0.1, 'span': {'offset': 4249, 'length': 12}}, {'state': 'selected', 'polygon': [0.2499, 6.1625, 0.4577, 6.1625, 0.4577, 6.3271, 0.2499, 6.3271], 'confidence': 0.1, 'span': {'offset': 4262, 'length': 10}}, {'state': 'selected', 'polygon': [0.2516, 6.4389, 0.4574, 6.4389, 0.4574, 6.5625, 0.2516, 6.5625], 'confidence': 0.1, 'span': {'offset': 4273, 'length': 10}}, {'state': 'selected', 'polygon': [0.2477, 6.6013, 0.454, 6.6013, 0.454, 6.7093, 0.2477, 6.7093], 'confidence': 0.1, 'span': {'offset': 4284, 'length': 10}}, {'state': 'unselected', 'polygon': [0.3094, 6.7167, 0.4568, 6.7167, 0.4568, 6.8412, 0.3094, 6.8412], 'confidence': 0.1, 'span': {'offset': 4295, 'length': 12}}, {'state': 'unselected', 'polygon': [0.2727, 6.8509, 0.4618, 6.8509, 0.4618, 6.9803, 0.2727, 6.9803], 'confidence': 0.1, 'span': {'offset': 4308, 'length': 12}}, {'state': 'unselected', 'polygon': [0.2513, 6.9919, 0.4532, 6.9919, 0.4532, 7.1269, 0.2513, 7.1269], 'confidence': 0.1, 'span': {'offset': 4321, 'length': 12}}, {'state': 'selected', 'polygon': [0.2518, 7.2091, 0.4513, 7.2091, 0.4513, 7.3454, 0.2518, 7.3454], 'confidence': 0.1, 'span': {'offset': 4334, 'length': 10}}, {'state': 'unselected', 'polygon': [0.2491, 7.3458, 0.4532, 7.3458, 0.4532, 7.4566, 0.2491, 7.4566], 'confidence': 0.1, 'span': {'offset': 4345, 'length': 12}}, {'state': 'unselected', 'polygon': [0.249, 7.4842, 0.461, 7.4842, 0.461, 7.6028, 0.249, 7.6028], 'confidence': 0.1, 'span': {'offset': 4358, 'length': 12}}, {'state': 'selected', 'polygon': [0.2605, 7.6212, 0.4534, 7.6212, 0.4534, 7.7571, 0.2605, 7.7571], 'confidence': 0.1, 'span': {'offset': 4371, 'length': 10}}, {'state': 'selected', 'polygon': [0.3014, 7.8445, 0.45, 7.8445, 0.45, 7.977, 0.3014, 7.977], 'confidence': 0.1, 'span': {'offset': 4382, 'length': 10}}, {'state': 'selected', 'polygon': [0.2946, 7.997, 0.4509, 7.997, 0.4509, 8.1603, 0.2946, 8.1603], 'confidence': 0.1, 'span': {'offset': 4393, 'length': 10}}], 'lines': [{'content': 'Dense XX Retrieval: What Retrieval Granularity Should We Use?\n===', 'polygon': [1.3486, 1.1153, 6.9102, 1.1407, 6.9102, 1.3485, 1.3486, 1.3283], 'spans': [{'offset': 0, 'length': 65}]}, {'content': 'Tong Chen \\*\\* Hongwei Wang', 'polygon': [2.0432, 1.6578, 4.1472, 1.6679, 4.1472, 1.8555, 2.0432, 1.8454], 'spans': [{'offset': 78, 'length': 27}]}, {'content': 'Sihao Chen', 'polygon': [4.3094, 1.6578, 5.2422, 1.6578, 5.2473, 1.8251, 4.3094, 1.8352], 'spans': [{'offset': 106, 'length': 10}]}, {'content': 'Wenhao Yu"', 'polygon': [5.3284, 1.6679, 6.2562, 1.6629, 6.2613, 1.82, 5.3284, 1.8302], 'spans': [{'offset': 117, 'length': 10}]}, {'content': 'Kaixin Ma', 'polygon': [1.9722, 1.8555, 2.7884, 1.8606, 2.7834, 2.0329, 1.9671, 2.0329], 'spans': [{'offset': 128, 'length': 9}]}, {'content': 'Xinran Zhao^ Hongming Zhang', 'polygon': [2.8797, 1.8606, 5.3943, 1.8707, 5.3943, 2.0482, 2.8797, 2.0329], 'spans': [{'offset': 151, 'length': 27}]}, {'content': 'Dong Yu', 'polygon': [5.5008, 1.8656, 6.241, 1.8656, 6.241, 2.038, 5.5008, 2.038], 'spans': [{'offset': 192, 'length': 7}]}, {'content': '\\*University of Washington', 'polygon': [2.6161, 2.1141, 4.4463, 2.1343, 4.4412, 2.3118, 2.6161, 2.2966], 'spans': [{'offset': 201, 'length': 26}]}, {'content': 'Tencent AI Lab', 'polygon': [4.5426, 2.1191, 5.6783, 2.1191, 5.6783, 2.3016, 4.5426, 2.3016], 'spans': [{'offset': 229, 'length': 14}]}, {'content': 'University of Pennsylvania ^Carnegie Mellon University', 'polygon': [2.1192, 2.3168, 6.094, 2.327, 6.094, 2.5044, 2.1192, 2.4943], 'spans': [{'offset': 258, 'length': 54}]}, {'content': '# Abstract', 'polygon': [2.175, 3.0722, 2.8087, 3.0722, 2.8087, 3.2193, 2.175, 3.2193], 'spans': [{'offset': 326, 'length': 10}]}, {'content': 'Dense retrieval has become a prominent', 'polygon': [1.1914, 3.4119, 3.7872, 3.417, 3.7872, 3.5538, 1.1914, 3.5488], 'spans': [{'offset': 338, 'length': 38}]}, {'content': 'method to obtain relevant context or world', 'polygon': [1.2016, 3.5792, 3.7821, 3.5792, 3.7821, 3.7211, 1.2016, 3.7211], 'spans': [{'offset': 377, 'length': 42}]}, {'content': 'knowledge in open-domain NLP tasks. When', 'polygon': [1.1965, 3.7516, 3.7821, 3.7465, 3.7821, 3.8884, 1.1965, 3.8986], 'spans': [{'offset': 420, 'length': 40}]}, {'content': 'we use a learned dense retriever on a retrieval', 'polygon': [1.2016, 3.9138, 3.7872, 3.9087, 3.7872, 4.0507, 1.2016, 4.0557], 'spans': [{'offset': 461, 'length': 47}]}, {'content': 'corpus at inference time, an often-overlooked', 'polygon': [1.2016, 4.0862, 3.7872, 4.076, 3.7872, 4.218, 1.2016, 4.2281], 'spans': [{'offset': 509, 'length': 45}]}, {'content': 'design choice is the retrieval unit in which the', 'polygon': [1.1965, 4.2433, 3.7872, 4.2433, 3.7872, 4.3853, 1.1965, 4.3954], 'spans': [{'offset': 555, 'length': 48}]}, {'content': 'corpus is indexed, e.g. document, passage, or', 'polygon': [1.2016, 4.4056, 3.7973, 4.4056, 3.7973, 4.5576, 1.2016, 4.5576], 'spans': [{'offset': 604, 'length': 45}]}, {'content': 'sentence. We discover that the retrieval unit', 'polygon': [1.2016, 4.5729, 3.7872, 4.5678, 3.7872, 4.7097, 1.2016, 4.7148], 'spans': [{'offset': 650, 'length': 45}]}, {'content': 'choice significantly impacts the performance', 'polygon': [1.1965, 4.7402, 3.7872, 4.7402, 3.7872, 4.8922, 1.1965, 4.8922], 'spans': [{'offset': 696, 'length': 44}]}, {'content': 'of both retrieval and downstream tasks. Dis-', 'polygon': [1.2016, 4.9075, 3.7973, 4.9075, 3.7973, 5.0545, 1.2016, 5.0545], 'spans': [{'offset': 741, 'length': 44}]}, {'content': 'tinct from the typical approach of using pas-', 'polygon': [1.1965, 5.0697, 3.8024, 5.0748, 3.8024, 5.2268, 1.1965, 5.2218], 'spans': [{'offset': 786, 'length': 45}]}, {'content': 'sages or sentences, we introduce a novel re-', 'polygon': [1.2016, 5.2471, 3.7973, 5.237, 3.7973, 5.3739, 1.2016, 5.3891], 'spans': [{'offset': 832, 'length': 44}]}, {'content': 'trieval unit, proposition, for dense retrieval.', 'polygon': [1.2016, 5.4043, 3.7973, 5.4043, 3.7973, 5.5513, 1.2016, 5.5564], 'spans': [{'offset': 877, 'length': 47}]}, {'content': 'Propositions are defined as atomic expressions', 'polygon': [1.2016, 5.5665, 3.7821, 5.5665, 3.7821, 5.7237, 1.2016, 5.7186], 'spans': [{'offset': 925, 'length': 46}]}, {'content': 'within text, each encapsulating a distinct fac-', 'polygon': [1.2066, 5.7338, 3.8075, 5.7338, 3.8075, 5.8808, 1.2066, 5.8859], 'spans': [{'offset': 972, 'length': 47}]}, {'content': 'toid and presented in a concise, self-contained', 'polygon': [1.2016, 5.9011, 3.7872, 5.896, 3.7872, 6.0431, 1.2016, 6.0532], 'spans': [{'offset': 1021, 'length': 47}]}, {'content': 'natural language format. We conduct an empir-', 'polygon': [1.2016, 6.0684, 3.8024, 6.0684, 3.8024, 6.2154, 1.2016, 6.2154], 'spans': [{'offset': 1069, 'length': 45}]}, {'content': 'ical comparison of different retrieval granular-', 'polygon': [1.2016, 6.2306, 3.7973, 6.2306, 3.7973, 6.3726, 1.2016, 6.3777], 'spans': [{'offset': 1115, 'length': 48}]}, {'content': 'ity. Our results reveal that proposition-based', 'polygon': [1.2016, 6.403, 3.7872, 6.3979, 3.7872, 6.545, 1.2016, 6.55], 'spans': [{'offset': 1164, 'length': 46}]}, {'content': 'retrieval significantly outperforms traditional', 'polygon': [1.2016, 6.5652, 3.7771, 6.5652, 3.7771, 6.7173, 1.2016, 6.7173], 'spans': [{'offset': 1211, 'length': 47}]}, {'content': 'passage or sentence-based methods in dense', 'polygon': [1.2016, 6.7427, 3.7821, 6.7275, 3.7821, 6.8745, 1.2016, 6.8897], 'spans': [{'offset': 1259, 'length': 42}]}, {'content': 'retrieval. Moreover, retrieval by proposition', 'polygon': [1.1965, 6.8948, 3.7771, 6.8948, 3.7771, 7.0469, 1.1965, 7.0418], 'spans': [{'offset': 1302, 'length': 45}]}, {'content': 'also enhances the performance of downstream', 'polygon': [1.2016, 7.0621, 3.7771, 7.0621, 3.7771, 7.2091, 1.2016, 7.2091], 'spans': [{'offset': 1348, 'length': 43}]}, {'content': 'QA tasks, since the retrieved texts are more', 'polygon': [1.2066, 7.2294, 3.7821, 7.2294, 3.7821, 7.3713, 1.2066, 7.3764], 'spans': [{'offset': 1392, 'length': 44}]}, {'content': 'condensed with question-relevant information,', 'polygon': [1.1965, 7.3916, 3.8075, 7.3916, 3.8075, 7.5386, 1.1965, 7.5386], 'spans': [{'offset': 1437, 'length': 45}]}, {'content': 'reducing the need for lengthy input tokens and', 'polygon': [1.2016, 7.564, 3.7923, 7.5589, 3.7923, 7.711, 1.2016, 7.711], 'spans': [{'offset': 1483, 'length': 46}]}, {'content': 'minimizing the inclusion of extraneous, irrele-', 'polygon': [1.2016, 7.7262, 3.8075, 7.7262, 3.8075, 7.8732, 1.2016, 7.8732], 'spans': [{'offset': 1530, 'length': 47}]}, {'content': 'vant information.', 'polygon': [1.2016, 7.8986, 2.1648, 7.8986, 2.1648, 8.0304, 1.2016, 8.0253], 'spans': [{'offset': 1578, 'length': 17}]}, {'content': '# 1 Introduction', 'polygon': [0.9481, 8.4512, 2.1395, 8.4512, 2.1395, 8.6286, 0.9481, 8.6286], 'spans': [{'offset': 1598, 'length': 16}]}, {'content': 'Dense retrievers are a popular class of techniques', 'polygon': [0.9633, 8.7807, 4.0255, 8.7807, 4.0255, 8.9429, 0.9633, 8.9379], 'spans': [{'offset': 1616, 'length': 50}]}, {'content': 'for accessing external information sources for', 'polygon': [0.9582, 8.9683, 4.0255, 8.9683, 4.0255, 9.1153, 0.9582, 9.1254], 'spans': [{'offset': 1667, 'length': 46}]}, {'content': 'knowledge-intensive tasks (Karpukhin et al., 2020).', 'polygon': [0.9683, 9.1508, 4.0356, 9.1508, 4.0356, 9.313, 0.9683, 9.313], 'spans': [{'offset': 1714, 'length': 51}]}, {'content': 'Before we use a learned dense retriever to retrieve', 'polygon': [0.9734, 9.3384, 4.0204, 9.3384, 4.0204, 9.4955, 0.9734, 9.4955], 'spans': [{'offset': 1766, 'length': 51}]}, {'content': 'from a corpus, an imperative design decision we', 'polygon': [0.9683, 9.531, 4.0255, 9.5259, 4.0255, 9.6932, 0.9683, 9.6932], 'spans': [{'offset': 1818, 'length': 47}]}, {'content': 'have to make is the retrieval unit - i.e. the granu-', 'polygon': [0.9683, 9.7135, 4.0407, 9.7186, 4.0407, 9.8707, 0.9683, 9.8707], 'spans': [{'offset': 1866, 'length': 52}]}, {'content': 'larity at which we segment and index the retrieval', 'polygon': [0.9633, 9.9062, 4.0204, 9.896, 4.0204, 10.0532, 0.9633, 10.0684], 'spans': [{'offset': 1919, 'length': 50}]}, {'content': 'Question: What is the angle of the Tower of Pisa?', 'polygon': [4.3398, 3.1939, 6.8342, 3.1939, 6.8342, 3.3308, 4.3398, 3.3359], 'spans': [{'offset': 1987, 'length': 49}]}, {'content': 'Passage', 'polygon': [4.3297, 3.422, 4.7555, 3.4271, 4.7555, 3.5488, 4.3297, 3.5437], 'spans': [{'offset': 2042, 'length': 7}]}, {'content': 'Retrieval', 'polygon': [4.3347, 3.5589, 4.8062, 3.564, 4.8012, 3.6755, 4.3347, 3.6705], 'spans': [{'offset': 2050, 'length': 9}]}, {'content': 'Prior to restoration work performed be-', 'polygon': [5.0395, 3.417, 7.2094, 3.417, 7.2094, 3.5488, 5.0395, 3.5488], 'spans': [{'offset': 2062, 'length': 39}]}, {'content': 'tween 1990 and 2001, the tower leaned at', 'polygon': [5.0445, 3.5589, 7.1992, 3.5589, 7.1992, 3.6806, 5.0445, 3.6857], 'spans': [{'offset': 2102, 'length': 40}]}, {'content': 'an angle of 5.5 degrees, but the tower now', 'polygon': [5.0496, 3.7009, 7.1891, 3.6958, 7.1891, 3.8225, 5.0496, 3.8276], 'spans': [{'offset': 2143, 'length': 42}]}, {'content': 'leans at about 3.99 degrees. This means', 'polygon': [5.0344, 3.8428, 7.1891, 3.8428, 7.1891, 3.9746, 5.0344, 3.9746], 'spans': [{'offset': 2191, 'length': 39}]}, {'content': 'the top of the Leaning Tower of Pisa is dis-', 'polygon': [5.0496, 3.9898, 7.2094, 3.9848, 7.2094, 4.1115, 5.0496, 4.1166], 'spans': [{'offset': 2231, 'length': 44}]}, {'content': 'placed horizontally 3.9 meters (12 ft 10 in)', 'polygon': [5.0547, 4.1217, 7.1992, 4.1166, 7.1992, 4.2585, 5.0547, 4.2585], 'spans': [{'offset': 2276, 'length': 44}]}, {'content': 'from the center.', 'polygon': [5.0496, 4.2687, 5.8405, 4.2687, 5.8354, 4.3853, 5.0496, 4.3802], 'spans': [{'offset': 2321, 'length': 16}]}, {'content': 'Sentence', 'polygon': [4.3347, 4.4816, 4.8113, 4.4816, 4.8113, 4.5931, 4.3347, 4.5931], 'spans': [{'offset': 2342, 'length': 8}]}, {'content': 'Retrieval', 'polygon': [4.3347, 4.6185, 4.8062, 4.6185, 4.8062, 4.7351, 4.3347, 4.7351], 'spans': [{'offset': 2351, 'length': 9}]}, {'content': 'Prior to restoration work performed be-', 'polygon': [5.0445, 4.4816, 7.2043, 4.4765, 7.2043, 4.6033, 5.0445, 4.6083], 'spans': [{'offset': 2363, 'length': 39}]}, {'content': 'tween 1990 and 2001, the tower leaned at', 'polygon': [5.0496, 4.6185, 7.1891, 4.6185, 7.1891, 4.7452, 5.0496, 4.7402], 'spans': [{'offset': 2403, 'length': 40}]}, {'content': 'an angle of 5.5 degrees, but the tower now', 'polygon': [5.0496, 4.7604, 7.1891, 4.7604, 7.1891, 4.8872, 5.0496, 4.8922], 'spans': [{'offset': 2444, 'length': 42}]}, {'content': 'leans at about 3.99 degrees.', 'polygon': [5.0597, 4.9176, 6.4387, 4.9176, 6.4387, 5.0393, 5.0597, 5.0342], 'spans': [{'offset': 2492, 'length': 28}]}, {'content': 'Proposition', 'polygon': [4.3297, 5.1305, 4.9127, 5.1305, 4.9127, 5.2522, 4.3297, 5.2573], 'spans': [{'offset': 2525, 'length': 11}]}, {'content': 'Retrieval', 'polygon': [4.3347, 5.2674, 4.8164, 5.2674, 4.8164, 5.3789, 4.3347, 5.3789], 'spans': [{'offset': 2537, 'length': 9}]}, {'content': 'The Leaning Tower of Pisa now leans at', 'polygon': [5.0597, 5.1305, 7.1891, 5.1305, 7.1891, 5.2573, 5.0597, 5.2623], 'spans': [{'offset': 2549, 'length': 38}]}, {'content': 'about 3.99 degrees.', 'polygon': [5.0496, 5.2877, 6.023, 5.2877, 6.023, 5.4144, 5.0496, 5.4094], 'spans': [{'offset': 2593, 'length': 19}]}, {'content': '<!-- FigureContent="Passage', 'polygon': [4.8265, 5.6071, 5.2068, 5.6071, 5.2068, 5.6983, 4.8265, 5.6983], 'spans': [{'offset': 2642, 'length': 27}]}, {'content': 'Sentence', 'polygon': [5.5921, 5.602, 6.023, 5.6071, 6.023, 5.6983, 5.5921, 5.6882], 'spans': [{'offset': 2670, 'length': 8}]}, {'content': 'Proposition', 'polygon': [6.4286, 5.602, 6.9356, 5.602, 6.9356, 5.6983, 6.4286, 5.6983], 'spans': [{'offset': 2679, 'length': 11}]}, {'content': '70', 'polygon': [4.6643, 5.8656, 4.7657, 5.8656, 4.7657, 5.9518, 4.6643, 5.9518], 'spans': [{'offset': 2691, 'length': 2}]}, {'content': 'Passage Retrieval', 'polygon': [4.8823, 5.7845, 5.6681, 5.7896, 5.663, 5.891, 4.8823, 5.8859], 'spans': [{'offset': 2694, 'length': 17}]}, {'content': 'Question Answering', 'polygon': [6.094, 5.7845, 6.9812, 5.7896, 6.9812, 5.891, 6.094, 5.8859], 'spans': [{'offset': 2712, 'length': 18}]}, {'content': '40', 'polygon': [5.9267, 5.8707, 6.0331, 5.8707, 6.0281, 5.9467, 5.9267, 5.9467], 'spans': [{'offset': 2731, 'length': 2}]}, {'content': 'Recall@5 (%)', 'polygon': [4.5375, 6.6464, 4.5325, 6.0177, 4.6339, 6.0177, 4.6389, 6.6464], 'spans': [{'offset': 2734, 'length': 12}]}, {'content': '60', 'polygon': [4.6541, 6.0836, 4.7606, 6.0836, 4.7606, 6.1698, 4.6541, 6.1698], 'spans': [{'offset': 2747, 'length': 2}]}, {'content': 'EM@100 (%)', 'polygon': [5.7898, 6.6362, 5.7847, 6.0126, 5.9013, 6.0126, 5.9115, 6.6362], 'spans': [{'offset': 2750, 'length': 10}]}, {'content': '30', 'polygon': [5.9317, 6.0735, 6.0281, 6.0786, 6.0281, 6.1698, 5.9317, 6.1647], 'spans': [{'offset': 2761, 'length': 2}]}, {'content': '50', 'polygon': [4.6592, 6.2915, 4.7606, 6.2915, 4.7606, 6.3777, 4.6592, 6.3777], 'spans': [{'offset': 2764, 'length': 2}]}, {'content': '20', 'polygon': [5.9267, 6.2915, 6.0281, 6.2915, 6.023, 6.3777, 5.9216, 6.3726], 'spans': [{'offset': 2767, 'length': 2}]}, {'content': '40', 'polygon': [4.6541, 6.5095, 4.7707, 6.5145, 4.7707, 6.5957, 4.6541, 6.5957], 'spans': [{'offset': 2770, 'length': 2}]}, {'content': '10', 'polygon': [5.9267, 6.5095, 6.0331, 6.5095, 6.0331, 6.5957, 5.9267, 6.5957], 'spans': [{'offset': 2773, 'length': 2}]}, {'content': '30', 'polygon': [4.6694, 6.7224, 4.7707, 6.7224, 4.7707, 6.8035, 4.6694, 6.8035], 'spans': [{'offset': 2776, 'length': 2}]}, {'content': '0', 'polygon': [5.9774, 6.7325, 6.0281, 6.7275, 6.0331, 6.7984, 5.9774, 6.8035], 'spans': [{'offset': 2779, 'length': 1}]}, {'content': 'Contriever', 'polygon': [4.8367, 6.8187, 5.2777, 6.8187, 5.2777, 6.9151, 4.8367, 6.9151], 'spans': [{'offset': 2781, 'length': 10}]}, {'content': 'GTR', 'polygon': [5.4045, 6.8238, 5.5769, 6.8238, 5.5718, 6.9049, 5.4045, 6.9049], 'spans': [{'offset': 2792, 'length': 3}]}, {'content': 'Contriever', 'polygon': [6.1092, 6.8238, 6.5503, 6.8238, 6.5503, 6.9151, 6.1092, 6.91], 'spans': [{'offset': 2796, 'length': 10}]}, {'content': 'GTR" -->', 'polygon': [6.6466, 6.8187, 6.8443, 6.8238, 6.8443, 6.9049, 6.6466, 6.91], 'spans': [{'offset': 2807, 'length': 8}]}, {'content': 'Figure 1: (Top) An example of three granularities of', 'polygon': [4.2283, 7.0519, 7.3006, 7.0519, 7.3057, 7.199, 4.2283, 7.199], 'spans': [{'offset': 2831, 'length': 52}]}, {'content': 'retrieval units of Wikipedia text when using dense re-', 'polygon': [4.2283, 7.2192, 7.3057, 7.2243, 7.3057, 7.3663, 4.2283, 7.3612], 'spans': [{'offset': 2884, 'length': 54}]}, {'content': 'trieval. (Bottom) We observe that retrieving by proposi-', 'polygon': [4.2333, 7.3865, 7.2955, 7.3916, 7.2955, 7.5386, 4.2333, 7.5336], 'spans': [{'offset': 2939, 'length': 56}]}, {'content': 'tions yields the best retrieval performance in both pas-', 'polygon': [4.2333, 7.5488, 7.3057, 7.5488, 7.3057, 7.7009, 4.2333, 7.6958], 'spans': [{'offset': 2996, 'length': 56}]}, {'content': 'sage retrieval task and downstream open-domain QA', 'polygon': [4.2333, 7.7161, 7.2803, 7.7161, 7.2803, 7.8682, 4.2333, 7.8732], 'spans': [{'offset': 3053, 'length': 49}]}, {'content': 'task, e.g. with Contriever (Izacard et al., 2022) or GTR', 'polygon': [4.2384, 7.8884, 7.2854, 7.8783, 7.2854, 8.0253, 4.2384, 8.0355], 'spans': [{'offset': 3103, 'length': 56}]}, {'content': '(Ni et al., 2022) as the backbone retriever. Highlight', 'polygon': [4.2435, 8.0507, 7.2905, 8.0507, 7.2905, 8.2028, 4.2435, 8.2028], 'spans': [{'offset': 3160, 'length': 54}]}, {'content': 'indicates the part that contains answer to the question.', 'polygon': [4.2333, 8.2129, 7.255, 8.218, 7.255, 8.365, 4.2333, 8.365], 'spans': [{'offset': 3215, 'length': 56}]}, {'content': 'corpus for inference. In practice, the choice of re-', 'polygon': [4.2384, 8.5171, 7.3006, 8.512, 7.3006, 8.6692, 4.2384, 8.6793], 'spans': [{'offset': 3300, 'length': 52}]}, {'content': 'trieval unit, e.g. documents, fixed-length passage', 'polygon': [4.2333, 8.6996, 7.2905, 8.7047, 7.2854, 8.8669, 4.2333, 8.8618], 'spans': [{'offset': 3353, 'length': 50}]}, {'content': 'chunks or sentences, etc, is usually pre-determined', 'polygon': [4.2333, 8.8872, 7.2854, 8.8872, 7.2803, 9.0494, 4.2333, 9.0494], 'spans': [{'offset': 3404, 'length': 51}]}, {'content': 'based on how the dense retrieval model is instanti-', 'polygon': [4.2283, 9.0798, 7.3158, 9.0747, 7.3158, 9.2268, 4.2283, 9.2319], 'spans': [{'offset': 3456, 'length': 51}]}, {'content': 'ated or trained (Lewis et al., 2020; Lee et al., 2021a;', 'polygon': [4.2333, 9.2623, 7.2955, 9.2623, 7.2955, 9.4195, 4.2333, 9.4195], 'spans': [{'offset': 3508, 'length': 55}]}, {'content': 'Santhanam et al., 2022; Ni et al., 2022).', 'polygon': [4.2384, 9.4499, 6.6618, 9.4499, 6.6618, 9.6071, 4.2384, 9.6071], 'spans': [{'offset': 3564, 'length': 41}]}, {'content': 'In this paper, we investigate an overlooked re-', 'polygon': [4.3854, 9.6628, 7.3057, 9.6628, 7.3057, 9.82, 4.3854, 9.8251], 'spans': [{'offset': 3607, 'length': 47}]}, {'content': 'search question with dense retrieval inference - at', 'polygon': [4.2333, 9.8504, 7.2905, 9.8453, 7.2905, 10.0076, 4.2333, 10.0076], 'spans': [{'offset': 3655, 'length': 51}]}, {'content': 'what retrieval granularity should we segment and', 'polygon': [4.2333, 10.0329, 7.2905, 10.0329, 7.2905, 10.1951, 4.2333, 10.1951], 'spans': [{'offset': 3707, 'length': 48}]}, {'content': 'index the retrieval corpus? We discover that se-', 'polygon': [4.2333, 10.2205, 7.3108, 10.2205, 7.3108, 10.3827, 4.2333, 10.3827], 'spans': [{'offset': 3756, 'length': 48}]}, {'content': 'lecting the proper retrieval granularity at inference', 'polygon': [4.2333, 10.4131, 7.2905, 10.403, 7.2905, 10.5754, 4.2333, 10.5804], 'spans': [{'offset': 3805, 'length': 53}]}, {'content': 'time can be a simple yet effective strategy for im-', 'polygon': [4.2384, 10.5906, 7.3158, 10.5906, 7.3158, 10.7629, 4.2384, 10.7629], 'spans': [{'offset': 3859, 'length': 51}]}, {'content': '<!-- Footnote="\\* Work was done during internship at Tencent AI Lab,', 'polygon': [1.2066, 10.185, 4.0356, 10.1901, 4.0356, 10.327, 1.2066, 10.3219], 'spans': [{'offset': 3912, 'length': 68}]}, {'content': 'Bellevue." -->', 'polygon': [0.9683, 10.332, 1.4449, 10.3371, 1.4449, 10.4588, 0.9683, 10.4486], 'spans': [{'offset': 3981, 'length': 14}]}, {'content': '<!-- Footnote="https://github.com/ct123098/', 'polygon': [1.4348, 10.4841, 3.5337, 10.479, 3.5337, 10.6109, 1.4348, 10.6159], 'spans': [{'offset': 4008, 'length': 43}]}, {'content': 'factoid-wiki" -->', 'polygon': [0.9785, 10.6261, 1.8911, 10.621, 1.8911, 10.7376, 0.9785, 10.7376], 'spans': [{'offset': 4052, 'length': 17}]}, {'content': '## arXiv:2312.06648v2 [cs.CL] 12 Dec 2023', 'polygon': [0.2231, 8.4765, 0.218, 3.6198, 0.4867, 3.6198, 0.4867, 8.4765], 'spans': [{'offset': 4072, 'length': 41}]}], 'spans': [{'offset': 0, 'length': 4403}]}





------------------------------------------------------------------------------------------







-------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------




line 0: Dense XX Retrieval: What Retrieval Granularity Should We Use?
line 1: ===
line 2:  :selected:
line 3: Tong Chen \*\* Hongwei Wang Sihao Chen Wenhao Yu" Kaixin Ma :unselected: Xinran Zhao^ Hongming Zhang :unselected: Dong Yu
line 4:
line 5: \*University of Washington
line 6:
line 7: Tencent AI Lab
line 8:  :unselected:
line 9: University of Pennsylvania ^Carnegie Mellon University
line 10:  :selected:
line 11:
line 12: # Abstract
line 13:
line 14: Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Dis- tinct from the typical approach of using pas- sages or sentences, we introduce a novel re- trieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct fac-
line 15:
line 16: toid and presented in a concise, self-contained natural language format. We conduct an empir- ical comparison of different retrieval granular- ity. Our results reveal that proposition-based retrieval significantly outperforms traditional passage or sentence-based methods in dense retrieval. Moreover, retrieval by proposition also enhances the performance of downstream QA tasks, since the retrieved texts are more condensed with question-relevant information, reducing the need for lengthy input tokens and minimizing the inclusion of extraneous, irrele- vant information.
line 17:
line 18:
line 19: # 1 Introduction
line 20:
line 21: Dense retrievers are a popular class of techniques for accessing external information sources for knowledge-intensive tasks (Karpukhin et al., 2020). Before we use a learned dense retriever to retrieve from a corpus, an imperative design decision we have to make is the retrieval unit - i.e. the granu- larity at which we segment and index the retrieval
line 22:
line 23: |||
line 24: | - | - |
line 25: | Question: What is the angle of the Tower of Pisa? ||
line 26: | Passage Retrieval | Prior to restoration work performed be- tween 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but the tower now |
line 27: || leans at about 3.99 degrees. This means the top of the Leaning Tower of Pisa is dis- placed horizontally 3.9 meters (12 ft 10 in) from the center. |
line 28: | Sentence Retrieval | Prior to restoration work performed be- tween 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but the tower now |
line 29: || leans at about 3.99 degrees. |
line 30: | Proposition Retrieval | The Leaning Tower of Pisa now leans at |
line 31: || about 3.99 degrees. |
line 32:
line 33: <figure>
line 34:
line 35: ![](figures/0)
line 36:
line 37: <!-- FigureContent="Passage Sentence Proposition 70 Passage Retrieval Question Answering 40 Recall@5 (%) 60 EM@100 (%) 30 50 20 40 10 30 0 Contriever GTR Contriever GTR" -->
line 38:
line 39: <figcaption>
line 40:
line 41: Figure 1: (Top) An example of three granularities of
line 42: retrieval units of Wikipedia text when using dense re-
line 43: trieval. (Bottom) We observe that retrieving by proposi-
line 44: tions yields the best retrieval performance in both pas-
line 45: sage retrieval task and downstream open-domain QA
line 46: task, e.g. with Contriever (Izacard et al., 2022) or GTR
line 47: (Ni et al., 2022) as the backbone retriever. Highlight
line 48: indicates the part that contains answer to the question.
line 49:
line 50: </figcaption>
line 51:
line 52: </figure>
line 53:
line 54:
line 55: corpus for inference. In practice, the choice of re- trieval unit, e.g. documents, fixed-length passage chunks or sentences, etc, is usually pre-determined based on how the dense retrieval model is instanti- ated or trained (Lewis et al., 2020; Lee et al., 2021a; Santhanam et al., 2022; Ni et al., 2022).
line 56:
line 57: In this paper, we investigate an overlooked re- search question with dense retrieval inference - at what retrieval granularity should we segment and index the retrieval corpus? We discover that se- lecting the proper retrieval granularity at inference time can be a simple yet effective strategy for im-
line 58:
line 59: <!-- Footnote="\* Work was done during internship at Tencent AI Lab, Bellevue." -->
line 60:  :selected:
line 61: <!-- Footnote="https://github.com/ct123098/ factoid-wiki" -->
line 62:
line 63:
line 64: ## arXiv:2312.06648v2 [cs.CL] 12 Dec 2023
line 65: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected:<figure>
line 66:
line 67: ![](figures/1)
line 68:
line 69: <!-- FigureContent="A Prior to restoration work performed between 1990 and 2001, the tower leaned at an angle of 5.5 degrees , but the tower now leans at B Corpus C Query about 3.99 degrees. This means the top of the Learning Tower of Pisa is displaced horizontally ? Retrieval Wikipedia Units 3.9 meters (12 ft 10 in) from the center. = Passage Retrieval Retriever 1. Prior to restoration work performed between 1990 and 2001, the Leaning Tower Proposition-izer Retrieval :selected: D of Pisa leaned at an angle of 5.5 degrees. Units QA Model 2. The Leaning Tower of Pisa now leans at about 3.99 degrees. Query Answer ? Sentences >  3. The top of the Leaning Tower of Pisa is Retrieval Units displaced horizontally 3.9 meters (12 ft 10 in) from the center. FactoidWiki Passages Propositions" -->
line 70:
line 71: <figcaption>
line 72:
line 73: Figure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet
line 74: effective strategy to increase dense retrievers' generalization performance at inference time (A, B). We empirically
line 75: compare the retrieval and downstream open-domain QA tasks performance when dense retrievers work with
line 76: Wikipedia indexed at the level of 100-word passage, sentence or proposition (C, D).
line 77:
line 78: </figcaption>
line 79:
line 80: </figure>
line 81:
line 82:
line 83: proving dense retrievers' retrieval and downstream task performance. We illustrate our intuition with an example of open-domain question-answering (QA) in Table 1. The example shows retrieved text by the same model at three different granularities. The passage, which represents a coarser retrieval unit with a longer context, is theoretically able to provide more relevant information for the ques- tion. However, a passage often includes extraneous details (e.g., restoration period and horizontal dis- placement in the example of Table 1) that could po- tentially distract both the retriever and the language model in downstream tasks (Shi et al., 2023; Yu et al., 2023b). On the other hand, sentence-level in- dexing provides a finer-grained approach but does not entirely address the issue (Akkalyoncu Yilmaz et al., 2019; Yang et al., 2020). This is because sen- tences can still be complex and compounded, and they are often not self-contained, lacking necessary contextual information (e.g., in the example of Ta- ble 1, "the tower" is coreference of "Pisa Tower") for judging the query-document relevance.
line 84:
line 85: To address these shortcomings of typical re- trieval units such as passages or sentences, we propose using proposition as a novel retrieval unit for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulat- ing a distinct factoid and presented in a concise, self-contained natural language format. We show an example proposition in Table 1. The proposi- tion describes the information regarding the Tower of Pisa's current leaning angle in a self-contained way and precisely responds to what the question is querying. We provide a more detailed definition and description of proposition in 2.
line 86:
line 87: To validate the efficacy of using proposition as a retrieval unit for dense retrievers inference, we first process and index an English Wikipedia dump with all documents segmented into propositions, which we refer to as FACTOIDWIKI. Then we con- duct experiments on five different open-domain QA datasets and empirically compare the performance of six dual-encoder retrievers when Wikipedia is indexed by passage, sentence, and our proposed proposition. Our evaluation is twofold: we exam- ine both the retrieval performance and the impact on downstream QA tasks. Notably, our findings in- dicate that proposition-based retrieval outperforms sentence and passage-based methods, especially in terms of generalization, as discussed in 5. This suggests that propositions, being both compact and rich in context, enable dense retrievers to access precise information while maintaining adequate context. The average improvement over passage- based retrieval of Recall@20 is +10.1 on unsu- pervised dense retrievers and +2.2 on supervised retrievers. Furthermore, we observe a distinct ad- vantage in downstream QA performance when us- ing proposition-based retrieval, as elaborated in 6. Given the often limited input token length in lan- guage models, propositions inherently provide a higher density of question-relevant information.
line 88:
line 89: Our main contributions in the paper are:
line 90:
line 91:  We propose using propositions as retrieval units when indexing a retrieval corpus to improve the dense retrieval performance.
line 92:
line 93:  We introduce FACTOIDWIKI, a processed En- glish Wikipedia dump, where each page is seg- mented into multiple granularities: 100-word pas- sages, sentences and propositions.
line 94:
line 95:  We discover that retrieval by proposition outper- forms passage or sentence retrieval in terms of generalization for passage retrieval and accuracy for downstream question-answering given the same input token limit.
line 96:
line 97:
line 98: # 2 Proposition as a Retrieval Unit
line 99:




















---------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------




---------------------------------------------------------------------------------------------------














line 0: Dense XX Retrieval: What Retrieval Granularity Should We Use?
line 1: ===
line 2:  :selected:
line 3: Tong Chen \*\* Hongwei Wang Sihao Chen Wenhao Yu" Kaixin Ma :unselected: Xinran Zhao^ Hongming Zhang :unselected: Dong Yu
line 4:
line 5: \*University of Washington
line 6:
line 7: Tencent AI Lab
line 8:  :unselected:
line 9: University of Pennsylvania ^Carnegie Mellon University
line 10:  :selected:
line 11:
line 12: # Abstract
line 13:
line 14: Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Dis- tinct from the typical approach of using pas- sages or sentences, we introduce a novel re- trieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct fac-
line 15:
line 16: toid and presented in a concise, self-contained natural language format. We conduct an empir- ical comparison of different retrieval granular- ity. Our results reveal that proposition-based retrieval significantly outperforms traditional passage or sentence-based methods in dense retrieval. Moreover, retrieval by proposition also enhances the performance of downstream QA tasks, since the retrieved texts are more condensed with question-relevant information, reducing the need for lengthy input tokens and minimizing the inclusion of extraneous, irrele- vant information.
line 17:
line 18:
line 19: # 1 Introduction
line 20:
line 21: Dense retrievers are a popular class of techniques for accessing external information sources for knowledge-intensive tasks (Karpukhin et al., 2020). Before we use a learned dense retriever to retrieve from a corpus, an imperative design decision we have to make is the retrieval unit - i.e. the granu- larity at which we segment and index the retrieval
line 22:
line 23: |||
line 24: | - | - |
line 25: | Question: What is the angle of the Tower of Pisa? ||
line 26: | Passage Retrieval | Prior to restoration work performed be- tween 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but the tower now |
line 27: || leans at about 3.99 degrees. This means the top of the Leaning Tower of Pisa is dis- placed horizontally 3.9 meters (12 ft 10 in) from the center. |
line 28: | Sentence Retrieval | Prior to restoration work performed be- tween 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but the tower now |
line 29: || leans at about 3.99 degrees. |
line 30: | Proposition Retrieval | The Leaning Tower of Pisa now leans at |
line 31: || about 3.99 degrees. |
line 32:
line 33: <figure>
line 34:
line 35: ![](figures/0)
line 36:
line 37: <!-- FigureContent="Passage Sentence Proposition 70 Passage Retrieval Question Answering 40 Recall@5 (%) 60 EM@100 (%) 30 50 20 40 10 30 0 Contriever GTR Contriever GTR" -->
line 38:
line 39: <figcaption>
line 40:
line 41: Figure 1: (Top) An example of three granularities of
line 42: retrieval units of Wikipedia text when using dense re-
line 43: trieval. (Bottom) We observe that retrieving by proposi-
line 44: tions yields the best retrieval performance in both pas-
line 45: sage retrieval task and downstream open-domain QA
line 46: task, e.g. with Contriever (Izacard et al., 2022) or GTR
line 47: (Ni et al., 2022) as the backbone retriever. Highlight
line 48: indicates the part that contains answer to the question.
line 49:
line 50: </figcaption>
line 51:
line 52: </figure>
line 53:
line 54:
line 55: corpus for inference. In practice, the choice of re- trieval unit, e.g. documents, fixed-length passage chunks or sentences, etc, is usually pre-determined based on how the dense retrieval model is instanti- ated or trained (Lewis et al., 2020; Lee et al., 2021a; Santhanam et al., 2022; Ni et al., 2022).
line 56:
line 57: In this paper, we investigate an overlooked re- search question with dense retrieval inference - at what retrieval granularity should we segment and index the retrieval corpus? We discover that se- lecting the proper retrieval granularity at inference time can be a simple yet effective strategy for im-
line 58:
line 59: <!-- Footnote="\* Work was done during internship at Tencent AI Lab, Bellevue." -->
line 60:  :selected:
line 61: <!-- Footnote="https://github.com/ct123098/ factoid-wiki" -->
line 62:
line 63:
line 64: ## arXiv:2312.06648v2 [cs.CL] 12 Dec 2023
line 65: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected:<figure>
line 66:
line 67: ![](figures/1)
line 68:
line 69: <!-- FigureContent="A Prior to restoration work performed between 1990 and 2001, the tower leaned at an angle of 5.5 degrees , but the tower now leans at B Corpus C Query about 3.99 degrees. This means the top of the Learning Tower of Pisa is displaced horizontally ? Retrieval Wikipedia Units 3.9 meters (12 ft 10 in) from the center. = Passage Retrieval Retriever 1. Prior to restoration work performed between 1990 and 2001, the Leaning Tower Proposition-izer Retrieval :selected: D of Pisa leaned at an angle of 5.5 degrees. Units QA Model 2. The Leaning Tower of Pisa now leans at about 3.99 degrees. Query Answer ? Sentences >  3. The top of the Leaning Tower of Pisa is Retrieval Units displaced horizontally 3.9 meters (12 ft 10 in) from the center. FactoidWiki Passages Propositions" -->
line 70:
line 71: <figcaption>
line 72:
line 73: Figure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet
line 74: effective strategy to increase dense retrievers' generalization performance at inference time (A, B). We empirically
line 75: compare the retrieval and downstream open-domain QA tasks performance when dense retrievers work with
line 76: Wikipedia indexed at the level of 100-word passage, sentence or proposition (C, D).
line 77:
line 78: </figcaption>
line 79:
line 80: </figure>
line 81:
line 82:
line 83: proving dense retrievers' retrieval and downstream task performance. We illustrate our intuition with an example of open-domain question-answering (QA) in Table 1. The example shows retrieved text by the same model at three different granularities. The passage, which represents a coarser retrieval unit with a longer context, is theoretically able to provide more relevant information for the ques- tion. However, a passage often includes extraneous details (e.g., restoration period and horizontal dis- placement in the example of Table 1) that could po- tentially distract both the retriever and the language model in downstream tasks (Shi et al., 2023; Yu et al., 2023b). On the other hand, sentence-level in- dexing provides a finer-grained approach but does not entirely address the issue (Akkalyoncu Yilmaz et al., 2019; Yang et al., 2020). This is because sen- tences can still be complex and compounded, and they are often not self-contained, lacking necessary contextual information (e.g., in the example of Ta- ble 1, "the tower" is coreference of "Pisa Tower") for judging the query-document relevance.
line 84:
line 85: To address these shortcomings of typical re- trieval units such as passages or sentences, we propose using proposition as a novel retrieval unit for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulat- ing a distinct factoid and presented in a concise, self-contained natural language format. We show an example proposition in Table 1. The proposi- tion describes the information regarding the Tower of Pisa's current leaning angle in a self-contained way and precisely responds to what the question is querying. We provide a more detailed definition and description of proposition in 2.
line 86:
line 87: To validate the efficacy of using proposition as a retrieval unit for dense retrievers inference, we first process and index an English Wikipedia dump with all documents segmented into propositions, which we refer to as FACTOIDWIKI. Then we con- duct experiments on five different open-domain QA datasets and empirically compare the performance of six dual-encoder retrievers when Wikipedia is indexed by passage, sentence, and our proposed proposition. Our evaluation is twofold: we exam- ine both the retrieval performance and the impact on downstream QA tasks. Notably, our findings in- dicate that proposition-based retrieval outperforms sentence and passage-based methods, especially in terms of generalization, as discussed in 5. This suggests that propositions, being both compact and rich in context, enable dense retrievers to access precise information while maintaining adequate context. The average improvement over passage- based retrieval of Recall@20 is +10.1 on unsu- pervised dense retrievers and +2.2 on supervised retrievers. Furthermore, we observe a distinct ad- vantage in downstream QA performance when us- ing proposition-based retrieval, as elaborated in 6. Given the often limited input token length in lan- guage models, propositions inherently provide a higher density of question-relevant information.
line 88:
line 89: Our main contributions in the paper are:
line 90:
line 91:  We propose using propositions as retrieval units when indexing a retrieval corpus to improve the dense retrieval performance.
line 92:
line 93:  We introduce FACTOIDWIKI, a processed En- glish Wikipedia dump, where each page is seg- mented into multiple granularities: 100-word pas- sages, sentences and propositions.
line 94:
line 95:  We discover that retrieval by proposition outper- forms passage or sentence retrieval in terms of generalization for passage retrieval and accuracy for downstream question-answering given the same input token limit.
line 96:
line 97:
line 98: # 2 Proposition as a Retrieval Unit
line 99:
line 100: The goal of our study is to understand how the gran- ularity of a retrieval corpus influences the dense retrieval models' performance empirically. Aside from commonly-used retrieval units such as 100- word passage (Karpukhin et al., 2020) or sentence, we propose using proposition as an alternative re- trieval unit choice. Here, propositions represent atomic expressions of meanings in text (Min et al., 2023) that are defined by the three principles below.
line 101:
line 102: 1\. Each proposition should correspond to a distinct piece of meaning in text, where the composition of all propositions would represent the seman- tics of the entire text.
line 103:
line 104: 2\. A proposition should be minimal, i.e. it cannot be further split into separate propositions.
line 105:
line 106: 3\. A proposition should be contextualized and self- contained (Choi et al., 2021). A proposition should include all the necessary context from the text (e.g. coreference) to interpret its meaning.
line 107:
line 108: The use of proposition as a retrieval unit is inspired by a recent line of work (Min et al., 2023; Kamoi et al., 2023; Chen et al., 2023a,b), which finds suc- cess in representing and evaluating text semantics at the level of propositions. We demonstrate the concept of proposition and how a passage can be split into its set of propositions by an example on the left side of Figure 2. The passage contains three propositions, each of which corresponds to a distinct factoid about the Leaning Tower of Pisa: the angle before the restoration, the current an- gle, and the horizontal displacement. Within each proposition, necessary context from the passage is incorporated so that the meaning of the proposition can be interpreted independently of the original text, e.g. the reference of the tower is resolved into its full mention, the Leaning Tower of Pisa, in the first proposition. We expect each proposition to de- scribe exactly one contextualized atomic fact, and so our intuition is that propositions would suitably work as a retrieval unit for information-seeking questions.
line 109:
line 110:
line 111: # 3 FACTOID WIKI: Proposition-Level Index and Retrieval for Wikipedia
line 112:
line 113: We empirically compare the use of 100-word pas- sages, sentences, and propositions as retrieval units on Wikipedia, a commonly-used retrieval source for knowledge-intensive NLP tasks (Petroni et al., 2021). To allow for a fair comparison across gran- ularities, we process an English Wikipedia dump from 2021-10-13, as used by Bohnet et al. (2022). We segment each document text into three different granularities: 100-word passages, sentences, and propositions. We include the details on passage- and sentence-level segmentation of the corpus in Appendix A.
line 114:
line 115: Parsing Passage to Propositions. To segment the Wikipedia pages into propositions, we finetune a text generation model, which we refer to as the Propositionizer. The Propositionizer takes a pas- sage as input and generates the list of propositions within the passage. Following Chen et al. (2023b), we train the Propositionizer with a two-step distil- lation process. We first prompt GPT-4 (OpenAI, 2023) with an instruction containing the propo- sition definition and 1-shot demonstrations. We include the details of the prompt in Figure 8. We start with a set of 42k passages and use GPT-4 to generate the seed set of paragraph-to-propositions pairs. Next, we use the seed set to finetune a Flan- T5-large model (Chung et al., 2022).
line 116:
line 117: We refer to the processed corpus as FACTOID- WIKI. The resulting statistics of FACTOID WIKI are shown in Table 1.
line 118:
line 119: | | # units | Avg. # words |
line 120: | - | - | - |
line 121: | Passage | 41,393,528 | 58.5 |
line 122: | Sentence | 114,219,127 | 21.0 |
line 123: | Proposition | 256,885,003 | 11.2 |
line 124:
line 125: Table 1: Statistics of text units in the English Wikipedia dump from 2021-10-13.
line 126:
line 127:
line 128: # 4 Experimental Settings
line 129:
line 130: To evaluate the impact of the three retrieval unit choices, we conduct experiments on five differ- ent open-domain QA datasets with FACTOIDWIKI. With each dataset, we evaluate both passage re- trieval and downstream QA performance when dense retrievers work with Wikipedia indexed in different granularities.
line 131:
line 132: ## 4.1 Open-Domain QA Datasets
line 133:
line 134: We evaluate on five different open-domain QA datasets with Wikipedia as the retrieval source: Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (TQA) (Joshi et al., 2017), Web Questions (WebQ) (Berant et al., 2013), SQUAD (Rajpurkar et al., 2016), and Entity Ques- tions (EQ) (Sciavolino et al., 2021).
line 135:
line 136:
line 137: ## 4.2 Dense Retrieval Models
line 138:
line 139: We compare the performance of the six following supervised or unsupervised dense retriever mod- els. Here, supervised models refer to ones that have used human-labeled query-passage pairs as supervision during training, and vice versa.
line 140:
line 141:  SimCSE (Gao et al., 2021) is a BERT-base (De- vlin et al., 2019) encoder trained on unlabeled sentence randomly sampled from Wikipedia.
line 142:
line 143:  Contriever (Izacard et al., 2022) is an unsuper- vised retriever, instantiated with a BERT-base encoder. Contriever is contrastively trained by segment pairs constructed from unlabeled docu- ments from Wikipedia and web crawl data.
line 144:
line 145:  DPR (Karpukhin et al., 2020) is a dual-encoder BERT-base model finetuned on five open-domain QA datasets, which includes four of the datasets (NQ, TQA, WebQ and SQUAD) in our evalua- tion.
line 146:
line 147:  ANCE (Xiong et al., 2020) used the same setting from DPR and trained the model with Approxi- mate nearest neighbor Negative Contrastive Esti- mation (ANCE), a hard negatives-based training approach.
line 148:
line 149:  TAS-B (Hofsttter et al., 2021) is a dual- encoder BERT-base model distilled from a teacher model with cross-attention trained on MS MARCO (Nguyen et al., 2016).
line 150:
line 151:  GTR (Hofsttter et al., 2021) is a T5-base en- coder (Raffel et al., 2020) pretrained on unla- beled pairs of online forum QA data, and fine- tuned on MS MARCO and Natural Question.
line 152:
line 153:
line 154: ## 4.3 Passage Retrieval Evaluation
line 155:
line 156: We evaluate retrieval performance at the passage level when the corpus is indexed at the passage, sentence, or proposition level respectively. For sen- tence and proposition level retrieval, we follow the setting introduced in Lee et al. (2021b), where the score of the passage is based on the maximum sim- ilarity score between the query and all sentences
line 157:
line 158: or propositions in a passage. In practice, we first retrieve a slightly larger number of text units, map each unit to the source passage, and return the top-k unique passages. We use Recall@k as our evalu- ation metric, which is defined as the percentage of questions for which the correct answer is found within the top-k retrieved passages.
line 159:
line 160:
line 161: ## 4.4 Downstream QA Evaluation
line 162:
line 163: To understand the implications of using different retrieval units on the downstream open-domain QA tasks, we evaluate the use of retrieval mod- els in retrieve-then-read setup (Izacard and Grave, 2021). With the retrieve-then-read setting, a re- trieval model first retrieves k text units given the query. The k retrieved text units are then used as input along with the query to a reader model to derive the final answer. Typically, the choice of k is subject to the reader model's maximum input length constraint, or the limit of compute budget, which scales with the number of input tokens.
line 164:
line 165: For this reason, we follow an evaluation setup where the maximum number of retrieved words is capped at l = 100 or 500, i.e. only the top l words from passage, sentence, or proposition level retrieval are feed into the reader model as input. We evaluate the percentage of questions for which the predicted answer exactly matches (EM) the ground truth. We denote our metric as EM @ l. For our evaluation, we use T5-large size UnifiedQA-v2 as the reader model (Khashabi et al., 2022).
line 166:
line 167:
line 168: # 5 Results: Passage Retrieval
line 169:
line 170: In this section, we report and discuss the retrieval tasks performance. Our results show that despite none of the models being trained with proposition- level data, all the retrieval models demonstrated on-par or superior performance when the corpus is indexed at the proposition level.
line 171:
line 172:
line 173: ## 5.1 Passage Retrieval Performance
line 174:
line 175: We report our evaluation results in Table 2. We observe that retrieval by propositions outperforms retrieval by sentence or passage on most tasks for both unsupervised and supervised retrievers.
line 176:
line 177: With all dense retrievers tested, proposition- level retrieval consistently outperforms sentence and passage-level retrieval on average across the five datasets. With the unsupervised retrievers, i.e. SimCSE and Contriever, we see an averaged Re- call@5 improvement of +12.0 and +9.3 (35.0%
line 178:
line 179: | Retriever | Granularity | NO || TA || WebQ || SQUAD || EQ || Avg. ||
line 180: |||| R@5 R@20  || R@5 R@20  || R@5 R@20  || R@5 R@20  || R@5 R@20  || R@5 R@20  |
line 181: | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
line 182: | Unsupervised Dense Retrievers ||||||||||||||
line 183: | SimCSE | Passage | 28.8 | 44.3 | 44.9 | 59.4 | 39.8 | 56.0 | 29.5 | 45.5 | 28.4 | 40.3 | 34.3 | 49.1 |
line 184: || Sentence | 35.5 | 53.1 | 50.5 | 64.3 | 45.3 | 64.1 | 37.1 | 52.3 | 36.3 | 50.1 | 40.9 | 56.8 |
line 185: || Proposition | 41.1 | 58.9 | 52.4 | 66.5 | 50.0 | 66.8 | 38.7 | 53.9 | 49.5 | 62.2 | 46.3 | 61.7 |
line 186: | Contriever | Passage | 42.5 | 63.8 | 58.1 | 73.7 | 37.1 | 60.6 | 40.8 | 59.8 | 36.3 | 56.3 | 43.0 | 62.8 |
line 187: || Sentence | 46.4 | 66.8 | 60.6 | 75.7 | 41.7 | 63.1 | 45.1 | 63.5 | 42.7 | 61.3 | 47.3 | 66.1 |
line 188: || Proposition | 50.1 | 70.0 | 65.1 | 77.9 | 45.9 | 66.8 | 50.7 | 67.7 | 51.7 | 70.1 | 52.7 | 70.5 |
line 189: | | Supervised Dense Retrievers |||||||||||||
line 190: | DPR | Passage | 66.0 | 78.0 | 71.6 | 80.2 | 62.9 | 74.9 | 38.3 | 53.9 | 47.5 | 60.4 | 57.3 | 69.5 |
line 191: || Sentence | 66.0 | 78.0 | 71.8 | 80.5 | 64.1 | 74.4 | 40.3 | 55.9 | 53.7 | 66.0 | 59.2 | 71.0 |
line 192: || Proposition | 65.4 | 77.7 | 70.7 | 79.6 | 62.8 | 75.1 | 41.4 | 57.2 | 59.4 | 71.3 | 59.9 | 72.2 |
line 193: | ANCE | Passage | 70.7 | 81.4 | 73.9 | 81.4 | 65.7 | 77.2 | 43.3 | 58.6 | 57.0 | 69.1 | 62.1 | 73.5 |
line 194: || Sentence | 70.3 | 81.6 | 73.9 | 81.5 | 65.2 | 77.4 | 45.8 | 60.7 | 61.4 | 72.8 | 63.3 | 74.8 |
line 195: || Proposition | 69.9 | 81.1 | 72.8 | 80.6 | 65.1 | 77.1 | 46.2 | 61.9 | 66.7 | 76.6 | 64.1 | 75.5 |
line 196: | TAS-B | Passage | 64.2 | 77.9 | 70.4 | 79.3 | 65.1 | 77.0 | 54.3 | 69.2 | 72.2 | 81.3 | 65.2 | 76.9 |
line 197: || Sentence | 64.0 | 78.4 | 71.4 | 80.2 | 63.9 | 76.7 | 58.9 | 72.3 | 72.7 | 82.0 | 66.2 | 77.9 |
line 198: || Proposition | 63.8 | 78.6 | 71.4 | 80.0 | 63.8 | 76.8 | 59.8 | 73.4 | 75.1 | 83.3 | 66.8 | 78.4 |
line 199: | GTR | Passage | 66.3 | 78.4 | 70.1 | 79.4 | 63.3 | 76.5 | 54.4 | 68.1 | 71.7 | 80.5 | 65.2 | 76.6 |
line 200: || Sentence | 66.4 | 79.4 | 71.6 | 80.9 | 62.2 | 76.8 | 60.9 | 73.4 | 72.5 | 81.3 | 66.7 | 78.4 |
line 201: || Proposition | 66.5 | 79.6 | 72.2 | 80.9 | 63.2 | 77.4 | 63.3 | 75.0 | 74.9 | 83.0 | 68.0 | 79.2 |
line 202:
line 203: Table 2: Passage retrieval performance (Recall@k = 5, 20) on five different open-domain QA datasets when pre-trained dense retrievers work with the three different granularity from the retrieval corpus. Underline denotes cases where the training split of the target dataset was included in the training data of the dense retriever.
line 204:
line 205: <figure>
line 206:
line 207: ![](figures/2)
line 208:
line 209: <!-- FigureContent="\- Passage -- Sentence -- Proposition SimCSE Contriever DPR ANCE TAS-B 60 GTR 60 70 80 80 Recall@5 Recall@5 Recall@5 60 Recall@5 Recall@5 Recall@5 40 60 70 70 40 50 50 60 60 20 20 40 40 50 50 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 Popularity Popularity Popularity Popularity Popularity Popularity" -->
line 210:
line 211: <figcaption>
line 212:
line 213: Figure 3: Document retrieval recall vs. the popularity of the target entity in each question from the EntityQuestions
line 214: dataset. The popularity of each entity (i.e. smaller value = less common entities, and vice versa) is estimated by
line 215: the occurrence of the entity in its top-1000 passage retrieved by BM25. On queries with less common entities, we
line 216: observe that retrieving by proposition shows a larger advantage over retrieval by proposition.
line 217:
line 218: </figcaption>
line 219:
line 220: </figure>
line 221:
line 222:
line 223: and 22.5% relative improvement) respectively over five datasets.
line 224:
line 225: With the supervised retrievers, proposition-level retrieval still shows an advantage on average, yet the sizes of improvements are smaller. We hypothe- size that this is due to these retrievers being trained on query-passage pairs. For instance, with DPR and ANCE, which have been trained on NQ, TQA, WebQ, and SQUAD, we observe that proposition and sentence level retrieval perform slightly worse compared to passage level on three out of the four datasets, with the exception of SQUAD. As shown in Table 2, all supervised retrievers demonstrate comparable performance across three levels of re- trieval granularity in NQ, TQA, and WebQ.
line 226:
line 227: However, on datasets that the retriever model has not seen during training, we observe that retrieval
line 228:
line 229: by proposition demonstrates a clear advantage. For instance, most notably on SQUAD or EntityQues- tions, we observe that proposition-based retrieval significantly outperforms the other two granulari- ties. We see 17-25% Recall@5 relative improve- ment on EntityQuestions with relatively weak re- trievers like DPR and ANCE. Furthermore, the Recall@5 of retrieval by proposition on SQUAD improved most on TAS-B and GTR, with 10-16% relative improvements.
line 230:
line 231:
line 232: ## 5.2 Retrieval by Proposition => Better Cross-Task Generalization
line 233:
line 234: Our results indicate that the advantage of retrieval by proposition becomes most visible in cross- task generalization settings. We observe that on SQUAD and EntityQuestions, retrieval by proposi-
line 235: :selected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :unselected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :selected: :selected: :unselected: :unselected: :selected: :unselected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected:
line 236: | Retriever | Granularity | NQ || TQA || WebQ || SQUAD || EQ || Avg. ||
line 237: ||| EM || EM || EM || EM || EM || EM ||
line 238: |||| @100 @500  | @100 | @500 | @100 | @500 | @100 | @500 | @100 | @500 || @100 @500  |
line 239: | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
line 240: | | Unsupervised Dense Retrievers |||||||||||||
line 241: | SimCSE | Passage | 8.1 | 16.3 | 22.6 | 33.7 | 7.7 | 14.9 | 9.8 | 17.8 | 10.9 | 17.5 | 11.8 | 20.0 |
line 242: || Sentence | 10.1 | 18.0 | 27.2 | 37.2 | 9.6 | 15.6 | 17.3 | 24.8 | 13.0 | 19.8 | 15.4 | 23.1 |
line 243: || Proposition | 12.7 | 20.2 | 28.4 | 37.7 | 11.2 | 17.2 | 18.0 | 25.1 | 18.3 | 25.0 | 17.7 | 25.0 |
line 244: | Contriever | Passage | 11.1 | 22.4 | 25.7 | 41.4 | 6.8 | 14.9 | 15.6 | 27.7 | 10.9 | 21.5 | 14.0 | 25.6 |
line 245: || Sentence | 13.8 | 23.9 | 30.5 | 44.2 | 9.1 | 17.2 | 22.6 | 32.8 | 12.2 | 22.2 | 17.6 | 28.1 |
line 246: || Proposition | 16.5 | 26.1 | 37.7 | 48.7 | 13.3 | 19.9 | 25.6 | 34.4 | 16.1 | 27.3 | 21.8 | 31.3 |
line 247: | | Supervised Dense Retrievers |||||||||||||
line 248: | DPR | Passage | 24.8 | 36.1 | 40.3 | 51.0 | 14.0 | 22.2 | 12.4 | 21.7 | 18.6 | 25.9 | 22.0 | 31.4 |
line 249: || Sentence | 27.6 | 35.9 | 44.6 | 52.8 | 16.3 | 23.7 | 18.6 | 26.1 | 21.8 | 28.2 | 25.8 | 33.3 |
line 250: || Proposition | 28.3 | 34.3 | 45.7 | 51.9 | 19.0 | 23.8 | 19.8 | 26.3 | 26.3 | 31.9 | 27.8 | 33.6 |
line 251: | ANCE | Passage | 27.1 | 38.3 | 43.1 | 53.1 | 15.2 | 23.0 | 15.3 | 26.0 | 23.4 | 31.1 | 24.8 | 34.3 |
line 252: || Sentence | 30.1 | 37.3 | 47.0 | 54.7 | 16.6 | 23.8 | 22.9 | 30.5 | 25.9 | 32.0 | 28.5 | 35.7 |
line 253: || Proposition | 29.8 | 37.0 | 47.4 | 53.5 | 19.3 | 24.1 | 22.9 | 30.1 | 29.1 | 33.7 | 29.7 | 35.7 |
line 254: | TAS-B | Passage | 21.1 | 33.9 | 39.3 | 50.5 | 13.1 | 20.7 | 23.9 | 34.6 | 30.9 | 37.3 | 25.7 | 35.4 |
line 255: || Sentence | 24.6 | 33.9 | 43.6 | 52.3 | 14.4 | 21.4 | 33.8 | 40.5 | 31.4 | 36.1 | 29.6 | 36.8 |
line 256: || Proposition | 26.6 | 34.0 | 44.9 | 51.8 | 18.1 | 23.7 | 34.2 | 38.9 | 34.2 | 37.8 | 31.6 | 37.2 |
line 257: | GTR | Passage | 23.4 | 34.5 | 38.7 | 49.3 | 13.1 | 20.1 | 23.9 | 33.8 | 31.3 | 36.7 | 26.1 | 34.9 |
line 258: || Sentence | 26.8 | 35.1 | 43.9 | 52.2 | 15.9 | 21.6 | 35.6 | 41.3 | 31.3 | 35.1 | 30.7 | 37.1 |
line 259: || Proposition | 29.5 | 34.4 | 45.9 | 52.6 | 18.7 | 23.8 | 37.0 | 40.4 | 34.1 | 37.1 | 33.0 | 37.7 |
line 260:
line 261: Table 3: Open-domain QA performance (EM = Exact Match) under retrieve-then-read setting where the number of retrieved words to the reader QA model is limited at l = 100 or 500. We use UnifedQA V2 (Khashabi et al., 2022) as the reader model. The first l words from the concatenated top retrieved text unit are feed as input to the reader model. Underline denotes cases where the training split of the target dataset was included in the training data of the dense retriever. In most cases, we see better QA performance with smaller retrieval units.
line 262:
line 263: tion brings more performance gain over retrieval by passage.
line 264:
line 265: To better understand where the improvements can be attributed, we conduct an additional analysis on EntityQuestions. As EntityQuestions features questions targeting the properties of longer-tail enti- ties, we study how the retrieval performance under three different granularities is affected by the popu- larity of the target entity in question, i.e. whether the entity appears frequently in Wikipedia or not. We estimate the popularity of each entity with the following method. Given the surface form of an en- tity, we use BM25 to retrieve the top-1000 relevant passages from Wikipedia. We use the number of occurrences of the entity in its top-1000 passages as an estimate of its popularity. With the 20,000 test queries in EntityQuestion, around 25% of the target entities have a popularity value of less or equal to 3.
line 266:
line 267: Figure 3 shows the passage retrieval perfor- mance vs. the popularity of the target entity in each question. Across all 6 dense retrievers, we ob- serve that retrieving by proposition shows a much larger advantage over retrieving by passage with questions targeting less common entities. As the popularity of entities increases, the performance
line 268:
line 269: gap decreases. Our findings indicate that the per- formance gain from retrieval by proposition can mostly be attributed to queries for long-tailed infor- mation. This echoes our observation that retrieval by proposition improves the cross-task generaliza- tion performance of dense retrievers.
line 270:
line 271:
line 272: # 6 Results: Open-Domain QA
line 273:
line 274: In this section, we study how the choice of retrieval granularity affects downstream open-domain QA tasks. We show that retrieval by proposition leads to strong downstream QA performance in the retrieve-then-read setting, where the number of re- trieved tokens for input to the reader QA model is capped at l = 100 or 500 words.
line 275:
line 276:
line 277: ## 6.1 QA Performance
line 278:
line 279: Table 3 shows the evaluation results. Across dif- ferent retrievers, we observe higher QA perfor- mance in terms of the EM@l metric on average when using propositions as the retrieval unit. The unsupervised retrievers, SimCSE and Contriever, demonstrate improvements of +5.9 and +7.8 in the EM@100 score (50% and 55% relative im- provement), respectively. The supervised retriev- ers, DPR, ANCE, TAS-B, and GTR, improve +5.8,
line 280:
line 281: Passage
line 282:
line 283: Sentence
line 284:
line 285: \-
line 286:
line 287: Proposition
line 288:
line 289: <figure>
line 290:
line 291: ![](figures/3)
line 292:
line 293: <!-- FigureContent="GTR / NQ GTR / TQA GTR / WebQ GTR / SQUAD GTR / EQ 70 70 80 Recall (%) Recall (%) 70 Recall (%) Recall (%) 60 Recall (%) 60 60 70 60 50 50 50 40 60 0 200 400 0 200 400 0 200 400 0 200 400 0 200 400 #Words #Words #Words #Words #Words" -->
line 294:
line 295: <figcaption>
line 296:
line 297: Figure 4: Recall of the gold answer in the retrieved text limited to first k words for the GTR retriever. Finer-grained
line 298: retrieval has a higher recall across all numbers of words.
line 299:
line 300: </figcaption>
line 301:
line 302: </figure>
line 303:
line 304:
line 305: \+4.9, +5.9, and +6.9 EM@100 (26%, 19%, 22%, 26% relative improvement), respectively. Similar to our observations from passage retrieval evalu- ations, we find retrieval by proposition becomes more beneficial to downstream QA performance when the retriever has not been trained on the target dataset. In other cases, retrieval by proposition still holds an advantage, but with a smaller margin on average.
line 306:
line 307:
line 308: ## 6.2 Propositions => Higher Density of Question-Related Information
line 309:
line 310: Intuitively, compared to sentences or passages as retrieval units, the advantage of propositions is that the retrieved propositions have a higher density of relevant information to the query. With finer- grained retrieval units, the correct answer to the query would more likely appear in the top-l re- trieved words by a dense retriever.
line 311:
line 312: We illustrate this phenomenon by an analysis shown in Figure 4. Here, we investigate the posi- tion at which the ground truth answer appears in the top-l retrieved words. Specifically, we calcu- late the recall of the gold answer within the initial l retrieved words with GTR working with Wikipedia indexed in three different granularities.
line 313:
line 314: We show the results in Figure 4 and Figure 7 with l ranging from 0 to 500 across all five datasets. For a fixed retrieved word budget, proposition retrieval demonstrates a higher success rate compared to sentence and passage retrieval methods. The most significant improvement of proposition retrieval over passage retrieval occurs within the range of 100-200 words, which corresponds to roughly 10 propositions, 5 sentences, or 2 passages. As the word count further increases, the recall rates of the three granularity converge since all question- relevant information is included in the retrieved text.
line 315:
line 316:
line 317: ## 6.3 Error Case Study
line 318:
line 319: To understand the source of errors from each type of retrieval granularity, we present and discuss four typical examples of mistakes in Table 4. With each example, we show the question and its correspond- ing top-1 retrieved text unit by the GTR retriever across the three granularities.
line 320:
line 321: We observe that with passage-level retrieval, the ambiguity of an entity or its references presents a challenge for dense retrievers, which echoes find- ings from (Min et al., 2020). For instance, in exam- ple Q1, the question asks for "Super Bowl 50", but the retrieved passage and sentence refers to "Super Bowl 5". In Example Q2, passage retrieval fails to identify the part referring to the correct "atomic number". Instead, the top-1 retrieved passage men- tions "atomic number" in a different and irrelevant context to the question. Retrieval by sentences can also have a similar problem as retrieval by passages like Example Q1. Also, retrieval by sentences faces another challenge of lacking context. In Example Q3, sentence-based retrieval fails as the correct sen- tence in the retrieved passage uses "it" to refer to the pericardial sac.
line 322:
line 323: Retrieval by propositions tackles the aforemen- tioned problems by ensuring each retrieval unit contains one piece of fact only and necessary con- text is incorporated in the propositions. However, proposition-based retrieval faces challenges with questions that involve multi-hop reasoning over long-range textual analysis. In Example Q4, the retrieved passage separately describes the actor's name and the character they portray. There is not a single proposition that entails both the question and the answer.
line 324:
line 325:
line 326: # 7 Related Work
line 327:
line 328: Recent works on dense retrievers typically adopt a dual-encoder architecture (Yih et al., 2011; Reimers and Gurevych, 2019; Karpukhin et al., 2020; Ni et al., 2022). With dual-encoders,
line 329: :unselected: :unselected: :unselected:
line 330: | Passage Retrieval | Sentence Retrieval | Proposition Retrieval |
line 331: | - | - | - |
line 332: | Q1: What was the theme of Super Bowl 50? | | |
line 333: | Title: Super Bowl X  The overall theme of the Super Bowl enter- tainment was to celebrate the United States Bicentennial. Each Cowboys and Steelers player wore a special patch with the Bicen- tennial logo on their jerseys ... :selected: :selected: | Title: Super Bowl X  The overall theme of the Super Bowl entertainment was to celebrate the United States Bicentennial. :selected: :selected: | Title: Super Bowl XLV  ... As this was the 50th Super Bowl game, the league [Super Bowl 50] emphasized the "golden anniversary" with various gold- themed initiatives during the 2015 season, as well as ... :selected: |
line 334: | Q2: The atomic number of indium which belongs to 5th period is? |||
line 335: | Title: Period 5 element  The periodic table is laid out in rows to illus- trate recurring (periodic) trends in the chemi- cal behaviour of the elements as their atomic number increases: ... :selected: | Title: Period 5 element  Indium is a chemical element with the symbol In and atomic number 49. :selected: | Title: Period 5 element  Indium is a chemical element with the sym- bol In and [Indium has a] atomic number 49. This rare, very soft, malleable ... :selected: |
line 336: | Q3: What is the function of the pericardial sac? |||
line 337: | Title: Pericardium  The pericardium, also called pericardial sac ... It separates the heart from interference of other structures, protects it against infection :selected: | Title: Pericardium  The pericardium, also called pericar- dial sac, is a double-walled sac con- taining the heart and the roots of the great vessels. :selected: | Title: Cardiac muscle  On the outer aspect of the myocardium is the epicardium which forms part of the pericar- dial sac that surrounds, protects, and lubri- :selected: |
line 338: | and blunt trauma, and lubricates the heart's || cates the heart. |
line 339: | movements. || |
line 340: | Q4: What is the main character's name in layer cake? |||
line 341: | Title: Layer Cake (film)  ... The film's plot revolves around a London- based criminal, played by Daniel Craig, ... Craig's character is unnamed in the film and is listed in the credits as "XXXX". :selected: | Title: Angelic Layer  The primary protagonist is Misaki Suzuhara. :selected: | Title: Plot twist  Sometimes the audience may discover that the true identity of a character is , in fact, unknown [in Layer Cake] , as in Layer Cake or the eponymous assassins in V for Vendetta and The Day of the Jackal. :selected: |
line 342:
line 343: Table 4: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correct answer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration purpose only and not provided to the retrievers and downstream QA models.
line 344:
line 345: each query and document is encoded into a low- dimensional feature vector respectively, and their relevance is measured by a non-parametric similar- ity function between the embedding vectors (Muss- mann and Ermon, 2016). Due to the limited expres- sivity from the similarity function, dual encoder models often generalize poorly to new tasks with scarce training data (Thakur et al., 2021). To this end, previous studies use techniques such as data augmentation (Wang et al., 2022; Yu et al., 2023a; Izacard et al., 2022; Gao and Callan, 2022; Lin et al., 2023; Dai et al., 2023), continual pre-training (Chang et al., 2020; Sachan et al., 2021; Oguz et al., 2022), task-aware training (Xin et al., 2022; Cheng et al., 2023), hybrid sparse-dense retrieval (Luan et al., 2021; Chen et al., 2022) or mixed strategy re- trieval (Ma et al., 2022, 2023) and so on to improve cross-task generalization performance of dense re- trievers.
line 346:
line 347: The motivation of our work echoes in part with multi-vector retrieval, e.g. ColBERT (Khattab and Zaharia, 2020), DensePhrase (Lee et al., 2021a,b), ME-BERT (Luan et al., 2021), MVR (Zhang et al., 2022), where the retrieval model learns to encode
line 348:
line 349: a candidate retrieval unit into multiple vectors to increase model expressivity and improve retrieval granularity (Seo et al., 2019; Humeau et al., 2019). Our work instead focuses on the setting where we do not update the dense retriever model or its pa- rameters. We show that segmenting the retrieval corpus into finer-grained units of proposition can be a simple and orthogonal strategy for improving the generalization of dual encoders dense retrievers at inference time.
line 350:
line 351: The idea of using propositions as a unit of text representation dates back to the Pyramid method in summarization evaluation (Nenkova and Passon- neau, 2004), where model generated summary is evaluated by each proposition. Proposition extrac- tion from text has been a long-standing task in NLP, with earlier formulations focusing on a structured representation of propositions, e.g. Open Infor- mation Extraction (Etzioni et al., 2008), Semantic Role Labeling (Gildea and Jurafsky, 2000). More recent studies have found success in extracting propositions in natural language form via few-shot prompting with large language models (Min et al., 2023; Kamoi et al., 2023), or finetuning smaller
line 352:
line 353: compact-sized models (Chen et al., 2023b).
line 354:
line 355: Retrieve-then-read, or more broadly - retrieval augmented generation, has recently merged as a popular paradigm for open-domain question an- swering (Lewis et al., 2021; Jiang et al., 2023; Asai et al., 2023). While earlier works provide up to the top 100 retrieved passages for the down- stream reader (Izacard and Grave, 2021; Kedia et al., 2022), the amount of allowed context is significantly reduced when using recent large lan- guage models (e.g. top 10) (Touvron et al., 2023; Yu et al., 2023b), due to their limited context win- dow length and inability to reason over long con- text (Liu et al., 2023). Recent efforts have tried to improve the quality of the reader context by filter- ing or compressing the retrieved documents (Wang et al., 2023; Xu et al., 2023). Our work offers a new perspective by leveraging a new retrieval unit, the proposition that not only reduces the context length but also offers greater information density, effectively addressing the issue.
line 356:
line 357:
line 358: # 8 Conclusion
line 359:
line 360: We propose the use of propositions as retrieval units for indexing corpus to improve dense retrieval per- formance at inference time. Through our experi- ments on five open-domain QA datasets with six different dense retrievers, we discovered that re- trieval by proposition outperforms passage or sen- tence in both passage retrieval accuracy and down- stream QA performance with a fixed retrieved word budget. We introduce FACTOIDWIKI, an indexed version of the English Wikipedia dump, where text from 6 million pages is segmented into 250 million propositions. We hope that FACTOIDWIKI, along with our findings in the paper, will facilitate future research on information retrieval.
line 361:
line 362:
line 363: # Limitations
line 364:
line 365: The scope of our current study on the granular- ity of retrieval corpus has the following limita- tions. (1) Retrieval Corpus - Our study only focus on Wikipedia as the retrieval corpus, due to the fact that most open-domain QA datasets adopts Wikipedia as the retrieval corpus. (2) Types of dense retrievers evaluated - In the current version of the paper, we only evaluate on 6 types of popular dense retrievers, most of which follow bi- or dual- encoder architecture. In future versions, we will include and discuss results on a broader range of dense retrievers. (3) Language - Our current study
line 366:
line 367: is limited to English Wikipedia only. We leave the exploration on other languages to future work.
line 368:
line 369:
line 370: # References
line 371:
line 372: Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Cross-domain modeling of sentence-level evidence for document retrieval. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3490- 3496, Hong Kong, China. Association for Computa- tional Linguistics.
line 373:
line 374: Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to re- trieve, generate, and critique through self-reflection.
line 375:
line 376: Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533-1544.
line 377:
line 378: Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037.
line 379:
line 380: Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yim- ing Yang, and Sanjiv Kumar. 2020. Pre-training tasks for embedding-based large-scale retrieval. In Inter- national Conference on Learning Representations.
line 381:
line 382: Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan Roth, and Tal Schuster. 2023a. PropSegmEnt: A large-scale corpus for proposition-level segmentation and entailment recognition. In Findings of the As- sociation for Computational Linguistics: ACL 2023, pages 8874-8893, Toronto, Canada. Association for Computational Linguistics.
line 383:
line 384: Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou, Wenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang, Dan Roth, and Dong Yu. 2023b. Sub-sentence en- coder: Contrastive learning of propositional semantic representations. arXiv preprint arXiv:2311.04335.
line 385:
line 386: Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. 2022. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 250-262, Abu Dhabi, United Arab Emi- rates. Association for Computational Linguistics.
line 387:
line 388: Hao Cheng, Hao Fang, Xiaodong Liu, and Jianfeng Gao. 2023. Task-aware specialization for efficient and robust dense retrieval for open-domain question
line 389:
line 390: answering. In Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1864-1875, Toronto, Canada. Association for Computational Linguistics.
line 391:
line 392: Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. 2021. Decontextualization: Making sen- tences stand-alone. Transactions of the Association for Computational Linguistics, 9:447-461.
line 393:
line 394: Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.
line 395:
line 396: Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few- shot dense retrieval from 8 examples. In The Eleventh International Conference on Learning Representa- tions.
line 397:
line 398: Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.
line 399:
line 400: Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extrac- tion from the web. Communications of the ACM, 51(12):68-74.
line 401:
line 402: Luyu Gao and Jamie Callan. 2022. Unsupervised cor- pus aware language model pre-training for dense pas- sage retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 2843-2853, Dublin, Ireland. Association for Computational Lin- guistics.
line 403:
line 404: Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence em- beddings. arXiv preprint arXiv:2104.08821.
line 405:
line 406: Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 512-520, Hong Kong. Association for Computational Linguistics.
line 407:
line 408: Sebastian Hofsttter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Ef- ficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 113-122.
line 409:
line 410: Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019. Poly-encoders: Trans- former architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint arXiv: 1905.01969.
line 411:
line 412: Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas- tian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense informa- tion retrieval with contrastive learning. Transactions on Machine Learning Research.
line 413:
line 414: Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open do- main question answering. In Proceedings of the 16th Conference of the European Chapter of the Associ- ation for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computa- tional Linguistics.
line 415:
line 416: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation.
line 417:
line 418: Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. arXiv preprint arXiv: 1705.03551.
line 419:
line 420: Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. Wice: Real-world entailment for claims in wikipedia. In Proceedings of the 2023 Con- ference on Empirical Methods in Natural Language Processing.
line 421:
line 422: Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open- domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.
line 423:
line 424: Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee. 2022. FiE: Building a global probability space by leverag- ing early fusion in encoder for open-domain question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process- ing, pages 4246-4260, Abu Dhabi, United Arab Emi- rates. Association for Computational Linguistics.
line 425:
line 426: Daniel Khashabi, Yeganeh Kordi, and Hannaneh Ha- jishirzi. 2022. Unifiedqa-v2: Stronger generalization via broader cross-format training. arXiv preprint arXiv:2202.12359.
line 427:
line 428: Omar Khattab and Matei Zaharia. 2020. Colbert: Effi- cient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39- 48.
line 429:
line 430: Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti,
line 431:
line 432: Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken- ton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453- 466.
line 433:
line 434: Jaewoong Lee, Heejoon Lee, Hwanhee Lee, and Ky- omin Jung. 2021a. Learning to select question- relevant relations for visual question answering. In Proceedings of the Third Workshop on Multimodal Artificial Intelligence, pages 87-96, Mexico City, Mexico. Association for Computational Linguistics.
line 435:
line 436: Jinhyuk Lee, Alexander Wettig, and Danqi Chen. 2021b. Phrase retrieval learns passage retrieval, too. In Pro- ceedings of the 2021 Conference on Empirical Meth- ods in Natural Language Processing, pages 3661- 3672, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
line 437:
line 438: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Kttler, Mike Lewis, Wen-tau Yih, Tim Rock- tschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neu- ral Information Processing Systems, 33:9459-9474.
line 439:
line 440: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Kttler, Mike Lewis, Wen tau Yih, Tim Rock- tschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledge- intensive nlp tasks.
line 441:
line 442: Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. How to Train Your DRAGON: Di- verse Augmentation Towards Generalizable Dense Retrieval. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.
line 443:
line 444: Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts.
line 445:
line 446: Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, dense, and attentional representations for text retrieval. Transactions of the Association for Computational Linguistics, 9:329- 345.
line 447:
line 448: Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2022. Open-domain question an- swering via chain of reasoning over heterogeneous knowledge. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 5360- 5374, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
line 449:
line 450: Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2023. Chain-of-skills: A configurable model for open-domain question an- swering. In Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1599-1618, Toronto, Canada. Association for Computational Linguistics.
line 451:
line 452: Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceed- ings of the 2023 Conference on Empirical Methods in Natural Language Processing.
line 453:
line 454: Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering am- biguous open-domain questions. In Proceedings of the 2020 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), pages 5783- 5797, Online. Association for Computational Lin- guistics.
line 455:
line 456: Stephen Mussmann and Stefano Ermon. 2016. Learning and inference via maximum inner product search. In International Conference on Machine Learning, pages 2587-2596. PMLR.
line 457:
line 458: Ani Nenkova and Rebecca Passonneau. 2004. Evaluat- ing content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chap- ter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 145-152, Boston, Mas- sachusetts, USA. Association for Computational Lin- guistics.
line 459:
line 460: Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human gener- ated machine reading comprehension dataset. CoRR, abs/1611.09268.
line 461:
line 462: Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9844-9855, Abu Dhabi, United Arab Emirates. As- sociation for Computational Linguistics.
line 463:
line 464: Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, and Yashar Mehdad. 2022. Domain-matched pre-training tasks for dense retrieval. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1524-1534, Seattle, United States. Association for Computational Linguistics.
line 465:
line 466: OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.
line 467:
line 468: Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktschel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human
line 469:
line 470: Language Technologies, pages 2523-2544, Online. Association for Computational Linguistics.
line 471:
line 472: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. The Journal of Machine Learning Research, 21(1):5485-5551.
line 473:
line 474: Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv: 1606.05250.
line 475:
line 476: Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Com- putational Linguistics.
line 477:
line 478: Devendra Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L. Hamil- ton, and Bryan Catanzaro. 2021. End-to-end training of neural retrievers for open-domain question answer- ing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 6648-6662, Online. Association for Computational Linguistics.
line 479:
line 480: Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. Col- BERTv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 3715-3734, Seat- tle, United States. Association for Computational Linguistics.
line 481:
line 482: Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Simple entity-centric ques- tions challenge dense retrievers. arXiv preprint arXiv:2109.08535.
line 483:
line 484: Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-time open-domain question answering with dense-sparse phrase index. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics, pages 4430-4441, Florence, Italy. Association for Computational Linguistics.
line 485:
line 486: Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schrli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In Inter- national Conference on Machine Learning, pages 31210-31227. PMLR.
line 487:
line 488: Nandan Thakur, Nils Reimers, Andreas Rckl, Ab- hishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Con- ference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).
line 489:
line 490: Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di- ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro- driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine- tuned chat models.
line 491:
line 492: Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 2345-2360, Seattle, United States. Association for Computational Linguistics.
line 493:
line 494: Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023. Learning to filter context for retrieval-augmented generation.
line 495:
line 496: Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita Sharma, Damien Jose, and Paul Bennett. 2022. Zero- shot dense retrieval with momentum adversarial do- main invariant representations. In Findings of the As- sociation for Computational Linguistics: ACL 2022, pages 4008-4020, Dublin, Ireland. Association for Computational Linguistics.
line 497:
line 498: Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor neg- ative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808.
line 499: