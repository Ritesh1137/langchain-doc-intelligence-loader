langchain_doc_intelligence package has been imported
Page 0 lines: [{'content': 'Dense XX Retrieval: What Retrieval Granularity Should We Use?\n===', 'polygon': [1.3486, 1.1153, 6.9102, 1.1407, 6.9102, 1.3485, 1.3486, 1.3283], 'spans': [{'offset': 0, 'length': 65}]}, {'content': 'Tong Chen \\*\\* Hongwei Wang', 'polygon': [2.0432, 1.6578, 4.1472, 1.6679, 4.1472, 1.8555, 2.0432, 1.8454], 'spans': [{'offset': 78, 'length': 27}]}, {'content': 'Sihao Chen', 'polygon': [4.3094, 1.6578, 5.2422, 1.6578, 5.2473, 1.8251, 4.3094, 1.8352], 'spans': [{'offset': 106, 'length': 10}]}, {'content': 'Wenhao Yu"', 'polygon': [5.3284, 1.6679, 6.2562, 1.6629, 6.2613, 1.82, 5.3284, 1.8302], 'spans': [{'offset': 117, 'length': 10}]}, {'content': 'Kaixin Ma', 'polygon': [1.9722, 1.8555, 2.7884, 1.8606, 2.7834, 2.0329, 1.9671, 2.0329], 'spans': [{'offset': 128, 'length': 9}]}, {'content': 'Xinran Zhao^ Hongming Zhang', 'polygon': [2.8797, 1.8606, 5.3943, 1.8707, 5.3943, 2.0482, 2.8797, 2.0329], 'spans': [{'offset': 151, 'length': 27}]}, {'content': 'Dong Yu', 'polygon': [5.5008, 1.8656, 6.241, 1.8656, 6.241, 2.038, 5.5008, 2.038], 'spans': [{'offset': 192, 'length': 7}]}, {'content': '\\*University of Washington', 'polygon': [2.6161, 2.1141, 4.4463, 2.1343, 4.4412, 2.3118, 2.6161, 2.2966], 'spans': [{'offset': 201, 'length': 26}]}, {'content': 'Tencent AI Lab', 'polygon': [4.5426, 2.1191, 5.6783, 2.1191, 5.6783, 2.3016, 4.5426, 2.3016], 'spans': [{'offset': 229, 'length': 14}]}, {'content': 'University of Pennsylvania ^Carnegie Mellon University', 'polygon': [2.1192, 2.3168, 6.094, 2.327, 6.094, 2.5044, 2.1192, 2.4943], 'spans': [{'offset': 258, 'length': 54}]}, {'content': '# Abstract', 'polygon': [2.175, 3.0722, 2.8087, 3.0722, 2.8087, 3.2193, 2.175, 3.2193], 'spans': [{'offset': 326, 'length': 10}]}, {'content': 'Dense retrieval has become a prominent', 'polygon': [1.1914, 3.4119, 3.7872, 3.417, 3.7872, 3.5538, 1.1914, 3.5488], 'spans': [{'offset': 338, 'length': 38}]}, {'content': 'method to obtain relevant context or world', 'polygon': [1.2016, 3.5792, 3.7821, 3.5792, 3.7821, 3.7211, 1.2016, 3.7211], 'spans': [{'offset': 377, 'length': 42}]}, {'content': 'knowledge in open-domain NLP tasks. When', 'polygon': [1.1965, 3.7516, 3.7821, 3.7465, 3.7821, 3.8884, 1.1965, 3.8986], 'spans': [{'offset': 420, 'length': 40}]}, {'content': 'we use a learned dense retriever on a retrieval', 'polygon': [1.2016, 3.9138, 3.7872, 3.9087, 3.7872, 4.0507, 1.2016, 4.0557], 'spans': [{'offset': 461, 'length': 47}]}, {'content': 'corpus at inference time, an often-overlooked', 'polygon': [1.2016, 4.0862, 3.7872, 4.076, 3.7872, 4.218, 1.2016, 4.2281], 'spans': [{'offset': 509, 'length': 45}]}, {'content': 'design choice is the retrieval unit in which the', 'polygon': [1.1965, 4.2433, 3.7872, 4.2433, 3.7872, 4.3853, 1.1965, 4.3954], 'spans': [{'offset': 555, 'length': 48}]}, {'content': 'corpus is indexed, e.g. document, passage, or', 'polygon': [1.2016, 4.4056, 3.7973, 4.4056, 3.7973, 4.5576, 1.2016, 4.5576], 'spans': [{'offset': 604, 'length': 45}]}, {'content': 'sentence. We discover that the retrieval unit', 'polygon': [1.2016, 4.5729, 3.7872, 4.5678, 3.7872, 4.7097, 1.2016, 4.7148], 'spans': [{'offset': 650, 'length': 45}]}, {'content': 'choice significantly impacts the performance', 'polygon': [1.1965, 4.7402, 3.7872, 4.7402, 3.7872, 4.8922, 1.1965, 4.8922], 'spans': [{'offset': 696, 'length': 44}]}, {'content': 'of both retrieval and downstream tasks. Dis-', 'polygon': [1.2016, 4.9075, 3.7973, 4.9075, 3.7973, 5.0545, 1.2016, 5.0545], 'spans': [{'offset': 741, 'length': 44}]}, {'content': 'tinct from the typical approach of using pas-', 'polygon': [1.1965, 5.0697, 3.8024, 5.0748, 3.8024, 5.2268, 1.1965, 5.2218], 'spans': [{'offset': 786, 'length': 45}]}, {'content': 'sages or sentences, we introduce a novel re-', 'polygon': [1.2016, 5.2471, 3.7973, 5.237, 3.7973, 5.3739, 1.2016, 5.3891], 'spans': [{'offset': 832, 'length': 44}]}, {'content': 'trieval unit, proposition, for dense retrieval.', 'polygon': [1.2016, 5.4043, 3.7973, 5.4043, 3.7973, 5.5513, 1.2016, 5.5564], 'spans': [{'offset': 877, 'length': 47}]}, {'content': 'Propositions are defined as atomic expressions', 'polygon': [1.2016, 5.5665, 3.7821, 5.5665, 3.7821, 5.7237, 1.2016, 5.7186], 'spans': [{'offset': 925, 'length': 46}]}, {'content': 'within text, each encapsulating a distinct fac-', 'polygon': [1.2066, 5.7338, 3.8075, 5.7338, 3.8075, 5.8808, 1.2066, 5.8859], 'spans': [{'offset': 972, 'length': 47}]}, {'content': 'toid and presented in a concise, self-contained', 'polygon': [1.2016, 5.9011, 3.7872, 5.896, 3.7872, 6.0431, 1.2016, 6.0532], 'spans': [{'offset': 1021, 'length': 47}]}, {'content': 'natural language format. We conduct an empir-', 'polygon': [1.2016, 6.0684, 3.8024, 6.0684, 3.8024, 6.2154, 1.2016, 6.2154], 'spans': [{'offset': 1069, 'length': 45}]}, {'content': 'ical comparison of different retrieval granular-', 'polygon': [1.2016, 6.2306, 3.7973, 6.2306, 3.7973, 6.3726, 1.2016, 6.3777], 'spans': [{'offset': 1115, 'length': 48}]}, {'content': 'ity. Our results reveal that proposition-based', 'polygon': [1.2016, 6.403, 3.7872, 6.3979, 3.7872, 6.545, 1.2016, 6.55], 'spans': [{'offset': 1164, 'length': 46}]}, {'content': 'retrieval significantly outperforms traditional', 'polygon': [1.2016, 6.5652, 3.7771, 6.5652, 3.7771, 6.7173, 1.2016, 6.7173], 'spans': [{'offset': 1211, 'length': 47}]}, {'content': 'passage or sentence-based methods in dense', 'polygon': [1.2016, 6.7427, 3.7821, 6.7275, 3.7821, 6.8745, 1.2016, 6.8897], 'spans': [{'offset': 1259, 'length': 42}]}, {'content': 'retrieval. Moreover, retrieval by proposition', 'polygon': [1.1965, 6.8948, 3.7771, 6.8948, 3.7771, 7.0469, 1.1965, 7.0418], 'spans': [{'offset': 1302, 'length': 45}]}, {'content': 'also enhances the performance of downstream', 'polygon': [1.2016, 7.0621, 3.7771, 7.0621, 3.7771, 7.2091, 1.2016, 7.2091], 'spans': [{'offset': 1348, 'length': 43}]}, {'content': 'QA tasks, since the retrieved texts are more', 'polygon': [1.2066, 7.2294, 3.7821, 7.2294, 3.7821, 7.3713, 1.2066, 7.3764], 'spans': [{'offset': 1392, 'length': 44}]}, {'content': 'condensed with question-relevant information,', 'polygon': [1.1965, 7.3916, 3.8075, 7.3916, 3.8075, 7.5386, 1.1965, 7.5386], 'spans': [{'offset': 1437, 'length': 45}]}, {'content': 'reducing the need for lengthy input tokens and', 'polygon': [1.2016, 7.564, 3.7923, 7.5589, 3.7923, 7.711, 1.2016, 7.711], 'spans': [{'offset': 1483, 'length': 46}]}, {'content': 'minimizing the inclusion of extraneous, irrele-', 'polygon': [1.2016, 7.7262, 3.8075, 7.7262, 3.8075, 7.8732, 1.2016, 7.8732], 'spans': [{'offset': 1530, 'length': 47}]}, {'content': 'vant information.', 'polygon': [1.2016, 7.8986, 2.1648, 7.8986, 2.1648, 8.0304, 1.2016, 8.0253], 'spans': [{'offset': 1578, 'length': 17}]}, {'content': '# 1 Introduction', 'polygon': [0.9481, 8.4512, 2.1395, 8.4512, 2.1395, 8.6286, 0.9481, 8.6286], 'spans': [{'offset': 1598, 'length': 16}]}, {'content': 'Dense retrievers are a popular class of techniques', 'polygon': [0.9633, 8.7807, 4.0255, 8.7807, 4.0255, 8.9429, 0.9633, 8.9379], 'spans': [{'offset': 1616, 'length': 50}]}, {'content': 'for accessing external information sources for', 'polygon': [0.9582, 8.9683, 4.0255, 8.9683, 4.0255, 9.1153, 0.9582, 9.1254], 'spans': [{'offset': 1667, 'length': 46}]}, {'content': 'knowledge-intensive tasks (Karpukhin et al., 2020).', 'polygon': [0.9683, 9.1508, 4.0356, 9.1508, 4.0356, 9.313, 0.9683, 9.313], 'spans': [{'offset': 1714, 'length': 51}]}, {'content': 'Before we use a learned dense retriever to retrieve', 'polygon': [0.9734, 9.3384, 4.0204, 9.3384, 4.0204, 9.4955, 0.9734, 9.4955], 'spans': [{'offset': 1766, 'length': 51}]}, {'content': 'from a corpus, an imperative design decision we', 'polygon': [0.9683, 9.531, 4.0255, 9.5259, 4.0255, 9.6932, 0.9683, 9.6932], 'spans': [{'offset': 1818, 'length': 47}]}, {'content': 'have to make is the retrieval unit - i.e. the granu-', 'polygon': [0.9683, 9.7135, 4.0407, 9.7186, 4.0407, 9.8707, 0.9683, 9.8707], 'spans': [{'offset': 1866, 'length': 52}]}, {'content': 'larity at which we segment and index the retrieval', 'polygon': [0.9633, 9.9062, 4.0204, 9.896, 4.0204, 10.0532, 0.9633, 10.0684], 'spans': [{'offset': 1919, 'length': 50}]}, {'content': 'Question: What is the angle of the Tower of Pisa?', 'polygon': [4.3398, 3.1939, 6.8342, 3.1939, 6.8342, 3.3308, 4.3398, 3.3359], 'spans': [{'offset': 1987, 'length': 49}]}, {'content': 'Passage', 'polygon': [4.3297, 3.422, 4.7555, 3.4271, 4.7555, 3.5488, 4.3297, 3.5437], 'spans': [{'offset': 2042, 'length': 7}]}, {'content': 'Retrieval', 'polygon': [4.3347, 3.5589, 4.8062, 3.564, 4.8012, 3.6755, 4.3347, 3.6705], 'spans': [{'offset': 2050, 'length': 9}]}, {'content': 'Prior to restoration work performed be-', 'polygon': [5.0395, 3.417, 7.2094, 3.417, 7.2094, 3.5488, 5.0395, 3.5488], 'spans': [{'offset': 2062, 'length': 39}]}, {'content': 'tween 1990 and 2001, the tower leaned at', 'polygon': [5.0445, 3.5589, 7.1992, 3.5589, 7.1992, 3.6806, 5.0445, 3.6857], 'spans': [{'offset': 2102, 'length': 40}]}, {'content': 'an angle of 5.5 degrees, but the tower now', 'polygon': [5.0496, 3.7009, 7.1891, 3.6958, 7.1891, 3.8225, 5.0496, 3.8276], 'spans': [{'offset': 2143, 'length': 42}]}, {'content': 'leans at about 3.99 degrees. This means', 'polygon': [5.0344, 3.8428, 7.1891, 3.8428, 7.1891, 3.9746, 5.0344, 3.9746], 'spans': [{'offset': 2191, 'length': 39}]}, {'content': 'the top of the Leaning Tower of Pisa is dis-', 'polygon': [5.0496, 3.9898, 7.2094, 3.9848, 7.2094, 4.1115, 5.0496, 4.1166], 'spans': [{'offset': 2231, 'length': 44}]}, {'content': 'placed horizontally 3.9 meters (12 ft 10 in)', 'polygon': [5.0547, 4.1217, 7.1992, 4.1166, 7.1992, 4.2585, 5.0547, 4.2585], 'spans': [{'offset': 2276, 'length': 44}]}, {'content': 'from the center.', 'polygon': [5.0496, 4.2687, 5.8405, 4.2687, 5.8354, 4.3853, 5.0496, 4.3802], 'spans': [{'offset': 2321, 'length': 16}]}, {'content': 'Sentence', 'polygon': [4.3347, 4.4816, 4.8113, 4.4816, 4.8113, 4.5931, 4.3347, 4.5931], 'spans': [{'offset': 2342, 'length': 8}]}, {'content': 'Retrieval', 'polygon': [4.3347, 4.6185, 4.8062, 4.6185, 4.8062, 4.7351, 4.3347, 4.7351], 'spans': [{'offset': 2351, 'length': 9}]}, {'content': 'Prior to restoration work performed be-', 'polygon': [5.0445, 4.4816, 7.2043, 4.4765, 7.2043, 4.6033, 5.0445, 4.6083], 'spans': [{'offset': 2363, 'length': 39}]}, {'content': 'tween 1990 and 2001, the tower leaned at', 'polygon': [5.0496, 4.6185, 7.1891, 4.6185, 7.1891, 4.7452, 5.0496, 4.7402], 'spans': [{'offset': 2403, 'length': 40}]}, {'content': 'an angle of 5.5 degrees, but the tower now', 'polygon': [5.0496, 4.7604, 7.1891, 4.7604, 7.1891, 4.8872, 5.0496, 4.8922], 'spans': [{'offset': 2444, 'length': 42}]}, {'content': 'leans at about 3.99 degrees.', 'polygon': [5.0597, 4.9176, 6.4387, 4.9176, 6.4387, 5.0393, 5.0597, 5.0342], 'spans': [{'offset': 2492, 'length': 28}]}, {'content': 'Proposition', 'polygon': [4.3297, 5.1305, 4.9127, 5.1305, 4.9127, 5.2522, 4.3297, 5.2573], 'spans': [{'offset': 2525, 'length': 11}]}, {'content': 'Retrieval', 'polygon': [4.3347, 5.2674, 4.8164, 5.2674, 4.8164, 5.3789, 4.3347, 5.3789], 'spans': [{'offset': 2537, 'length': 9}]}, {'content': 'The Leaning Tower of Pisa now leans at', 'polygon': [5.0597, 5.1305, 7.1891, 5.1305, 7.1891, 5.2573, 5.0597, 5.2623], 'spans': [{'offset': 2549, 'length': 38}]}, {'content': 'about 3.99 degrees.', 'polygon': [5.0496, 5.2877, 6.023, 5.2877, 6.023, 5.4144, 5.0496, 5.4094], 'spans': [{'offset': 2593, 'length': 19}]}, {'content': '<!-- FigureContent="Passage', 'polygon': [4.8265, 5.6071, 5.2068, 5.6071, 5.2068, 5.6983, 4.8265, 5.6983], 'spans': [{'offset': 2642, 'length': 27}]}, {'content': 'Sentence', 'polygon': [5.5921, 5.602, 6.023, 5.6071, 6.023, 5.6983, 5.5921, 5.6882], 'spans': [{'offset': 2670, 'length': 8}]}, {'content': 'Proposition', 'polygon': [6.4286, 5.602, 6.9356, 5.602, 6.9356, 5.6983, 6.4286, 5.6983], 'spans': [{'offset': 2679, 'length': 11}]}, {'content': '70', 'polygon': [4.6643, 5.8656, 4.7657, 5.8656, 4.7657, 5.9518, 4.6643, 5.9518], 'spans': [{'offset': 2691, 'length': 2}]}, {'content': 'Passage Retrieval', 'polygon': [4.8823, 5.7845, 5.6681, 5.7896, 5.663, 5.891, 4.8823, 5.8859], 'spans': [{'offset': 2694, 'length': 17}]}, {'content': 'Question Answering', 'polygon': [6.094, 5.7845, 6.9812, 5.7896, 6.9812, 5.891, 6.094, 5.8859], 'spans': [{'offset': 2712, 'length': 18}]}, {'content': '40', 'polygon': [5.9267, 5.8707, 6.0331, 5.8707, 6.0281, 5.9467, 5.9267, 5.9467], 'spans': [{'offset': 2731, 'length': 2}]}, {'content': 'Recall@5 (%)', 'polygon': [4.5375, 6.6464, 4.5325, 6.0177, 4.6339, 6.0177, 4.6389, 6.6464], 'spans': [{'offset': 2734, 'length': 12}]}, {'content': '60', 'polygon': [4.6541, 6.0836, 4.7606, 6.0836, 4.7606, 6.1698, 4.6541, 6.1698], 'spans': [{'offset': 2747, 'length': 2}]}, {'content': 'EM@100 (%)', 'polygon': [5.7898, 6.6362, 5.7847, 6.0126, 5.9013, 6.0126, 5.9115, 6.6362], 'spans': [{'offset': 2750, 'length': 10}]}, {'content': '30', 'polygon': [5.9317, 6.0735, 6.0281, 6.0786, 6.0281, 6.1698, 5.9317, 6.1647], 'spans': [{'offset': 2761, 'length': 2}]}, {'content': '50', 'polygon': [4.6592, 6.2915, 4.7606, 6.2915, 4.7606, 6.3777, 4.6592, 6.3777], 'spans': [{'offset': 2764, 'length': 2}]}, {'content': '20', 'polygon': [5.9267, 6.2915, 6.0281, 6.2915, 6.023, 6.3777, 5.9216, 6.3726], 'spans': [{'offset': 2767, 'length': 2}]}, {'content': '40', 'polygon': [4.6541, 6.5095, 4.7707, 6.5145, 4.7707, 6.5957, 4.6541, 6.5957], 'spans': [{'offset': 2770, 'length': 2}]}, {'content': '10', 'polygon': [5.9267, 6.5095, 6.0331, 6.5095, 6.0331, 6.5957, 5.9267, 6.5957], 'spans': [{'offset': 2773, 'length': 2}]}, {'content': '30', 'polygon': [4.6694, 6.7224, 4.7707, 6.7224, 4.7707, 6.8035, 4.6694, 6.8035], 'spans': [{'offset': 2776, 'length': 2}]}, {'content': '0', 'polygon': [5.9774, 6.7325, 6.0281, 6.7275, 6.0331, 6.7984, 5.9774, 6.8035], 'spans': [{'offset': 2779, 'length': 1}]}, {'content': 'Contriever', 'polygon': [4.8367, 6.8187, 5.2777, 6.8187, 5.2777, 6.9151, 4.8367, 6.9151], 'spans': [{'offset': 2781, 'length': 10}]}, {'content': 'GTR', 'polygon': [5.4045, 6.8238, 5.5769, 6.8238, 5.5718, 6.9049, 5.4045, 6.9049], 'spans': [{'offset': 2792, 'length': 3}]}, {'content': 'Contriever', 'polygon': [6.1092, 6.8238, 6.5503, 6.8238, 6.5503, 6.9151, 6.1092, 6.91], 'spans': [{'offset': 2796, 'length': 10}]}, {'content': 'GTR" -->', 'polygon': [6.6466, 6.8187, 6.8443, 6.8238, 6.8443, 6.9049, 6.6466, 6.91], 'spans': [{'offset': 2807, 'length': 8}]}, {'content': 'Figure 1: (Top) An example of three granularities of', 'polygon': [4.2283, 7.0519, 7.3006, 7.0519, 7.3057, 7.199, 4.2283, 7.199], 'spans': [{'offset': 2831, 'length': 52}]}, {'content': 'retrieval units of Wikipedia text when using dense re-', 'polygon': [4.2283, 7.2192, 7.3057, 7.2243, 7.3057, 7.3663, 4.2283, 7.3612], 'spans': [{'offset': 2884, 'length': 54}]}, {'content': 'trieval. (Bottom) We observe that retrieving by proposi-', 'polygon': [4.2333, 7.3865, 7.2955, 7.3916, 7.2955, 7.5386, 4.2333, 7.5336], 'spans': [{'offset': 2939, 'length': 56}]}, {'content': 'tions yields the best retrieval performance in both pas-', 'polygon': [4.2333, 7.5488, 7.3057, 7.5488, 7.3057, 7.7009, 4.2333, 7.6958], 'spans': [{'offset': 2996, 'length': 56}]}, {'content': 'sage retrieval task and downstream open-domain QA', 'polygon': [4.2333, 7.7161, 7.2803, 7.7161, 7.2803, 7.8682, 4.2333, 7.8732], 'spans': [{'offset': 3053, 'length': 49}]}, {'content': 'task, e.g. with Contriever (Izacard et al., 2022) or GTR', 'polygon': [4.2384, 7.8884, 7.2854, 7.8783, 7.2854, 8.0253, 4.2384, 8.0355], 'spans': [{'offset': 3103, 'length': 56}]}, {'content': '(Ni et al., 2022) as the backbone retriever. Highlight', 'polygon': [4.2435, 8.0507, 7.2905, 8.0507, 7.2905, 8.2028, 4.2435, 8.2028], 'spans': [{'offset': 3160, 'length': 54}]}, {'content': 'indicates the part that contains answer to the question.', 'polygon': [4.2333, 8.2129, 7.255, 8.218, 7.255, 8.365, 4.2333, 8.365], 'spans': [{'offset': 3215, 'length': 56}]}, {'content': 'corpus for inference. In practice, the choice of re-', 'polygon': [4.2384, 8.5171, 7.3006, 8.512, 7.3006, 8.6692, 4.2384, 8.6793], 'spans': [{'offset': 3300, 'length': 52}]}, {'content': 'trieval unit, e.g. documents, fixed-length passage', 'polygon': [4.2333, 8.6996, 7.2905, 8.7047, 7.2854, 8.8669, 4.2333, 8.8618], 'spans': [{'offset': 3353, 'length': 50}]}, {'content': 'chunks or sentences, etc, is usually pre-determined', 'polygon': [4.2333, 8.8872, 7.2854, 8.8872, 7.2803, 9.0494, 4.2333, 9.0494], 'spans': [{'offset': 3404, 'length': 51}]}, {'content': 'based on how the dense retrieval model is instanti-', 'polygon': [4.2283, 9.0798, 7.3158, 9.0747, 7.3158, 9.2268, 4.2283, 9.2319], 'spans': [{'offset': 3456, 'length': 51}]}, {'content': 'ated or trained (Lewis et al., 2020; Lee et al., 2021a;', 'polygon': [4.2333, 9.2623, 7.2955, 9.2623, 7.2955, 9.4195, 4.2333, 9.4195], 'spans': [{'offset': 3508, 'length': 55}]}, {'content': 'Santhanam et al., 2022; Ni et al., 2022).', 'polygon': [4.2384, 9.4499, 6.6618, 9.4499, 6.6618, 9.6071, 4.2384, 9.6071], 'spans': [{'offset': 3564, 'length': 41}]}, {'content': 'In this paper, we investigate an overlooked re-', 'polygon': [4.3854, 9.6628, 7.3057, 9.6628, 7.3057, 9.82, 4.3854, 9.8251], 'spans': [{'offset': 3607, 'length': 47}]}, {'content': 'search question with dense retrieval inference - at', 'polygon': [4.2333, 9.8504, 7.2905, 9.8453, 7.2905, 10.0076, 4.2333, 10.0076], 'spans': [{'offset': 3655, 'length': 51}]}, {'content': 'what retrieval granularity should we segment and', 'polygon': [4.2333, 10.0329, 7.2905, 10.0329, 7.2905, 10.1951, 4.2333, 10.1951], 'spans': [{'offset': 3707, 'length': 48}]}, {'content': 'index the retrieval corpus? We discover that se-', 'polygon': [4.2333, 10.2205, 7.3108, 10.2205, 7.3108, 10.3827, 4.2333, 10.3827], 'spans': [{'offset': 3756, 'length': 48}]}, {'content': 'lecting the proper retrieval granularity at inference', 'polygon': [4.2333, 10.4131, 7.2905, 10.403, 7.2905, 10.5754, 4.2333, 10.5804], 'spans': [{'offset': 3805, 'length': 53}]}, {'content': 'time can be a simple yet effective strategy for im-', 'polygon': [4.2384, 10.5906, 7.3158, 10.5906, 7.3158, 10.7629, 4.2384, 10.7629], 'spans': [{'offset': 3859, 'length': 51}]}, {'content': '<!-- Footnote="\\* Work was done during internship at Tencent AI Lab,', 'polygon': [1.2066, 10.185, 4.0356, 10.1901, 4.0356, 10.327, 1.2066, 10.3219], 'spans': [{'offset': 3912, 'length': 68}]}, {'content': 'Bellevue." -->', 'polygon': [0.9683, 10.332, 1.4449, 10.3371, 1.4449, 10.4588, 0.9683, 10.4486], 'spans': [{'offset': 3981, 'length': 14}]}, {'content': '<!-- Footnote="https://github.com/ct123098/', 'polygon': [1.4348, 10.4841, 3.5337, 10.479, 3.5337, 10.6109, 1.4348, 10.6159], 'spans': [{'offset': 4008, 'length': 43}]}, {'content': 'factoid-wiki" -->', 'polygon': [0.9785, 10.6261, 1.8911, 10.621, 1.8911, 10.7376, 0.9785, 10.7376], 'spans': [{'offset': 4052, 'length': 17}]}, {'content': '## arXiv:2312.06648v2 [cs.CL] 12 Dec 2023', 'polygon': [0.2231, 8.4765, 0.218, 3.6198, 0.4867, 3.6198, 0.4867, 8.4765], 'spans': [{'offset': 4072, 'length': 41}]}]
Line 0 content: Dense XX Retrieval: What Retrieval Granularity Should We Use?
===
Line 1 content: Tong Chen \*\* Hongwei Wang
Line 2 content: Sihao Chen
Line 3 content: Wenhao Yu"
Line 4 content: Kaixin Ma
Line 5 content: Xinran Zhao^ Hongming Zhang
Line 6 content: Dong Yu
Line 7 content: \*University of Washington
Line 8 content: Tencent AI Lab
Line 9 content: University of Pennsylvania ^Carnegie Mellon University
Line 10 content: # Abstract
Line 11 content: Dense retrieval has become a prominent
Line 12 content: method to obtain relevant context or world
Line 13 content: knowledge in open-domain NLP tasks. When
Line 14 content: we use a learned dense retriever on a retrieval
Line 15 content: corpus at inference time, an often-overlooked
Line 16 content: design choice is the retrieval unit in which the
Line 17 content: corpus is indexed, e.g. document, passage, or
Line 18 content: sentence. We discover that the retrieval unit
Line 19 content: choice significantly impacts the performance
Line 20 content: of both retrieval and downstream tasks. Dis-
Line 21 content: tinct from the typical approach of using pas-
Line 22 content: sages or sentences, we introduce a novel re-
Line 23 content: trieval unit, proposition, for dense retrieval.
Line 24 content: Propositions are defined as atomic expressions
Line 25 content: within text, each encapsulating a distinct fac-
Line 26 content: toid and presented in a concise, self-contained
Line 27 content: natural language format. We conduct an empir-
Line 28 content: ical comparison of different retrieval granular-
Line 29 content: ity. Our results reveal that proposition-based
Line 30 content: retrieval significantly outperforms traditional
Line 31 content: passage or sentence-based methods in dense
Line 32 content: retrieval. Moreover, retrieval by proposition
Line 33 content: also enhances the performance of downstream
Line 34 content: QA tasks, since the retrieved texts are more
Line 35 content: condensed with question-relevant information,
Line 36 content: reducing the need for lengthy input tokens and
Line 37 content: minimizing the inclusion of extraneous, irrele-
Line 38 content: vant information.
Line 39 content: # 1 Introduction
Line 40 content: Dense retrievers are a popular class of techniques
Line 41 content: for accessing external information sources for
Line 42 content: knowledge-intensive tasks (Karpukhin et al., 2020).
Line 43 content: Before we use a learned dense retriever to retrieve
Line 44 content: from a corpus, an imperative design decision we
Line 45 content: have to make is the retrieval unit - i.e. the granu-
Line 46 content: larity at which we segment and index the retrieval
Line 47 content: Question: What is the angle of the Tower of Pisa?
Line 48 content: Passage
Line 49 content: Retrieval
Page 1 lines: [{'content': '<!-- FigureContent="A', 'polygon': [1.1306, 1.0596, 1.1965, 1.0697, 1.1914, 1.1407, 1.1255, 1.1407], 'spans': [{'offset': 4429, 'length': 21}]}, {'content': 'Prior to restoration work performed between', 'polygon': [1.3486, 1.1001, 2.9557, 1.1052, 2.9557, 1.1964, 1.3486, 1.1964], 'spans': [{'offset': 4451, 'length': 43}]}, {'content': '1990 and 2001, the tower leaned at an angle of', 'polygon': [1.2421, 1.1964, 2.9912, 1.2015, 2.9912, 1.308, 1.2421, 1.2978], 'spans': [{'offset': 4495, 'length': 46}]}, {'content': '5.5 degrees ,', 'polygon': [1.2472, 1.308, 1.7491, 1.3232, 1.7491, 1.4094, 1.2421, 1.3992], 'spans': [{'offset': 4542, 'length': 13}]}, {'content': 'but the tower now leans at', 'polygon': [1.8404, 1.3181, 2.8087, 1.3181, 2.8087, 1.3992, 1.8404, 1.4043], 'spans': [{'offset': 4556, 'length': 26}]}, {'content': 'B', 'polygon': [4.2688, 1.0596, 4.3246, 1.0646, 4.3195, 1.1407, 4.2587, 1.1356], 'spans': [{'offset': 4583, 'length': 1}]}, {'content': 'Corpus', 'polygon': [4.8468, 1.1052, 5.1713, 1.1052, 5.1713, 1.2117, 4.8468, 1.2066], 'spans': [{'offset': 4585, 'length': 6}]}, {'content': 'C', 'polygon': [5.4805, 1.0444, 5.5921, 1.0444, 5.587, 1.1559, 5.4755, 1.1508], 'spans': [{'offset': 4592, 'length': 1}]}, {'content': 'Query', 'polygon': [4.3499, 1.2218, 4.6339, 1.2319, 4.6288, 1.3181, 4.3499, 1.308], 'spans': [{'offset': 4594, 'length': 5}]}, {'content': 'about 3.99 degrees.', 'polygon': [1.2472, 1.4144, 1.9874, 1.4195, 1.9874, 1.5108, 1.2421, 1.5057], 'spans': [{'offset': 4600, 'length': 19}]}, {'content': 'This means the top of the', 'polygon': [2.104, 1.4144, 3.0014, 1.4094, 3.0014, 1.5057, 2.104, 1.5108], 'spans': [{'offset': 4620, 'length': 25}]}, {'content': 'Learning Tower of Pisa is displaced horizontally', 'polygon': [1.2421, 1.5158, 2.9456, 1.5209, 2.9456, 1.6172, 1.2421, 1.6172], 'spans': [{'offset': 4646, 'length': 48}]}, {'content': '?', 'polygon': [4.6213, 1.3397, 4.6789, 1.3397, 4.6789, 1.4613, 4.6213, 1.4613], 'spans': [{'offset': 4695, 'length': 1}]}, {'content': 'Retrieval', 'polygon': [5.4805, 1.4094, 5.8709, 1.4144, 5.8658, 1.5108, 5.4805, 1.5057], 'spans': [{'offset': 4697, 'length': 9}]}, {'content': 'Wikipedia', 'polygon': [3.3208, 1.5767, 3.8176, 1.5767, 3.8176, 1.6933, 3.3208, 1.6933], 'spans': [{'offset': 4707, 'length': 9}]}, {'content': 'Units', 'polygon': [5.5667, 1.5057, 5.7797, 1.5108, 5.7797, 1.602, 5.5616, 1.597], 'spans': [{'offset': 4717, 'length': 5}]}, {'content': '3.9 meters (12 ft 10 in) from the center.', 'polygon': [1.2472, 1.6223, 2.6515, 1.6223, 2.6515, 1.7136, 1.2472, 1.7136], 'spans': [{'offset': 4723, 'length': 41}]}, {'content': '=', 'polygon': [5.5109, 1.6629, 5.6123, 1.6578, 5.6123, 1.7947, 5.5109, 1.7947], 'spans': [{'offset': 4765, 'length': 1}]}, {'content': 'Passage Retrieval', 'polygon': [6.2106, 1.597, 7.0573, 1.5919, 7.0573, 1.7237, 6.2106, 1.7288], 'spans': [{'offset': 4767, 'length': 17}]}, {'content': 'Retriever', 'polygon': [4.5578, 1.7643, 4.9482, 1.7643, 4.9482, 1.8555, 4.5578, 1.8504], 'spans': [{'offset': 4785, 'length': 9}]}, {'content': '1. Prior to restoration work performed', 'polygon': [1.3638, 1.9113, 2.8087, 1.9113, 2.8087, 2.0127, 1.3638, 2.0177], 'spans': [{'offset': 4795, 'length': 38}]}, {'content': 'between 1990 and 2001, the Leaning Tower', 'polygon': [1.2421, 2.0279, 2.9456, 2.0279, 2.9456, 2.1293, 1.2421, 2.1242], 'spans': [{'offset': 4834, 'length': 40}]}, {'content': 'Proposition-izer', 'polygon': [3.2346, 1.967, 3.8937, 1.967, 3.8987, 2.0684, 3.2346, 2.0684], 'spans': [{'offset': 4875, 'length': 16}]}, {'content': 'Retrieval', 'polygon': [4.2131, 2.0431, 4.573, 2.0431, 4.5781, 2.1242, 4.2131, 2.1293], 'spans': [{'offset': 4892, 'length': 9}]}, {'content': 'D', 'polygon': [5.4755, 2.0279, 5.587, 2.0279, 5.5769, 2.1394, 5.4653, 2.1343], 'spans': [{'offset': 4913, 'length': 1}]}, {'content': 'of Pisa leaned at an angle of 5.5 degrees.', 'polygon': [1.2421, 2.1394, 2.8493, 2.1394, 2.8493, 2.2357, 1.2421, 2.2357], 'spans': [{'offset': 4915, 'length': 42}]}, {'content': 'Units', 'polygon': [4.284, 2.1293, 4.497, 2.1343, 4.4868, 2.2256, 4.284, 2.2205], 'spans': [{'offset': 4958, 'length': 5}]}, {'content': 'QA Model', 'polygon': [5.729, 2.0532, 6.2512, 2.0532, 6.2512, 2.1648, 5.729, 2.1648], 'spans': [{'offset': 4964, 'length': 8}]}, {'content': '2. The Leaning Tower of Pisa now leans at', 'polygon': [1.3486, 2.3219, 2.9608, 2.3219, 2.9608, 2.4182, 1.3486, 2.4182], 'spans': [{'offset': 4973, 'length': 41}]}, {'content': 'about 3.99 degrees.', 'polygon': [1.237, 2.4284, 2.033, 2.4334, 2.033, 2.5298, 1.237, 2.5298], 'spans': [{'offset': 5015, 'length': 19}]}, {'content': 'Query', 'polygon': [5.7746, 2.2307, 6.0991, 2.2357, 6.094, 2.3371, 5.7746, 2.327], 'spans': [{'offset': 5035, 'length': 5}]}, {'content': 'Answer', 'polygon': [6.6567, 2.2307, 7.0218, 2.2357, 7.0218, 2.3371, 6.6517, 2.327], 'spans': [{'offset': 5041, 'length': 6}]}, {'content': '?', 'polygon': [6.0577, 2.343, 6.1154, 2.343, 6.1154, 2.4645, 6.0577, 2.4645], 'spans': [{'offset': 5048, 'length': 1}]}, {'content': 'Sentences', 'polygon': [4.7555, 2.4436, 5.1104, 2.4537, 5.1054, 2.5348, 4.7555, 2.5247], 'spans': [{'offset': 5050, 'length': 9}]}, {'content': '>', 'polygon': [6.4134, 2.3726, 6.4996, 2.3726, 6.4996, 2.4689, 6.4083, 2.4639], 'spans': [{'offset': 5060, 'length': 1}]}, {'content': '✓', 'polygon': [6.9933, 2.3798, 7.0753, 2.3798, 7.0753, 2.4568, 6.9933, 2.4568], 'spans': [{'offset': 5062, 'length': 1}]}, {'content': '3. The top of the Leaning Tower of Pisa is', 'polygon': [1.3435, 2.6007, 2.9202, 2.6007, 2.9202, 2.7072, 1.3435, 2.7072], 'spans': [{'offset': 5064, 'length': 42}]}, {'content': 'Retrieval Units', 'polygon': [5.7087, 2.5703, 6.3171, 2.5703, 6.312, 2.6717, 5.7087, 2.6667], 'spans': [{'offset': 5107, 'length': 15}]}, {'content': 'displaced horizontally 3.9 meters (12 ft 10 in)', 'polygon': [1.2421, 2.7123, 2.9963, 2.7123, 2.9963, 2.8238, 1.2421, 2.8238], 'spans': [{'offset': 5123, 'length': 47}]}, {'content': 'from the center.', 'polygon': [1.2421, 2.8289, 1.8505, 2.8289, 1.8505, 2.9252, 1.2421, 2.9201], 'spans': [{'offset': 5171, 'length': 16}]}, {'content': 'FactoidWiki', 'polygon': [3.2903, 2.7985, 3.8531, 2.7985, 3.8531, 2.9049, 3.2903, 2.91], 'spans': [{'offset': 5188, 'length': 11}]}, {'content': 'Passages', 'polygon': [4.2232, 2.7985, 4.5578, 2.8035, 4.5578, 2.8897, 4.2181, 2.8796], 'spans': [{'offset': 5200, 'length': 8}]}, {'content': 'Propositions" -->', 'polygon': [4.6998, 2.7934, 5.1408, 2.7934, 5.1408, 2.8847, 4.6998, 2.8847], 'spans': [{'offset': 5209, 'length': 17}]}, {'content': 'Figure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet', 'polygon': [0.9633, 3.199, 7.2905, 3.204, 7.2905, 3.3511, 0.9633, 3.3511], 'spans': [{'offset': 5242, 'length': 114}]}, {'content': "effective strategy to increase dense retrievers' generalization performance at inference time (A, B). We empirically", 'polygon': [0.9734, 3.3663, 7.2803, 3.3663, 7.2803, 3.5184, 0.9734, 3.5184], 'spans': [{'offset': 5357, 'length': 116}]}, {'content': 'compare the retrieval and downstream open-domain QA tasks performance when dense retrievers work with', 'polygon': [0.9683, 3.5386, 7.2854, 3.5336, 7.2854, 3.6806, 0.9683, 3.6857], 'spans': [{'offset': 5474, 'length': 101}]}, {'content': 'Wikipedia indexed at the level of 100-word passage, sentence or proposition (C, D).', 'polygon': [0.9683, 3.7009, 5.6225, 3.7059, 5.6225, 3.853, 0.9683, 3.8479], 'spans': [{'offset': 5576, 'length': 83}]}, {'content': "proving dense retrievers' retrieval and downstream", 'polygon': [0.9683, 4.0405, 4.0255, 4.0355, 4.0255, 4.1876, 0.9683, 4.2028], 'spans': [{'offset': 5688, 'length': 50}]}, {'content': 'task performance. We illustrate our intuition with', 'polygon': [0.9734, 4.223, 4.0204, 4.218, 4.0204, 4.3701, 0.9734, 4.3802], 'spans': [{'offset': 5739, 'length': 50}]}, {'content': 'an example of open-domain question-answering', 'polygon': [0.9633, 4.4106, 4.0255, 4.4056, 4.0255, 4.5729, 0.9633, 4.5779], 'spans': [{'offset': 5790, 'length': 44}]}, {'content': '(QA) in Table 1. The example shows retrieved text', 'polygon': [0.9683, 4.5931, 4.0255, 4.5931, 4.0255, 4.7554, 0.9683, 4.7554], 'spans': [{'offset': 5835, 'length': 49}]}, {'content': 'by the same model at three different granularities.', 'polygon': [0.9683, 4.7807, 4.0458, 4.7807, 4.0458, 4.9379, 0.9683, 4.9429], 'spans': [{'offset': 5885, 'length': 51}]}, {'content': 'The passage, which represents a coarser retrieval', 'polygon': [0.9633, 4.9683, 4.0204, 4.9683, 4.0204, 5.1305, 0.9633, 5.1407], 'spans': [{'offset': 5937, 'length': 49}]}, {'content': 'unit with a longer context, is theoretically able to', 'polygon': [0.9683, 5.1559, 4.0204, 5.1559, 4.0204, 5.3181, 0.9683, 5.3232], 'spans': [{'offset': 5987, 'length': 52}]}, {'content': 'provide more relevant information for the ques-', 'polygon': [0.9633, 5.3485, 4.0508, 5.3434, 4.0508, 5.5057, 0.9633, 5.5107], 'spans': [{'offset': 6040, 'length': 47}]}, {'content': 'tion. However, a passage often includes extraneous', 'polygon': [0.9633, 5.531, 4.0255, 5.531, 4.0255, 5.6933, 0.9633, 5.6983], 'spans': [{'offset': 6088, 'length': 50}]}, {'content': 'details (e.g., restoration period and horizontal dis-', 'polygon': [0.9734, 5.7237, 4.0407, 5.7186, 4.0407, 5.8758, 0.9734, 5.8859], 'spans': [{'offset': 6139, 'length': 53}]}, {'content': 'placement in the example of Table 1) that could po-', 'polygon': [0.9582, 5.9113, 4.0407, 5.9062, 4.0407, 6.0684, 0.9582, 6.0735], 'spans': [{'offset': 6193, 'length': 51}]}, {'content': 'tentially distract both the retriever and the language', 'polygon': [0.9683, 6.1039, 4.0255, 6.1039, 4.0255, 6.2611, 0.9683, 6.256], 'spans': [{'offset': 6245, 'length': 54}]}, {'content': 'model in downstream tasks (Shi et al., 2023; Yu', 'polygon': [0.9683, 6.2864, 4.0255, 6.2864, 4.0255, 6.4436, 0.9683, 6.4385], 'spans': [{'offset': 6300, 'length': 47}]}, {'content': 'et al., 2023b). On the other hand, sentence-level in-', 'polygon': [0.9683, 6.4791, 4.0458, 6.4791, 4.0458, 6.6261, 0.9683, 6.6311], 'spans': [{'offset': 6348, 'length': 53}]}, {'content': 'dexing provides a finer-grained approach but does', 'polygon': [0.9734, 6.6666, 4.0255, 6.6666, 4.0255, 6.8187, 0.9734, 6.8238], 'spans': [{'offset': 6402, 'length': 49}]}, {'content': 'not entirely address the issue (Akkalyoncu Yilmaz', 'polygon': [0.9633, 6.8542, 4.0204, 6.8542, 4.0204, 7.0063, 0.9633, 7.0114], 'spans': [{'offset': 6452, 'length': 49}]}, {'content': 'et al., 2019; Yang et al., 2020). This is because sen-', 'polygon': [0.9683, 7.0418, 4.0407, 7.0367, 4.0458, 7.1939, 0.9683, 7.199], 'spans': [{'offset': 6502, 'length': 54}]}, {'content': 'tences can still be complex and compounded, and', 'polygon': [0.9683, 7.2243, 4.0305, 7.2243, 4.0305, 7.3865, 0.9683, 7.3865], 'spans': [{'offset': 6557, 'length': 47}]}, {'content': 'they are often not self-contained, lacking necessary', 'polygon': [0.9683, 7.4119, 4.0255, 7.417, 4.0255, 7.5792, 0.9683, 7.5741], 'spans': [{'offset': 6605, 'length': 52}]}, {'content': 'contextual information (e.g., in the example of Ta-', 'polygon': [0.9683, 7.5995, 4.0407, 7.5995, 4.0407, 7.7617, 0.9683, 7.7617], 'spans': [{'offset': 6658, 'length': 51}]}, {'content': 'ble 1, "the tower" is coreference of "Pisa Tower")', 'polygon': [0.9683, 7.787, 4.0153, 7.787, 4.0153, 7.9442, 0.9683, 7.9442], 'spans': [{'offset': 6710, 'length': 50}]}, {'content': 'for judging the query-document relevance.', 'polygon': [0.9633, 7.9746, 3.554, 7.9746, 3.554, 8.1368, 0.9633, 8.147], 'spans': [{'offset': 6761, 'length': 41}]}, {'content': 'To address these shortcomings of typical re-', 'polygon': [1.1154, 8.3396, 4.0458, 8.3447, 4.0458, 8.5069, 1.1154, 8.4968], 'spans': [{'offset': 6804, 'length': 44}]}, {'content': 'trieval units such as passages or sentences, we', 'polygon': [0.9683, 8.5272, 4.0305, 8.5374, 4.0305, 8.6945, 0.9683, 8.6844], 'spans': [{'offset': 6849, 'length': 47}]}, {'content': 'propose using proposition as a novel retrieval unit', 'polygon': [0.9683, 8.73, 4.0255, 8.7097, 4.0255, 8.8669, 0.9683, 8.8872], 'spans': [{'offset': 6897, 'length': 51}]}, {'content': 'for dense retrieval. Propositions are defined as', 'polygon': [0.9683, 8.9024, 4.0305, 8.9024, 4.0305, 9.0646, 0.9683, 9.0646], 'spans': [{'offset': 6949, 'length': 48}]}, {'content': 'atomic expressions within text, each encapsulat-', 'polygon': [0.9633, 9.095, 4.0407, 9.0899, 4.0407, 9.2522, 0.9633, 9.2572], 'spans': [{'offset': 6998, 'length': 48}]}, {'content': 'ing a distinct factoid and presented in a concise,', 'polygon': [0.9582, 9.2775, 4.0204, 9.2775, 4.0204, 9.4398, 0.9582, 9.4448], 'spans': [{'offset': 7047, 'length': 50}]}, {'content': 'self-contained natural language format. We show', 'polygon': [0.9683, 9.4702, 4.0204, 9.4702, 4.0204, 9.6324, 0.9683, 9.6273], 'spans': [{'offset': 7098, 'length': 47}]}, {'content': 'an example proposition in Table 1. The proposi-', 'polygon': [0.9633, 9.6578, 4.0356, 9.6578, 4.0356, 9.82, 0.9633, 9.8301], 'spans': [{'offset': 7146, 'length': 47}]}, {'content': 'tion describes the information regarding the Tower', 'polygon': [0.9633, 9.8403, 4.0305, 9.8453, 4.0255, 10.0126, 0.9633, 10.0076], 'spans': [{'offset': 7194, 'length': 50}]}, {'content': "of Pisa's current leaning angle in a self-contained", 'polygon': [0.9683, 10.0329, 4.0305, 10.0278, 4.0305, 10.1951, 0.9683, 10.2002], 'spans': [{'offset': 7245, 'length': 51}]}, {'content': 'way and precisely responds to what the question', 'polygon': [0.9633, 10.2256, 4.0255, 10.2256, 4.0255, 10.3827, 0.9633, 10.3878], 'spans': [{'offset': 7297, 'length': 47}]}, {'content': 'is querying. We provide a more detailed definition', 'polygon': [0.9531, 10.4081, 4.0204, 10.403, 4.0204, 10.5652, 0.9531, 10.5804], 'spans': [{'offset': 7345, 'length': 50}]}, {'content': 'and description of proposition in §2.', 'polygon': [0.9633, 10.5956, 3.1889, 10.5906, 3.1889, 10.7629, 0.9633, 10.768], 'spans': [{'offset': 7396, 'length': 37}]}, {'content': 'To validate the efficacy of using proposition as', 'polygon': [4.3854, 4.0304, 7.2905, 4.0355, 7.2905, 4.1977, 4.3854, 4.1876], 'spans': [{'offset': 7435, 'length': 48}]}, {'content': 'a retrieval unit for dense retrievers inference, we', 'polygon': [4.2384, 4.218, 7.2905, 4.218, 7.2905, 4.3751, 4.2384, 4.3701], 'spans': [{'offset': 7484, 'length': 51}]}, {'content': 'first process and index an English Wikipedia dump', 'polygon': [4.2333, 4.4056, 7.2854, 4.4056, 7.2854, 4.5779, 4.2333, 4.5779], 'spans': [{'offset': 7536, 'length': 49}]}, {'content': 'with all documents segmented into propositions,', 'polygon': [4.2384, 4.5931, 7.3158, 4.5982, 7.3158, 4.7604, 4.2384, 4.7554], 'spans': [{'offset': 7586, 'length': 47}]}, {'content': 'which we refer to as FACTOIDWIKI. Then we con-', 'polygon': [4.2333, 4.7807, 7.3006, 4.7858, 7.3006, 4.9379, 4.2333, 4.9328], 'spans': [{'offset': 7634, 'length': 46}]}, {'content': 'duct experiments on five different open-domain QA', 'polygon': [4.2435, 4.9683, 7.2854, 4.9632, 7.2854, 5.1305, 4.2435, 5.1356], 'spans': [{'offset': 7681, 'length': 49}]}, {'content': 'datasets and empirically compare the performance', 'polygon': [4.2283, 5.1559, 7.2905, 5.1559, 7.2905, 5.3232, 4.2283, 5.3232], 'spans': [{'offset': 7731, 'length': 48}]}, {'content': 'of six dual-encoder retrievers when Wikipedia is', 'polygon': [4.2333, 5.3384, 7.2905, 5.3384, 7.2905, 5.5107, 4.2333, 5.5057], 'spans': [{'offset': 7780, 'length': 48}]}, {'content': 'indexed by passage, sentence, and our proposed', 'polygon': [4.2283, 5.5361, 7.2905, 5.5361, 7.2905, 5.6983, 4.2283, 5.6983], 'spans': [{'offset': 7829, 'length': 46}]}, {'content': 'proposition. Our evaluation is twofold: we exam-', 'polygon': [4.2333, 5.7237, 7.3108, 5.7237, 7.3108, 5.8808, 4.2333, 5.891], 'spans': [{'offset': 7876, 'length': 48}]}, {'content': 'ine both the retrieval performance and the impact', 'polygon': [4.2283, 5.9062, 7.2955, 5.9062, 7.2955, 6.0735, 4.2283, 6.0684], 'spans': [{'offset': 7925, 'length': 49}]}, {'content': 'on downstream QA tasks. Notably, our findings in-', 'polygon': [4.2384, 6.1039, 7.3057, 6.0988, 7.3057, 6.2611, 4.2384, 6.2661], 'spans': [{'offset': 7975, 'length': 49}]}, {'content': 'dicate that proposition-based retrieval outperforms', 'polygon': [4.2333, 6.2915, 7.2854, 6.2864, 7.2854, 6.4486, 4.2333, 6.4537], 'spans': [{'offset': 8025, 'length': 51}]}, {'content': 'sentence and passage-based methods, especially in', 'polygon': [4.2384, 6.4791, 7.2905, 6.474, 7.2905, 6.6413, 4.2384, 6.6464], 'spans': [{'offset': 8077, 'length': 49}]}, {'content': 'terms of generalization, as discussed in §5. This', 'polygon': [4.2384, 6.6717, 7.2905, 6.6565, 7.2905, 6.8187, 4.2384, 6.8289], 'spans': [{'offset': 8127, 'length': 49}]}, {'content': 'suggests that propositions, being both compact and', 'polygon': [4.2384, 6.8593, 7.2905, 6.8491, 7.2905, 7.0114, 4.2384, 7.0215], 'spans': [{'offset': 8177, 'length': 50}]}, {'content': 'rich in context, enable dense retrievers to access', 'polygon': [4.2333, 7.0367, 7.2803, 7.0367, 7.2803, 7.1939, 4.2333, 7.1939], 'spans': [{'offset': 8228, 'length': 50}]}, {'content': 'precise information while maintaining adequate', 'polygon': [4.2283, 7.2243, 7.2905, 7.2243, 7.2905, 7.3916, 4.2283, 7.3916], 'spans': [{'offset': 8279, 'length': 46}]}, {'content': 'context. The average improvement over passage-', 'polygon': [4.2333, 7.417, 7.2955, 7.422, 7.2955, 7.5792, 4.2333, 7.5741], 'spans': [{'offset': 8326, 'length': 46}]}, {'content': 'based retrieval of Recall@20 is +10.1 on unsu-', 'polygon': [4.2333, 7.5995, 7.3057, 7.5995, 7.3006, 7.7566, 4.2333, 7.7566], 'spans': [{'offset': 8373, 'length': 46}]}, {'content': 'pervised dense retrievers and +2.2 on supervised', 'polygon': [4.2333, 7.7921, 7.2905, 7.787, 7.2905, 7.9493, 4.2333, 7.9543], 'spans': [{'offset': 8420, 'length': 48}]}, {'content': 'retrievers. Furthermore, we observe a distinct ad-', 'polygon': [4.2333, 7.9746, 7.3108, 7.9746, 7.3108, 8.1368, 4.2333, 8.1368], 'spans': [{'offset': 8469, 'length': 50}]}, {'content': 'vantage in downstream QA performance when us-', 'polygon': [4.2384, 8.1622, 7.3006, 8.1622, 7.3057, 8.3244, 4.2384, 8.3295], 'spans': [{'offset': 8520, 'length': 45}]}, {'content': 'ing proposition-based retrieval, as elaborated in §6.', 'polygon': [4.2333, 8.3599, 7.3057, 8.3498, 7.3057, 8.5171, 4.2333, 8.5272], 'spans': [{'offset': 8566, 'length': 53}]}, {'content': 'Given the often limited input token length in lan-', 'polygon': [4.2333, 8.5424, 7.3057, 8.5424, 7.3006, 8.7097, 4.2333, 8.7047], 'spans': [{'offset': 8620, 'length': 50}]}, {'content': 'guage models, propositions inherently provide a', 'polygon': [4.2333, 8.7401, 7.2905, 8.7351, 7.2905, 8.8922, 4.2333, 8.9024], 'spans': [{'offset': 8671, 'length': 47}]}, {'content': 'higher density of question-relevant information.', 'polygon': [4.2232, 8.9176, 7.1434, 8.9176, 7.1434, 9.0798, 4.2232, 9.0849], 'spans': [{'offset': 8719, 'length': 48}]}, {'content': 'Our main contributions in the paper are:', 'polygon': [4.3905, 9.1052, 6.8342, 9.1153, 6.8342, 9.2725, 4.3905, 9.2623], 'spans': [{'offset': 8769, 'length': 40}]}, {'content': '· We propose using propositions as retrieval units', 'polygon': [4.2333, 9.4195, 7.2905, 9.4093, 7.2905, 9.5665, 4.2333, 9.5817], 'spans': [{'offset': 8811, 'length': 50}]}, {'content': 'when indexing a retrieval corpus to improve the', 'polygon': [4.3499, 9.5969, 7.2905, 9.5969, 7.2905, 9.7693, 4.3499, 9.7642], 'spans': [{'offset': 8862, 'length': 47}]}, {'content': 'dense retrieval performance.', 'polygon': [4.3499, 9.7845, 6.0991, 9.7845, 6.0991, 9.9518, 4.3499, 9.9518], 'spans': [{'offset': 8910, 'length': 28}]}, {'content': '· We introduce FACTOIDWIKI, a processed En-', 'polygon': [4.2283, 10.0329, 7.3057, 10.038, 7.3006, 10.1951, 4.2283, 10.1901], 'spans': [{'offset': 8940, 'length': 43}]}, {'content': 'glish Wikipedia dump, where each page is seg-', 'polygon': [4.3449, 10.2205, 7.3108, 10.2256, 7.3108, 10.3929, 4.3449, 10.3878], 'spans': [{'offset': 8984, 'length': 45}]}, {'content': 'mented into multiple granularities: 100-word pas-', 'polygon': [4.3499, 10.4081, 7.3209, 10.4081, 7.3209, 10.5754, 4.3499, 10.5754], 'spans': [{'offset': 9030, 'length': 49}]}, {'content': 'sages, sentences and propositions.', 'polygon': [4.355, 10.6058, 6.4438, 10.5956, 6.4438, 10.7579, 4.355, 10.768], 'spans': [{'offset': 9080, 'length': 34}]}]
Line 0 content: <!-- FigureContent="A
Line 1 content: Prior to restoration work performed between
Line 2 content: 1990 and 2001, the tower leaned at an angle of
Line 3 content: 5.5 degrees ,
Line 4 content: but the tower now leans at
Line 5 content: B
Line 6 content: Corpus
Line 7 content: C
Line 8 content: Query
Line 9 content: about 3.99 degrees.
Line 10 content: This means the top of the
Line 11 content: Learning Tower of Pisa is displaced horizontally
Line 12 content: ?
Line 13 content: Retrieval
Line 14 content: Wikipedia
Line 15 content: Units
Line 16 content: 3.9 meters (12 ft 10 in) from the center.
Line 17 content: =
Line 18 content: Passage Retrieval
Line 19 content: Retriever
Line 20 content: 1. Prior to restoration work performed
Line 21 content: between 1990 and 2001, the Leaning Tower
Line 22 content: Proposition-izer
Line 23 content: Retrieval
Line 24 content: D
Line 25 content: of Pisa leaned at an angle of 5.5 degrees.
Line 26 content: Units
Line 27 content: QA Model
Line 28 content: 2. The Leaning Tower of Pisa now leans at
Line 29 content: about 3.99 degrees.
Line 30 content: Query
Line 31 content: Answer
Line 32 content: ?
Line 33 content: Sentences
Line 34 content: >
Line 35 content: ✓
Line 36 content: 3. The top of the Leaning Tower of Pisa is
Line 37 content: Retrieval Units
Line 38 content: displaced horizontally 3.9 meters (12 ft 10 in)
Line 39 content: from the center.
Line 40 content: FactoidWiki
Line 41 content: Passages
Line 42 content: Propositions" -->
Line 43 content: Figure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet
Line 44 content: effective strategy to increase dense retrievers' generalization performance at inference time (A, B). We empirically
Line 45 content: compare the retrieval and downstream open-domain QA tasks performance when dense retrievers work with
Line 46 content: Wikipedia indexed at the level of 100-word passage, sentence or proposition (C, D).
Line 47 content: proving dense retrievers' retrieval and downstream
Line 48 content: task performance. We illustrate our intuition with
Line 49 content: an example of open-domain question-answering