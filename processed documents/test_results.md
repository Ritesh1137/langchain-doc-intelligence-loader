langchain_doc_intelligence package has been imported
line 0: Dense XX Retrieval: What Retrieval Granularity Should We Use?
line 1: ===
line 2:  :selected:
line 3: Tong Chen \*\* Hongwei Wang Sihao Chen Wenhao Yu" Kaixin Ma :unselected: Xinran Zhao^ Hongming Zhang :unselected: Dong Yu
line 4:
line 5: \*University of Washington
line 6:
line 7: Tencent AI Lab
line 8:  :unselected:
line 9: University of Pennsylvania ^Carnegie Mellon University
line 10:  :selected:
line 11:
line 12: # Abstract
line 13:
line 14: Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Dis- tinct from the typical approach of using pas- sages or sentences, we introduce a novel re- trieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct fac-
line 15:
line 16: toid and presented in a concise, self-contained natural language format. We conduct an empir- ical comparison of different retrieval granular- ity. Our results reveal that proposition-based retrieval significantly outperforms traditional passage or sentence-based methods in dense retrieval. Moreover, retrieval by proposition also enhances the performance of downstream QA tasks, since the retrieved texts are more condensed with question-relevant information, reducing the need for lengthy input tokens and minimizing the inclusion of extraneous, irrele- vant information.
line 17:
line 18:
line 19: # 1 Introduction
line 20:
line 21: Dense retrievers are a popular class of techniques for accessing external information sources for knowledge-intensive tasks (Karpukhin et al., 2020). Before we use a learned dense retriever to retrieve from a corpus, an imperative design decision we have to make is the retrieval unit - i.e. the granu- larity at which we segment and index the retrieval
line 22:
line 23: |||
line 24: | - | - |
line 25: | Question: What is the angle of the Tower of Pisa? ||
line 26: | Passage Retrieval | Prior to restoration work performed be- tween 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but the tower now |
line 27: || leans at about 3.99 degrees. This means the top of the Leaning Tower of Pisa is dis- placed horizontally 3.9 meters (12 ft 10 in) from the center. |
line 28: | Sentence Retrieval | Prior to restoration work performed be- tween 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but the tower now |
line 29: || leans at about 3.99 degrees. |
line 30: | Proposition Retrieval | The Leaning Tower of Pisa now leans at |
line 31: || about 3.99 degrees. |
line 32:
line 33: <figure>
line 34:
line 35: ![](figures/0)
line 36:
line 37: <!-- FigureContent="Passage Sentence Proposition 70 Passage Retrieval Question Answering 40 Recall@5 (%) 60 EM@100 (%) 30 50 20 40 10 30 0 Contriever GTR Contriever GTR" -->
line 38:
line 39: <figcaption>
line 40:
line 41: Figure 1: (Top) An example of three granularities of
line 42: retrieval units of Wikipedia text when using dense re-
line 43: trieval. (Bottom) We observe that retrieving by proposi-
line 44: tions yields the best retrieval performance in both pas-
line 45: sage retrieval task and downstream open-domain QA
line 46: task, e.g. with Contriever (Izacard et al., 2022) or GTR
line 47: (Ni et al., 2022) as the backbone retriever. Highlight
line 48: indicates the part that contains answer to the question.
line 49:
line 50: </figcaption>
line 51:
line 52: </figure>
line 53:
line 54:
line 55: corpus for inference. In practice, the choice of re- trieval unit, e.g. documents, fixed-length passage chunks or sentences, etc, is usually pre-determined based on how the dense retrieval model is instanti- ated or trained (Lewis et al., 2020; Lee et al., 2021a; Santhanam et al., 2022; Ni et al., 2022).
line 56:
line 57: In this paper, we investigate an overlooked re- search question with dense retrieval inference - at what retrieval granularity should we segment and index the retrieval corpus? We discover that se- lecting the proper retrieval granularity at inference time can be a simple yet effective strategy for im-
line 58:
line 59: <!-- Footnote="\* Work was done during internship at Tencent AI Lab, Bellevue." -->
line 60:  :selected:
line 61: <!-- Footnote="https://github.com/ct123098/ factoid-wiki" -->
line 62:
line 63:
line 64: ## arXiv:2312.06648v2 [cs.CL] 12 Dec 2023
line 65: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected:<figure>
line 66:
line 67: ![](figures/1)
line 68:
line 69: <!-- FigureContent="A Prior to restoration work performed between 1990 and 2001, the tower leaned at an angle of 5.5 degrees , but the tower now leans at B Corpus C Query about 3.99 degrees. This means the top of the Learning Tower of Pisa is displaced horizontally ? Retrieval Wikipedia Units 3.9 meters (12 ft 10 in) from the center. = Passage Retrieval Retriever 1. Prior to restoration work performed between 1990 and 2001, the Leaning Tower Proposition-izer Retrieval :selected: D of Pisa leaned at an angle of 5.5 degrees. Units QA Model 2. The Leaning Tower of Pisa now leans at about 3.99 degrees. Query Answer ? Sentences > ✓ 3. The top of the Leaning Tower of Pisa is Retrieval Units displaced horizontally 3.9 meters (12 ft 10 in) from the center. FactoidWiki Passages Propositions" -->
line 70:
line 71: <figcaption>
line 72:
line 73: Figure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet
line 74: effective strategy to increase dense retrievers' generalization performance at inference time (A, B). We empirically
line 75: compare the retrieval and downstream open-domain QA tasks performance when dense retrievers work with
line 76: Wikipedia indexed at the level of 100-word passage, sentence or proposition (C, D).
line 77:
line 78: </figcaption>
line 79:
line 80: </figure>
line 81:
line 82:
line 83: proving dense retrievers' retrieval and downstream task performance. We illustrate our intuition with an example of open-domain question-answering (QA) in Table 1. The example shows retrieved text by the same model at three different granularities. The passage, which represents a coarser retrieval unit with a longer context, is theoretically able to provide more relevant information for the ques- tion. However, a passage often includes extraneous details (e.g., restoration period and horizontal dis- placement in the example of Table 1) that could po- tentially distract both the retriever and the language model in downstream tasks (Shi et al., 2023; Yu et al., 2023b). On the other hand, sentence-level in- dexing provides a finer-grained approach but does not entirely address the issue (Akkalyoncu Yilmaz et al., 2019; Yang et al., 2020). This is because sen- tences can still be complex and compounded, and they are often not self-contained, lacking necessary contextual information (e.g., in the example of Ta- ble 1, "the tower" is coreference of "Pisa Tower") for judging the query-document relevance.
line 84:
line 85: To address these shortcomings of typical re- trieval units such as passages or sentences, we propose using proposition as a novel retrieval unit for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulat- ing a distinct factoid and presented in a concise, self-contained natural language format. We show an example proposition in Table 1. The proposi- tion describes the information regarding the Tower of Pisa's current leaning angle in a self-contained way and precisely responds to what the question is querying. We provide a more detailed definition and description of proposition in §2.
line 86:
line 87: To validate the efficacy of using proposition as a retrieval unit for dense retrievers inference, we first process and index an English Wikipedia dump with all documents segmented into propositions, which we refer to as FACTOIDWIKI. Then we con- duct experiments on five different open-domain QA datasets and empirically compare the performance of six dual-encoder retrievers when Wikipedia is indexed by passage, sentence, and our proposed proposition. Our evaluation is twofold: we exam- ine both the retrieval performance and the impact on downstream QA tasks. Notably, our findings in- dicate that proposition-based retrieval outperforms sentence and passage-based methods, especially in terms of generalization, as discussed in §5. This suggests that propositions, being both compact and rich in context, enable dense retrievers to access precise information while maintaining adequate context. The average improvement over passage- based retrieval of Recall@20 is +10.1 on unsu- pervised dense retrievers and +2.2 on supervised retrievers. Furthermore, we observe a distinct ad- vantage in downstream QA performance when us- ing proposition-based retrieval, as elaborated in §6. Given the often limited input token length in lan- guage models, propositions inherently provide a higher density of question-relevant information.
line 88:
line 89: Our main contributions in the paper are:
line 90:
line 91: · We propose using propositions as retrieval units when indexing a retrieval corpus to improve the dense retrieval performance.
line 92:
line 93: · We introduce FACTOIDWIKI, a processed En- glish Wikipedia dump, where each page is seg- mented into multiple granularities: 100-word pas- sages, sentences and propositions.
line 94:
line 95: · We discover that retrieval by proposition outper- forms passage or sentence retrieval in terms of generalization for passage retrieval and accuracy for downstream question-answering given the same input token limit.
line 96:
line 97:
line 98: # 2 Proposition as a Retrieval Unit
line 99:
line 100: The goal of our study is to understand how the gran- ularity of a retrieval corpus influences the dense retrieval models' performance empirically. Aside from commonly-used retrieval units such as 100- word passage (Karpukhin et al., 2020) or sentence, we propose using proposition as an alternative re- trieval unit choice. Here, propositions represent atomic expressions of meanings in text (Min et al., 2023) that are defined by the three principles below.
line 101:
line 102: 1\. Each proposition should correspond to a distinct piece of meaning in text, where the composition of all propositions would represent the seman- tics of the entire text.
line 103:
line 104: 2\. A proposition should be minimal, i.e. it cannot be further split into separate propositions.
line 105:
line 106: 3\. A proposition should be contextualized and self- contained (Choi et al., 2021). A proposition should include all the necessary context from the text (e.g. coreference) to interpret its meaning.
line 107:
line 108: The use of proposition as a retrieval unit is inspired by a recent line of work (Min et al., 2023; Kamoi et al., 2023; Chen et al., 2023a,b), which finds suc- cess in representing and evaluating text semantics at the level of propositions. We demonstrate the concept of proposition and how a passage can be split into its set of propositions by an example on the left side of Figure 2. The passage contains three propositions, each of which corresponds to a distinct factoid about the Leaning Tower of Pisa: the angle before the restoration, the current an- gle, and the horizontal displacement. Within each proposition, necessary context from the passage is incorporated so that the meaning of the proposition can be interpreted independently of the original text, e.g. the reference of the tower is resolved into its full mention, the Leaning Tower of Pisa, in the first proposition. We expect each proposition to de- scribe exactly one contextualized atomic fact, and so our intuition is that propositions would suitably work as a retrieval unit for information-seeking questions.
line 109:
line 110:
line 111: # 3 FACTOID WIKI: Proposition-Level Index and Retrieval for Wikipedia
line 112:
line 113: We empirically compare the use of 100-word pas- sages, sentences, and propositions as retrieval units on Wikipedia, a commonly-used retrieval source for knowledge-intensive NLP tasks (Petroni et al., 2021). To allow for a fair comparison across gran- ularities, we process an English Wikipedia dump from 2021-10-13, as used by Bohnet et al. (2022). We segment each document text into three different granularities: 100-word passages, sentences, and propositions. We include the details on passage- and sentence-level segmentation of the corpus in Appendix A.
line 114:
line 115: Parsing Passage to Propositions. To segment the Wikipedia pages into propositions, we finetune a text generation model, which we refer to as the Propositionizer. The Propositionizer takes a pas- sage as input and generates the list of propositions within the passage. Following Chen et al. (2023b), we train the Propositionizer with a two-step distil- lation process. We first prompt GPT-4 (OpenAI, 2023) with an instruction containing the propo- sition definition and 1-shot demonstrations. We include the details of the prompt in Figure 8. We start with a set of 42k passages and use GPT-4 to generate the seed set of paragraph-to-propositions pairs. Next, we use the seed set to finetune a Flan- T5-large model (Chung et al., 2022).
line 116:
line 117: We refer to the processed corpus as FACTOID- WIKI. The resulting statistics of FACTOID WIKI are shown in Table 1.
line 118:
line 119: | | # units | Avg. # words |
line 120: | - | - | - |
line 121: | Passage | 41,393,528 | 58.5 |
line 122: | Sentence | 114,219,127 | 21.0 |
line 123: | Proposition | 256,885,003 | 11.2 |
line 124:
line 125: Table 1: Statistics of text units in the English Wikipedia dump from 2021-10-13.
line 126:
line 127:
line 128: # 4 Experimental Settings
line 129:
line 130: To evaluate the impact of the three retrieval unit choices, we conduct experiments on five differ- ent open-domain QA datasets with FACTOIDWIKI. With each dataset, we evaluate both passage re- trieval and downstream QA performance when dense retrievers work with Wikipedia indexed in different granularities.
line 131:
line 132: ## 4.1 Open-Domain QA Datasets
line 133:
line 134: We evaluate on five different open-domain QA datasets with Wikipedia as the retrieval source: Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (TQA) (Joshi et al., 2017), Web Questions (WebQ) (Berant et al., 2013), SQUAD (Rajpurkar et al., 2016), and Entity Ques- tions (EQ) (Sciavolino et al., 2021).
line 135:
line 136:
line 137: ## 4.2 Dense Retrieval Models
line 138:
line 139: We compare the performance of the six following supervised or unsupervised dense retriever mod- els. Here, supervised models refer to ones that have used human-labeled query-passage pairs as supervision during training, and vice versa.
line 140:
line 141: · SimCSE (Gao et al., 2021) is a BERT-base (De- vlin et al., 2019) encoder trained on unlabeled sentence randomly sampled from Wikipedia.
line 142:
line 143: · Contriever (Izacard et al., 2022) is an unsuper- vised retriever, instantiated with a BERT-base encoder. Contriever is contrastively trained by segment pairs constructed from unlabeled docu- ments from Wikipedia and web crawl data.
line 144:
line 145: · DPR (Karpukhin et al., 2020) is a dual-encoder BERT-base model finetuned on five open-domain QA datasets, which includes four of the datasets (NQ, TQA, WebQ and SQUAD) in our evalua- tion.
line 146:
line 147: · ANCE (Xiong et al., 2020) used the same setting from DPR and trained the model with Approxi- mate nearest neighbor Negative Contrastive Esti- mation (ANCE), a hard negatives-based training approach.
line 148:
line 149: · TAS-B (Hofstätter et al., 2021) is a dual- encoder BERT-base model distilled from a teacher model with cross-attention trained on MS MARCO (Nguyen et al., 2016).
line 150:
line 151: · GTR (Hofstätter et al., 2021) is a T5-base en- coder (Raffel et al., 2020) pretrained on unla- beled pairs of online forum QA data, and fine- tuned on MS MARCO and Natural Question.
line 152:
line 153:
line 154: ## 4.3 Passage Retrieval Evaluation
line 155:
line 156: We evaluate retrieval performance at the passage level when the corpus is indexed at the passage, sentence, or proposition level respectively. For sen- tence and proposition level retrieval, we follow the setting introduced in Lee et al. (2021b), where the score of the passage is based on the maximum sim- ilarity score between the query and all sentences
line 157:
line 158: or propositions in a passage. In practice, we first retrieve a slightly larger number of text units, map each unit to the source passage, and return the top-k unique passages. We use Recall@k as our evalu- ation metric, which is defined as the percentage of questions for which the correct answer is found within the top-k retrieved passages.
line 159:
line 160:
line 161: ## 4.4 Downstream QA Evaluation
line 162:
line 163: To understand the implications of using different retrieval units on the downstream open-domain QA tasks, we evaluate the use of retrieval mod- els in retrieve-then-read setup (Izacard and Grave, 2021). With the retrieve-then-read setting, a re- trieval model first retrieves k text units given the query. The k retrieved text units are then used as input along with the query to a reader model to derive the final answer. Typically, the choice of k is subject to the reader model's maximum input length constraint, or the limit of compute budget, which scales with the number of input tokens.
line 164:
line 165: For this reason, we follow an evaluation setup where the maximum number of retrieved words is capped at l = 100 or 500, i.e. only the top l words from passage, sentence, or proposition level retrieval are feed into the reader model as input. We evaluate the percentage of questions for which the predicted answer exactly matches (EM) the ground truth. We denote our metric as EM @ l. For our evaluation, we use T5-large size UnifiedQA-v2 as the reader model (Khashabi et al., 2022).
line 166:
line 167:
line 168: # 5 Results: Passage Retrieval
line 169:
line 170: In this section, we report and discuss the retrieval tasks performance. Our results show that despite none of the models being trained with proposition- level data, all the retrieval models demonstrated on-par or superior performance when the corpus is indexed at the proposition level.
line 171:
line 172:
line 173: ## 5.1 Passage Retrieval Performance
line 174:
line 175: We report our evaluation results in Table 2. We observe that retrieval by propositions outperforms retrieval by sentence or passage on most tasks for both unsupervised and supervised retrievers.
line 176:
line 177: With all dense retrievers tested, proposition- level retrieval consistently outperforms sentence and passage-level retrieval on average across the five datasets. With the unsupervised retrievers, i.e. SimCSE and Contriever, we see an averaged Re- call@5 improvement of +12.0 and +9.3 (35.0%
line 178:
line 179: | Retriever | Granularity | NO || TỌA || WebQ || SQUAD || EQ || Avg. ||
line 180: |||| R@5 R@20  || R@5 R@20  || R@5 R@20  || R@5 R@20  || R@5 R@20  || R@5 R@20  |
line 181: | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
line 182: | Unsupervised Dense Retrievers ||||||||||||||
line 183: | SimCSE | Passage | 28.8 | 44.3 | 44.9 | 59.4 | 39.8 | 56.0 | 29.5 | 45.5 | 28.4 | 40.3 | 34.3 | 49.1 |
line 184: || Sentence | 35.5 | 53.1 | 50.5 | 64.3 | 45.3 | 64.1 | 37.1 | 52.3 | 36.3 | 50.1 | 40.9 | 56.8 |
line 185: || Proposition | 41.1 | 58.9 | 52.4 | 66.5 | 50.0 | 66.8 | 38.7 | 53.9 | 49.5 | 62.2 | 46.3 | 61.7 |
line 186: | Contriever | Passage | 42.5 | 63.8 | 58.1 | 73.7 | 37.1 | 60.6 | 40.8 | 59.8 | 36.3 | 56.3 | 43.0 | 62.8 |
line 187: || Sentence | 46.4 | 66.8 | 60.6 | 75.7 | 41.7 | 63.1 | 45.1 | 63.5 | 42.7 | 61.3 | 47.3 | 66.1 |
line 188: || Proposition | 50.1 | 70.0 | 65.1 | 77.9 | 45.9 | 66.8 | 50.7 | 67.7 | 51.7 | 70.1 | 52.7 | 70.5 |
line 189: | | Supervised Dense Retrievers |||||||||||||
line 190: | DPR | Passage | 66.0 | 78.0 | 71.6 | 80.2 | 62.9 | 74.9 | 38.3 | 53.9 | 47.5 | 60.4 | 57.3 | 69.5 |
line 191: || Sentence | 66.0 | 78.0 | 71.8 | 80.5 | 64.1 | 74.4 | 40.3 | 55.9 | 53.7 | 66.0 | 59.2 | 71.0 |
line 192: || Proposition | 65.4 | 77.7 | 70.7 | 79.6 | 62.8 | 75.1 | 41.4 | 57.2 | 59.4 | 71.3 | 59.9 | 72.2 |
line 193: | ANCE | Passage | 70.7 | 81.4 | 73.9 | 81.4 | 65.7 | 77.2 | 43.3 | 58.6 | 57.0 | 69.1 | 62.1 | 73.5 |
line 194: || Sentence | 70.3 | 81.6 | 73.9 | 81.5 | 65.2 | 77.4 | 45.8 | 60.7 | 61.4 | 72.8 | 63.3 | 74.8 |
line 195: || Proposition | 69.9 | 81.1 | 72.8 | 80.6 | 65.1 | 77.1 | 46.2 | 61.9 | 66.7 | 76.6 | 64.1 | 75.5 |
line 196: | TAS-B | Passage | 64.2 | 77.9 | 70.4 | 79.3 | 65.1 | 77.0 | 54.3 | 69.2 | 72.2 | 81.3 | 65.2 | 76.9 |
line 197: || Sentence | 64.0 | 78.4 | 71.4 | 80.2 | 63.9 | 76.7 | 58.9 | 72.3 | 72.7 | 82.0 | 66.2 | 77.9 |
line 198: || Proposition | 63.8 | 78.6 | 71.4 | 80.0 | 63.8 | 76.8 | 59.8 | 73.4 | 75.1 | 83.3 | 66.8 | 78.4 |
line 199: | GTR | Passage | 66.3 | 78.4 | 70.1 | 79.4 | 63.3 | 76.5 | 54.4 | 68.1 | 71.7 | 80.5 | 65.2 | 76.6 |
line 200: || Sentence | 66.4 | 79.4 | 71.6 | 80.9 | 62.2 | 76.8 | 60.9 | 73.4 | 72.5 | 81.3 | 66.7 | 78.4 |
line 201: || Proposition | 66.5 | 79.6 | 72.2 | 80.9 | 63.2 | 77.4 | 63.3 | 75.0 | 74.9 | 83.0 | 68.0 | 79.2 |
line 202:
line 203: Table 2: Passage retrieval performance (Recall@k = 5, 20) on five different open-domain QA datasets when pre-trained dense retrievers work with the three different granularity from the retrieval corpus. Underline denotes cases where the training split of the target dataset was included in the training data of the dense retriever.
line 204:
line 205: <figure>
line 206:
line 207: ![](figures/2)
line 208:
line 209: <!-- FigureContent="\- Passage -- Sentence -- Proposition SimCSE Contriever DPR ANCE TAS-B 60 GTR 60 70 80 80 Recall@5 Recall@5 Recall@5 60 Recall@5 Recall@5 Recall@5 40 60 70 70 40 50 50 60 60 20 20 40 40 50 50 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 Popularity Popularity Popularity Popularity Popularity Popularity" -->
line 210:
line 211: <figcaption>
line 212:
line 213: Figure 3: Document retrieval recall vs. the popularity of the target entity in each question from the EntityQuestions
line 214: dataset. The popularity of each entity (i.e. smaller value = less common entities, and vice versa) is estimated by
line 215: the occurrence of the entity in its top-1000 passage retrieved by BM25. On queries with less common entities, we
line 216: observe that retrieving by proposition shows a larger advantage over retrieval by proposition.
line 217:
line 218: </figcaption>
line 219:
line 220: </figure>
line 221:
line 222:
line 223: and 22.5% relative improvement) respectively over five datasets.
line 224:
line 225: With the supervised retrievers, proposition-level retrieval still shows an advantage on average, yet the sizes of improvements are smaller. We hypothe- size that this is due to these retrievers being trained on query-passage pairs. For instance, with DPR and ANCE, which have been trained on NQ, TQA, WebQ, and SQUAD, we observe that proposition and sentence level retrieval perform slightly worse compared to passage level on three out of the four datasets, with the exception of SQUAD. As shown in Table 2, all supervised retrievers demonstrate comparable performance across three levels of re- trieval granularity in NQ, TQA, and WebQ.
line 226:
line 227: However, on datasets that the retriever model has not seen during training, we observe that retrieval
line 228:
line 229: by proposition demonstrates a clear advantage. For instance, most notably on SQUAD or EntityQues- tions, we observe that proposition-based retrieval significantly outperforms the other two granulari- ties. We see 17-25% Recall@5 relative improve- ment on EntityQuestions with relatively weak re- trievers like DPR and ANCE. Furthermore, the Recall@5 of retrieval by proposition on SQUAD improved most on TAS-B and GTR, with 10-16% relative improvements.
line 230:
line 231:
line 232: ## 5.2 Retrieval by Proposition => Better Cross-Task Generalization
line 233:
line 234: Our results indicate that the advantage of retrieval by proposition becomes most visible in cross- task generalization settings. We observe that on SQUAD and EntityQuestions, retrieval by proposi-
line 235: :selected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :unselected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :selected: :selected: :unselected: :unselected: :selected: :unselected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :selected:
line 236: | Retriever | Granularity | NQ || TQA || WebQ || SQUAD || EQ || Avg. ||
line 237: ||| EM || EM || EM || EM || EM || EM ||
line 238: |||| @100 @500  | @100 | @500 | @100 | @500 | @100 | @500 | @100 | @500 || @100 @500  |
line 239: | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
line 240: | | Unsupervised Dense Retrievers |||||||||||||
line 241: | SimCSE | Passage | 8.1 | 16.3 | 22.6 | 33.7 | 7.7 | 14.9 | 9.8 | 17.8 | 10.9 | 17.5 | 11.8 | 20.0 |
line 242: || Sentence | 10.1 | 18.0 | 27.2 | 37.2 | 9.6 | 15.6 | 17.3 | 24.8 | 13.0 | 19.8 | 15.4 | 23.1 |
line 243: || Proposition | 12.7 | 20.2 | 28.4 | 37.7 | 11.2 | 17.2 | 18.0 | 25.1 | 18.3 | 25.0 | 17.7 | 25.0 |
line 244: | Contriever | Passage | 11.1 | 22.4 | 25.7 | 41.4 | 6.8 | 14.9 | 15.6 | 27.7 | 10.9 | 21.5 | 14.0 | 25.6 |
line 245: || Sentence | 13.8 | 23.9 | 30.5 | 44.2 | 9.1 | 17.2 | 22.6 | 32.8 | 12.2 | 22.2 | 17.6 | 28.1 |
line 246: || Proposition | 16.5 | 26.1 | 37.7 | 48.7 | 13.3 | 19.9 | 25.6 | 34.4 | 16.1 | 27.3 | 21.8 | 31.3 |
line 247: | | Supervised Dense Retrievers |||||||||||||
line 248: | DPR | Passage | 24.8 | 36.1 | 40.3 | 51.0 | 14.0 | 22.2 | 12.4 | 21.7 | 18.6 | 25.9 | 22.0 | 31.4 |
line 249: || Sentence | 27.6 | 35.9 | 44.6 | 52.8 | 16.3 | 23.7 | 18.6 | 26.1 | 21.8 | 28.2 | 25.8 | 33.3 |
line 250: || Proposition | 28.3 | 34.3 | 45.7 | 51.9 | 19.0 | 23.8 | 19.8 | 26.3 | 26.3 | 31.9 | 27.8 | 33.6 |
line 251: | ANCE | Passage | 27.1 | 38.3 | 43.1 | 53.1 | 15.2 | 23.0 | 15.3 | 26.0 | 23.4 | 31.1 | 24.8 | 34.3 |
line 252: || Sentence | 30.1 | 37.3 | 47.0 | 54.7 | 16.6 | 23.8 | 22.9 | 30.5 | 25.9 | 32.0 | 28.5 | 35.7 |
line 253: || Proposition | 29.8 | 37.0 | 47.4 | 53.5 | 19.3 | 24.1 | 22.9 | 30.1 | 29.1 | 33.7 | 29.7 | 35.7 |
line 254: | TAS-B | Passage | 21.1 | 33.9 | 39.3 | 50.5 | 13.1 | 20.7 | 23.9 | 34.6 | 30.9 | 37.3 | 25.7 | 35.4 |
line 255: || Sentence | 24.6 | 33.9 | 43.6 | 52.3 | 14.4 | 21.4 | 33.8 | 40.5 | 31.4 | 36.1 | 29.6 | 36.8 |
line 256: || Proposition | 26.6 | 34.0 | 44.9 | 51.8 | 18.1 | 23.7 | 34.2 | 38.9 | 34.2 | 37.8 | 31.6 | 37.2 |
line 257: | GTR | Passage | 23.4 | 34.5 | 38.7 | 49.3 | 13.1 | 20.1 | 23.9 | 33.8 | 31.3 | 36.7 | 26.1 | 34.9 |
line 258: || Sentence | 26.8 | 35.1 | 43.9 | 52.2 | 15.9 | 21.6 | 35.6 | 41.3 | 31.3 | 35.1 | 30.7 | 37.1 |
line 259: || Proposition | 29.5 | 34.4 | 45.9 | 52.6 | 18.7 | 23.8 | 37.0 | 40.4 | 34.1 | 37.1 | 33.0 | 37.7 |
line 260:
line 261: Table 3: Open-domain QA performance (EM = Exact Match) under retrieve-then-read setting where the number of retrieved words to the reader QA model is limited at l = 100 or 500. We use UnifedQA V2 (Khashabi et al., 2022) as the reader model. The first l words from the concatenated top retrieved text unit are feed as input to the reader model. Underline denotes cases where the training split of the target dataset was included in the training data of the dense retriever. In most cases, we see better QA performance with smaller retrieval units.
line 262:
line 263: tion brings more performance gain over retrieval by passage.
line 264:
line 265: To better understand where the improvements can be attributed, we conduct an additional analysis on EntityQuestions. As EntityQuestions features questions targeting the properties of longer-tail enti- ties, we study how the retrieval performance under three different granularities is affected by the popu- larity of the target entity in question, i.e. whether the entity appears frequently in Wikipedia or not. We estimate the popularity of each entity with the following method. Given the surface form of an en- tity, we use BM25 to retrieve the top-1000 relevant passages from Wikipedia. We use the number of occurrences of the entity in its top-1000 passages as an estimate of its popularity. With the 20,000 test queries in EntityQuestion, around 25% of the target entities have a popularity value of less or equal to 3.
line 266:
line 267: Figure 3 shows the passage retrieval perfor- mance vs. the popularity of the target entity in each question. Across all 6 dense retrievers, we ob- serve that retrieving by proposition shows a much larger advantage over retrieving by passage with questions targeting less common entities. As the popularity of entities increases, the performance
line 268:
line 269: gap decreases. Our findings indicate that the per- formance gain from retrieval by proposition can mostly be attributed to queries for long-tailed infor- mation. This echoes our observation that retrieval by proposition improves the cross-task generaliza- tion performance of dense retrievers.
line 270:
line 271:
line 272: # 6 Results: Open-Domain QA
line 273:
line 274: In this section, we study how the choice of retrieval granularity affects downstream open-domain QA tasks. We show that retrieval by proposition leads to strong downstream QA performance in the retrieve-then-read setting, where the number of re- trieved tokens for input to the reader QA model is capped at l = 100 or 500 words.
line 275:
line 276:
line 277: ## 6.1 QA Performance
line 278:
line 279: Table 3 shows the evaluation results. Across dif- ferent retrievers, we observe higher QA perfor- mance in terms of the EM@l metric on average when using propositions as the retrieval unit. The unsupervised retrievers, SimCSE and Contriever, demonstrate improvements of +5.9 and +7.8 in the EM@100 score (50% and 55% relative im- provement), respectively. The supervised retriev- ers, DPR, ANCE, TAS-B, and GTR, improve +5.8,
line 280:
line 281: Passage
line 282:
line 283: Sentence
line 284:
line 285: \-
line 286:
line 287: Proposition
line 288:
line 289: <figure>
line 290:
line 291: ![](figures/3)
line 292:
line 293: <!-- FigureContent="GTR / NQ GTR / TQA GTR / WebQ GTR / SQUAD GTR / EQ 70 70 80 Recall (%) Recall (%) 70 Recall (%) Recall (%) 60 Recall (%) 60 60 70 60 50 50 50 40 60 0 200 400 0 200 400 0 200 400 0 200 400 0 200 400 #Words #Words #Words #Words #Words" -->
line 294:
line 295: <figcaption>
line 296:
line 297: Figure 4: Recall of the gold answer in the retrieved text limited to first k words for the GTR retriever. Finer-grained
line 298: retrieval has a higher recall across all numbers of words.
line 299:
line 300: </figcaption>
line 301:
line 302: </figure>
line 303:
line 304:
line 305: \+4.9, +5.9, and +6.9 EM@100 (26%, 19%, 22%, 26% relative improvement), respectively. Similar to our observations from passage retrieval evalu- ations, we find retrieval by proposition becomes more beneficial to downstream QA performance when the retriever has not been trained on the target dataset. In other cases, retrieval by proposition still holds an advantage, but with a smaller margin on average.
line 306:
line 307:
line 308: ## 6.2 Propositions => Higher Density of Question-Related Information
line 309:
line 310: Intuitively, compared to sentences or passages as retrieval units, the advantage of propositions is that the retrieved propositions have a higher density of relevant information to the query. With finer- grained retrieval units, the correct answer to the query would more likely appear in the top-l re- trieved words by a dense retriever.
line 311:
line 312: We illustrate this phenomenon by an analysis shown in Figure 4. Here, we investigate the posi- tion at which the ground truth answer appears in the top-l retrieved words. Specifically, we calcu- late the recall of the gold answer within the initial l retrieved words with GTR working with Wikipedia indexed in three different granularities.
line 313:
line 314: We show the results in Figure 4 and Figure 7 with l ranging from 0 to 500 across all five datasets. For a fixed retrieved word budget, proposition retrieval demonstrates a higher success rate compared to sentence and passage retrieval methods. The most significant improvement of proposition retrieval over passage retrieval occurs within the range of 100-200 words, which corresponds to roughly 10 propositions, 5 sentences, or 2 passages. As the word count further increases, the recall rates of the three granularity converge since all question- relevant information is included in the retrieved text.
line 315:
line 316:
line 317: ## 6.3 Error Case Study
line 318:
line 319: To understand the source of errors from each type of retrieval granularity, we present and discuss four typical examples of mistakes in Table 4. With each example, we show the question and its correspond- ing top-1 retrieved text unit by the GTR retriever across the three granularities.
line 320:
line 321: We observe that with passage-level retrieval, the ambiguity of an entity or its references presents a challenge for dense retrievers, which echoes find- ings from (Min et al., 2020). For instance, in exam- ple Q1, the question asks for "Super Bowl 50", but the retrieved passage and sentence refers to "Super Bowl 5". In Example Q2, passage retrieval fails to identify the part referring to the correct "atomic number". Instead, the top-1 retrieved passage men- tions "atomic number" in a different and irrelevant context to the question. Retrieval by sentences can also have a similar problem as retrieval by passages like Example Q1. Also, retrieval by sentences faces another challenge of lacking context. In Example Q3, sentence-based retrieval fails as the correct sen- tence in the retrieved passage uses "it" to refer to the pericardial sac.
line 322:
line 323: Retrieval by propositions tackles the aforemen- tioned problems by ensuring each retrieval unit contains one piece of fact only and necessary con- text is incorporated in the propositions. However, proposition-based retrieval faces challenges with questions that involve multi-hop reasoning over long-range textual analysis. In Example Q4, the retrieved passage separately describes the actor's name and the character they portray. There is not a single proposition that entails both the question and the answer.
line 324:
line 325:
line 326: # 7 Related Work
line 327:
line 328: Recent works on dense retrievers typically adopt a dual-encoder architecture (Yih et al., 2011; Reimers and Gurevych, 2019; Karpukhin et al., 2020; Ni et al., 2022). With dual-encoders,
line 329: :unselected: :unselected: :unselected:
line 330: | Passage Retrieval | Sentence Retrieval | Proposition Retrieval |
line 331: | - | - | - |
line 332: | Q1: What was the theme of Super Bowl 50? | | |
line 333: | Title: Super Bowl X ✗ The overall theme of the Super Bowl enter- tainment was to celebrate the United States Bicentennial. Each Cowboys and Steelers player wore a special patch with the Bicen- tennial logo on their jerseys ... :selected: :selected: | Title: Super Bowl X ✗ The overall theme of the Super Bowl entertainment was to celebrate the United States Bicentennial. :selected: :selected: | Title: Super Bowl XLV ✓ ... As this was the 50th Super Bowl game, the league [Super Bowl 50] emphasized the "golden anniversary" with various gold- themed initiatives during the 2015 season, as well as ... :selected: |
line 334: | Q2: The atomic number of indium which belongs to 5th period is? |||
line 335: | Title: Period 5 element ✗ The periodic table is laid out in rows to illus- trate recurring (periodic) trends in the chemi- cal behaviour of the elements as their atomic number increases: ... :selected: | Title: Period 5 element ✓ Indium is a chemical element with the symbol In and atomic number 49. :selected: | Title: Period 5 element ✓ Indium is a chemical element with the sym- bol In and [Indium has a] atomic number 49. This rare, very soft, malleable ... :selected: |
line 336: | Q3: What is the function of the pericardial sac? |||
line 337: | Title: Pericardium ✓ The pericardium, also called pericardial sac ... It separates the heart from interference of other structures, protects it against infection :selected: | Title: Pericardium ✗ The pericardium, also called pericar- dial sac, is a double-walled sac con- taining the heart and the roots of the great vessels. :selected: | Title: Cardiac muscle ✓ On the outer aspect of the myocardium is the epicardium which forms part of the pericar- dial sac that surrounds, protects, and lubri- :selected: |
line 338: | and blunt trauma, and lubricates the heart's || cates the heart. |
line 339: | movements. || |
line 340: | Q4: What is the main character's name in layer cake? |||
line 341: | Title: Layer Cake (film) ✓ ... The film's plot revolves around a London- based criminal, played by Daniel Craig, ... Craig's character is unnamed in the film and is listed in the credits as "XXXX". :selected: | Title: Angelic Layer ✗ The primary protagonist is Misaki Suzuhara. :selected: | Title: Plot twist ✗ Sometimes the audience may discover that the true identity of a character is , in fact, unknown [in Layer Cake] , as in Layer Cake or the eponymous assassins in V for Vendetta and The Day of the Jackal. :selected: |
line 342:
line 343: Table 4: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correct answer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration purpose only and not provided to the retrievers and downstream QA models.
line 344:
line 345: each query and document is encoded into a low- dimensional feature vector respectively, and their relevance is measured by a non-parametric similar- ity function between the embedding vectors (Muss- mann and Ermon, 2016). Due to the limited expres- sivity from the similarity function, dual encoder models often generalize poorly to new tasks with scarce training data (Thakur et al., 2021). To this end, previous studies use techniques such as data augmentation (Wang et al., 2022; Yu et al., 2023a; Izacard et al., 2022; Gao and Callan, 2022; Lin et al., 2023; Dai et al., 2023), continual pre-training (Chang et al., 2020; Sachan et al., 2021; Oguz et al., 2022), task-aware training (Xin et al., 2022; Cheng et al., 2023), hybrid sparse-dense retrieval (Luan et al., 2021; Chen et al., 2022) or mixed strategy re- trieval (Ma et al., 2022, 2023) and so on to improve cross-task generalization performance of dense re- trievers.
line 346:
line 347: The motivation of our work echoes in part with multi-vector retrieval, e.g. ColBERT (Khattab and Zaharia, 2020), DensePhrase (Lee et al., 2021a,b), ME-BERT (Luan et al., 2021), MVR (Zhang et al., 2022), where the retrieval model learns to encode
line 348:
line 349: a candidate retrieval unit into multiple vectors to increase model expressivity and improve retrieval granularity (Seo et al., 2019; Humeau et al., 2019). Our work instead focuses on the setting where we do not update the dense retriever model or its pa- rameters. We show that segmenting the retrieval corpus into finer-grained units of proposition can be a simple and orthogonal strategy for improving the generalization of dual encoders dense retrievers at inference time.
line 350:
line 351: The idea of using propositions as a unit of text representation dates back to the Pyramid method in summarization evaluation (Nenkova and Passon- neau, 2004), where model generated summary is evaluated by each proposition. Proposition extrac- tion from text has been a long-standing task in NLP, with earlier formulations focusing on a structured representation of propositions, e.g. Open Infor- mation Extraction (Etzioni et al., 2008), Semantic Role Labeling (Gildea and Jurafsky, 2000). More recent studies have found success in extracting propositions in natural language form via few-shot prompting with large language models (Min et al., 2023; Kamoi et al., 2023), or finetuning smaller
line 352:
line 353: compact-sized models (Chen et al., 2023b).
line 354:
line 355: Retrieve-then-read, or more broadly - retrieval augmented generation, has recently merged as a popular paradigm for open-domain question an- swering (Lewis et al., 2021; Jiang et al., 2023; Asai et al., 2023). While earlier works provide up to the top 100 retrieved passages for the down- stream reader (Izacard and Grave, 2021; Kedia et al., 2022), the amount of allowed context is significantly reduced when using recent large lan- guage models (e.g. top 10) (Touvron et al., 2023; Yu et al., 2023b), due to their limited context win- dow length and inability to reason over long con- text (Liu et al., 2023). Recent efforts have tried to improve the quality of the reader context by filter- ing or compressing the retrieved documents (Wang et al., 2023; Xu et al., 2023). Our work offers a new perspective by leveraging a new retrieval unit, the proposition that not only reduces the context length but also offers greater information density, effectively addressing the issue.
line 356:
line 357:
line 358: # 8 Conclusion
line 359:
line 360: We propose the use of propositions as retrieval units for indexing corpus to improve dense retrieval per- formance at inference time. Through our experi- ments on five open-domain QA datasets with six different dense retrievers, we discovered that re- trieval by proposition outperforms passage or sen- tence in both passage retrieval accuracy and down- stream QA performance with a fixed retrieved word budget. We introduce FACTOIDWIKI, an indexed version of the English Wikipedia dump, where text from 6 million pages is segmented into 250 million propositions. We hope that FACTOIDWIKI, along with our findings in the paper, will facilitate future research on information retrieval.
line 361:
line 362:
line 363: # Limitations
line 364:
line 365: The scope of our current study on the granular- ity of retrieval corpus has the following limita- tions. (1) Retrieval Corpus - Our study only focus on Wikipedia as the retrieval corpus, due to the fact that most open-domain QA datasets adopts Wikipedia as the retrieval corpus. (2) Types of dense retrievers evaluated - In the current version of the paper, we only evaluate on 6 types of popular dense retrievers, most of which follow bi- or dual- encoder architecture. In future versions, we will include and discuss results on a broader range of dense retrievers. (3) Language - Our current study
line 366:
line 367: is limited to English Wikipedia only. We leave the exploration on other languages to future work.
line 368:
line 369:
line 370: # References
line 371:
line 372: Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Cross-domain modeling of sentence-level evidence for document retrieval. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3490- 3496, Hong Kong, China. Association for Computa- tional Linguistics.
line 373:
line 374: Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to re- trieve, generate, and critique through self-reflection.
line 375:
line 376: Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533-1544.
line 377:
line 378: Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037.
line 379:
line 380: Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yim- ing Yang, and Sanjiv Kumar. 2020. Pre-training tasks for embedding-based large-scale retrieval. In Inter- national Conference on Learning Representations.
line 381:
line 382: Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan Roth, and Tal Schuster. 2023a. PropSegmEnt: A large-scale corpus for proposition-level segmentation and entailment recognition. In Findings of the As- sociation for Computational Linguistics: ACL 2023, pages 8874-8893, Toronto, Canada. Association for Computational Linguistics.
line 383:
line 384: Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou, Wenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang, Dan Roth, and Dong Yu. 2023b. Sub-sentence en- coder: Contrastive learning of propositional semantic representations. arXiv preprint arXiv:2311.04335.
line 385:
line 386: Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. 2022. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 250-262, Abu Dhabi, United Arab Emi- rates. Association for Computational Linguistics.
line 387:
line 388: Hao Cheng, Hao Fang, Xiaodong Liu, and Jianfeng Gao. 2023. Task-aware specialization for efficient and robust dense retrieval for open-domain question
line 389:
line 390: answering. In Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1864-1875, Toronto, Canada. Association for Computational Linguistics.
line 391:
line 392: Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. 2021. Decontextualization: Making sen- tences stand-alone. Transactions of the Association for Computational Linguistics, 9:447-461.
line 393:
line 394: Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.
line 395:
line 396: Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few- shot dense retrieval from 8 examples. In The Eleventh International Conference on Learning Representa- tions.
line 397:
line 398: Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.
line 399:
line 400: Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extrac- tion from the web. Communications of the ACM, 51(12):68-74.
line 401:
line 402: Luyu Gao and Jamie Callan. 2022. Unsupervised cor- pus aware language model pre-training for dense pas- sage retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 2843-2853, Dublin, Ireland. Association for Computational Lin- guistics.
line 403:
line 404: Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence em- beddings. arXiv preprint arXiv:2104.08821.
line 405:
line 406: Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 512-520, Hong Kong. Association for Computational Linguistics.
line 407:
line 408: Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Ef- ficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 113-122.
line 409:
line 410: Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019. Poly-encoders: Trans- former architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint arXiv: 1905.01969.
line 411:
line 412: Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas- tian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense informa- tion retrieval with contrastive learning. Transactions on Machine Learning Research.
line 413:
line 414: Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open do- main question answering. In Proceedings of the 16th Conference of the European Chapter of the Associ- ation for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computa- tional Linguistics.
line 415:
line 416: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation.
line 417:
line 418: Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. arXiv preprint arXiv: 1705.03551.
line 419:
line 420: Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. Wice: Real-world entailment for claims in wikipedia. In Proceedings of the 2023 Con- ference on Empirical Methods in Natural Language Processing.
line 421:
line 422: Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open- domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.
line 423:
line 424: Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee. 2022. FiE: Building a global probability space by leverag- ing early fusion in encoder for open-domain question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process- ing, pages 4246-4260, Abu Dhabi, United Arab Emi- rates. Association for Computational Linguistics.
line 425:
line 426: Daniel Khashabi, Yeganeh Kordi, and Hannaneh Ha- jishirzi. 2022. Unifiedqa-v2: Stronger generalization via broader cross-format training. arXiv preprint arXiv:2202.12359.
line 427:
line 428: Omar Khattab and Matei Zaharia. 2020. Colbert: Effi- cient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39- 48.
line 429:
line 430: Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti,
line 431:
line 432: Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken- ton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453- 466.
line 433:
line 434: Jaewoong Lee, Heejoon Lee, Hwanhee Lee, and Ky- omin Jung. 2021a. Learning to select question- relevant relations for visual question answering. In Proceedings of the Third Workshop on Multimodal Artificial Intelligence, pages 87-96, Mexico City, Mexico. Association for Computational Linguistics.
line 435:
line 436: Jinhyuk Lee, Alexander Wettig, and Danqi Chen. 2021b. Phrase retrieval learns passage retrieval, too. In Pro- ceedings of the 2021 Conference on Empirical Meth- ods in Natural Language Processing, pages 3661- 3672, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
line 437:
line 438: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock- täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neu- ral Information Processing Systems, 33:9459-9474.
line 439:
line 440: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Küttler, Mike Lewis, Wen tau Yih, Tim Rock- täschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledge- intensive nlp tasks.
line 441:
line 442: Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. How to Train Your DRAGON: Di- verse Augmentation Towards Generalizable Dense Retrieval. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.
line 443:
line 444: Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts.
line 445:
line 446: Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, dense, and attentional representations for text retrieval. Transactions of the Association for Computational Linguistics, 9:329- 345.
line 447:
line 448: Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2022. Open-domain question an- swering via chain of reasoning over heterogeneous knowledge. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 5360- 5374, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
line 449:
line 450: Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, and Jianfeng Gao. 2023. Chain-of-skills: A configurable model for open-domain question an- swering. In Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1599-1618, Toronto, Canada. Association for Computational Linguistics.
line 451:
line 452: Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceed- ings of the 2023 Conference on Empirical Methods in Natural Language Processing.
line 453:
line 454: Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering am- biguous open-domain questions. In Proceedings of the 2020 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), pages 5783- 5797, Online. Association for Computational Lin- guistics.
line 455:
line 456: Stephen Mussmann and Stefano Ermon. 2016. Learning and inference via maximum inner product search. In International Conference on Machine Learning, pages 2587-2596. PMLR.
line 457:
line 458: Ani Nenkova and Rebecca Passonneau. 2004. Evaluat- ing content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chap- ter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 145-152, Boston, Mas- sachusetts, USA. Association for Computational Lin- guistics.
line 459:
line 460: Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human gener- ated machine reading comprehension dataset. CoRR, abs/1611.09268.
line 461:
line 462: Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9844-9855, Abu Dhabi, United Arab Emirates. As- sociation for Computational Linguistics.
line 463:
line 464: Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, and Yashar Mehdad. 2022. Domain-matched pre-training tasks for dense retrieval. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1524-1534, Seattle, United States. Association for Computational Linguistics.
line 465:
line 466: OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.
line 467:
line 468: Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human
line 469:
line 470: Language Technologies, pages 2523-2544, Online. Association for Computational Linguistics.
line 471:
line 472: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. The Journal of Machine Learning Research, 21(1):5485-5551.
line 473:
line 474: Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv: 1606.05250.
line 475:
line 476: Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Com- putational Linguistics.
line 477:
line 478: Devendra Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L. Hamil- ton, and Bryan Catanzaro. 2021. End-to-end training of neural retrievers for open-domain question answer- ing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 6648-6662, Online. Association for Computational Linguistics.
line 479:
line 480: Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. Col- BERTv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 3715-3734, Seat- tle, United States. Association for Computational Linguistics.
line 481:
line 482: Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Simple entity-centric ques- tions challenge dense retrievers. arXiv preprint arXiv:2109.08535.
line 483:
line 484: Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-time open-domain question answering with dense-sparse phrase index. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics, pages 4430-4441, Florence, Italy. Association for Computational Linguistics.
line 485:
line 486: Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In Inter- national Conference on Machine Learning, pages 31210-31227. PMLR.
line 487:
line 488: Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab- hishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Con- ference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).
line 489:
line 490: Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di- ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro- driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine- tuned chat models.
line 491:
line 492: Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 2345-2360, Seattle, United States. Association for Computational Linguistics.
line 493:
line 494: Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023. Learning to filter context for retrieval-augmented generation.
line 495:
line 496: Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita Sharma, Damien Jose, and Paul Bennett. 2022. Zero- shot dense retrieval with momentum adversarial do- main invariant representations. In Findings of the As- sociation for Computational Linguistics: ACL 2022, pages 4008-4020, Dublin, Ireland. Association for Computational Linguistics.
line 497:
line 498: Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor neg- ative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808.
line 499:
Page Content 1: Dense XX Retrieval: What Retrieval Granularity Should We Use?
===Tong Chen \*\* Hongwei WangSihao ChenWenhao Yu"Kaixin MaXinran Zhao^ Hongming ZhangDong Yu\*University of WashingtonTencent AI LabUniversity of Pennsylvania ^Carnegie Mellon University# AbstractDense retrieval has become a prominentmethod to obtain relevant context or worldknowledge in open-domain NLP tasks. Whenwe use a learned dense retriever on a retrievalcorpus at inference time, an often-overlookeddesign choice is the retrieval unit in which thecorpus is indexed, e.g. document, passage, orsentence. We discover that the retrieval unitchoice significantly impacts the performanceof both retrieval and downstream tasks. Dis-tinct from the typical approach of using pas-sages or sentences, we introduce a novel re-trieval unit, proposition, for dense retrieval.Propositions are defined as atomic expressionswithin text, each encapsulating a distinct fac-toid and presented in a concise, self-containednatural language format. We conduct an empir-ical comparison of different retrieval granular-ity. Our results reveal that proposition-basedretrieval significantly outperforms traditionalpassage or sentence-based methods in denseretrieval. Moreover, retrieval by propositionalso enhances the performance of downstreamQA tasks, since the retrieved texts are morecondensed with question-relevant information,reducing the need for lengthy input tokens andminimizing the inclusion of extraneous, irrele-vant information.# 1 IntroductionDense retrievers are a popular class of techniquesfor accessing external information sources forknowledge-intensive tasks (Karpukhin et al., 2020).Before we use a learned dense retriever to retrievefrom a corpus, an imperative design decision wehave to make is the retrieval unit - i.e. the granu-larity at which we segment and index the retrievalQuestion: What is the angle of the Tower of Pisa?PassageRetrievalPrior to restoration work performed be-tween 1990 and 2001, the tower leaned atan angle of 5.5 degrees, but the tower nowleans at about 3.99 degrees. This meansthe top of the Leaning Tower of Pisa is dis-placed horizontally 3.9 meters (12 ft 10 in)from the center.SentenceRetrievalPrior to restoration work performed be-tween 1990 and 2001, the tower leaned atan angle of 5.5 degrees, but the tower nowleans at about 3.99 degrees.PropositionRetrievalThe Leaning Tower of Pisa now leans atabout 3.99 degrees.<!-- FigureContent="PassageSentenceProposition70Passage RetrievalQuestion Answering40Recall@5 (%)60EM@100 (%)3050204010300ContrieverGTRContrieverGTR" -->Figure 1: (Top) An example of three granularities ofretrieval units of Wikipedia text when using dense re-trieval. (Bottom) We observe that retrieving by proposi-tions yields the best retrieval performance in both pas-sage retrieval task and downstream open-domain QAtask, e.g. with Contriever (Izacard et al., 2022) or GTR(Ni et al., 2022) as the backbone retriever. Highlightindicates the part that contains answer to the question.corpus for inference. In practice, the choice of re-trieval unit, e.g. documents, fixed-length passagechunks or sentences, etc, is usually pre-determinedbased on how the dense retrieval model is instanti-ated or trained (Lewis et al., 2020; Lee et al., 2021a;Santhanam et al., 2022; Ni et al., 2022).In this paper, we investigate an overlooked re-search question with dense retrieval inference - atwhat retrieval granularity should we segment andindex the retrieval corpus? We discover that se-lecting the proper retrieval granularity at inferencetime can be a simple yet effective strategy for im-<!-- Footnote="\* Work was done during internship at Tencent AI Lab,Bellevue." --><!-- Footnote="https://github.com/ct123098/factoid-wiki" -->## arXiv:2312.06648v2 [cs.CL] 12 Dec 2023
Metadata 1: {'page': 1}
Page Content 2: <!-- FigureContent="APrior to restoration work performed between1990 and 2001, the tower leaned at an angle of5.5 degrees ,but the tower now leans atBCorpusCQueryabout 3.99 degrees.This means the top of theLearning Tower of Pisa is displaced horizontally?RetrievalWikipediaUnits3.9 meters (12 ft 10 in) from the center.=Passage RetrievalRetriever1. Prior to restoration work performedbetween 1990 and 2001, the Leaning TowerProposition-izerRetrievalDof Pisa leaned at an angle of 5.5 degrees.UnitsQA Model2. The Leaning Tower of Pisa now leans atabout 3.99 degrees.QueryAnswer?Sentences>✓3. The top of the Leaning Tower of Pisa isRetrieval Unitsdisplaced horizontally 3.9 meters (12 ft 10 in)from the center.FactoidWikiPassagesPropositions" -->Figure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yeteffective strategy to increase dense retrievers' generalization performance at inference time (A, B). We empiricallycompare the retrieval and downstream open-domain QA tasks performance when dense retrievers work withWikipedia indexed at the level of 100-word passage, sentence or proposition (C, D).proving dense retrievers' retrieval and downstreamtask performance. We illustrate our intuition withan example of open-domain question-answering(QA) in Table 1. The example shows retrieved textby the same model at three different granularities.The passage, which represents a coarser retrievalunit with a longer context, is theoretically able toprovide more relevant information for the ques-tion. However, a passage often includes extraneousdetails (e.g., restoration period and horizontal dis-placement in the example of Table 1) that could po-tentially distract both the retriever and the languagemodel in downstream tasks (Shi et al., 2023; Yuet al., 2023b). On the other hand, sentence-level in-dexing provides a finer-grained approach but doesnot entirely address the issue (Akkalyoncu Yilmazet al., 2019; Yang et al., 2020). This is because sen-tences can still be complex and compounded, andthey are often not self-contained, lacking necessarycontextual information (e.g., in the example of Ta-ble 1, "the tower" is coreference of "Pisa Tower")for judging the query-document relevance.To address these shortcomings of typical re-trieval units such as passages or sentences, wepropose using proposition as a novel retrieval unitfor dense retrieval. Propositions are defined asatomic expressions within text, each encapsulat-ing a distinct factoid and presented in a concise,self-contained natural language format. We showan example proposition in Table 1. The proposi-tion describes the information regarding the Towerof Pisa's current leaning angle in a self-containedway and precisely responds to what the questionis querying. We provide a more detailed definitionand description of proposition in §2.To validate the efficacy of using proposition asa retrieval unit for dense retrievers inference, wefirst process and index an English Wikipedia dumpwith all documents segmented into propositions,which we refer to as FACTOIDWIKI. Then we con-duct experiments on five different open-domain QAdatasets and empirically compare the performanceof six dual-encoder retrievers when Wikipedia isindexed by passage, sentence, and our proposedproposition. Our evaluation is twofold: we exam-ine both the retrieval performance and the impacton downstream QA tasks. Notably, our findings in-dicate that proposition-based retrieval outperformssentence and passage-based methods, especially interms of generalization, as discussed in §5. Thissuggests that propositions, being both compact andrich in context, enable dense retrievers to accessprecise information while maintaining adequatecontext. The average improvement over passage-based retrieval of Recall@20 is +10.1 on unsu-pervised dense retrievers and +2.2 on supervisedretrievers. Furthermore, we observe a distinct ad-vantage in downstream QA performance when us-ing proposition-based retrieval, as elaborated in §6.Given the often limited input token length in lan-guage models, propositions inherently provide ahigher density of question-relevant information.Our main contributions in the paper are:· We propose using propositions as retrieval unitswhen indexing a retrieval corpus to improve thedense retrieval performance.· We introduce FACTOIDWIKI, a processed En-glish Wikipedia dump, where each page is seg-mented into multiple granularities: 100-word pas-sages, sentences and propositions.
Metadata 2: {'page': 2}
Page Content 3: · We discover that retrieval by proposition outper-forms passage or sentence retrieval in terms ofgeneralization for passage retrieval and accuracyfor downstream question-answering given thesame input token limit.# 2 Proposition as a Retrieval UnitThe goal of our study is to understand how the gran-ularity of a retrieval corpus influences the denseretrieval models' performance empirically. Asidefrom commonly-used retrieval units such as 100-word passage (Karpukhin et al., 2020) or sentence,we propose using proposition as an alternative re-trieval unit choice. Here, propositions representatomic expressions of meanings in text (Min et al.,2023) that are defined by the three principles below.1\. Each proposition should correspond to a distinctpiece of meaning in text, where the compositionof all propositions would represent the seman-tics of the entire text.2\. A proposition should be minimal, i.e. it cannotbe further split into separate propositions.3\. A proposition should be contextualized and self-contained (Choi et al., 2021). A propositionshould include all the necessary context from thetext (e.g. coreference) to interpret its meaning.The use of proposition as a retrieval unit is inspiredby a recent line of work (Min et al., 2023; Kamoiet al., 2023; Chen et al., 2023a,b), which finds suc-cess in representing and evaluating text semanticsat the level of propositions. We demonstrate theconcept of proposition and how a passage can besplit into its set of propositions by an example onthe left side of Figure 2. The passage containsthree propositions, each of which corresponds toa distinct factoid about the Leaning Tower of Pisa:the angle before the restoration, the current an-gle, and the horizontal displacement. Within eachproposition, necessary context from the passage isincorporated so that the meaning of the propositioncan be interpreted independently of the originaltext, e.g. the reference of the tower is resolved intoits full mention, the Leaning Tower of Pisa, in thefirst proposition. We expect each proposition to de-scribe exactly one contextualized atomic fact, andso our intuition is that propositions would suitablywork as a retrieval unit for information-seekingquestions.# 3FACTOID WIKI: Proposition-LevelIndex and Retrieval for WikipediaWe empirically compare the use of 100-word pas-sages, sentences, and propositions as retrieval unitson Wikipedia, a commonly-used retrieval sourcefor knowledge-intensive NLP tasks (Petroni et al.,2021). To allow for a fair comparison across gran-ularities, we process an English Wikipedia dumpfrom 2021-10-13, as used by Bohnet et al. (2022).We segment each document text into three differentgranularities: 100-word passages, sentences, andpropositions. We include the details on passage-and sentence-level segmentation of the corpus inAppendix A.Parsing Passage to Propositions. To segmentthe Wikipedia pages into propositions, we finetunea text generation model, which we refer to as thePropositionizer. The Propositionizer takes a pas-sage as input and generates the list of propositionswithin the passage. Following Chen et al. (2023b),we train the Propositionizer with a two-step distil-lation process. We first prompt GPT-4 (OpenAI,2023) with an instruction containing the propo-sition definition and 1-shot demonstrations. Weinclude the details of the prompt in Figure 8. Westart with a set of 42k passages and use GPT-4 togenerate the seed set of paragraph-to-propositionspairs. Next, we use the seed set to finetune a Flan-T5-large model (Chung et al., 2022).We refer to the processed corpus as FACTOID-WIKI. The resulting statistics of FACTOID WIKI areshown in Table 1.# unitsAvg. # wordsPassage41,393,52858.5Sentence114,219,12721.0Proposition256,885,00311.2Table 1: Statistics of text units in the English Wikipediadump from 2021-10-13.# 4 Experimental SettingsTo evaluate the impact of the three retrieval unitchoices, we conduct experiments on five differ-ent open-domain QA datasets with FACTOIDWIKI.With each dataset, we evaluate both passage re-trieval and downstream QA performance whendense retrievers work with Wikipedia indexed indifferent granularities.
Metadata 3: {'page': 3}
Page Content 4: ## 4.1 Open-Domain QA DatasetsWe evaluate on five different open-domain QAdatasets with Wikipedia as the retrieval source:Natural Questions (NQ) (Kwiatkowski et al.,2019), TriviaQA (TQA) (Joshi et al., 2017),Web Questions (WebQ) (Berant et al., 2013),SQUAD (Rajpurkar et al., 2016), and Entity Ques-tions (EQ) (Sciavolino et al., 2021).## 4.2 Dense Retrieval ModelsWe compare the performance of the six followingsupervised or unsupervised dense retriever mod-els. Here, supervised models refer to ones thathave used human-labeled query-passage pairs assupervision during training, and vice versa.· SimCSE (Gao et al., 2021) is a BERT-base (De-vlin et al., 2019) encoder trained on unlabeledsentence randomly sampled from Wikipedia.· Contriever (Izacard et al., 2022) is an unsuper-vised retriever, instantiated with a BERT-baseencoder. Contriever is contrastively trained bysegment pairs constructed from unlabeled docu-ments from Wikipedia and web crawl data.· DPR (Karpukhin et al., 2020) is a dual-encoderBERT-base model finetuned on five open-domainQA datasets, which includes four of the datasets(NQ, TQA, WebQ and SQUAD) in our evalua-tion.· ANCE (Xiong et al., 2020) used the same settingfrom DPR and trained the model with Approxi-mate nearest neighbor Negative Contrastive Esti-mation (ANCE), a hard negatives-based trainingapproach.· TAS-B (Hofstätter et al., 2021) is a dual-encoder BERT-base model distilled from ateacher model with cross-attention trained on MSMARCO (Nguyen et al., 2016).· GTR (Hofstätter et al., 2021) is a T5-base en-coder (Raffel et al., 2020) pretrained on unla-beled pairs of online forum QA data, and fine-tuned on MS MARCO and Natural Question.## 4.3 Passage Retrieval EvaluationWe evaluate retrieval performance at the passagelevel when the corpus is indexed at the passage,sentence, or proposition level respectively. For sen-tence and proposition level retrieval, we follow thesetting introduced in Lee et al. (2021b), where thescore of the passage is based on the maximum sim-ilarity score between the query and all sentencesor propositions in a passage. In practice, we firstretrieve a slightly larger number of text units, mapeach unit to the source passage, and return the top-kunique passages. We use Recall@k as our evalu-ation metric, which is defined as the percentageof questions for which the correct answer is foundwithin the top-k retrieved passages.## 4.4 Downstream QA EvaluationTo understand the implications of using differentretrieval units on the downstream open-domainQA tasks, we evaluate the use of retrieval mod-els in retrieve-then-read setup (Izacard and Grave,2021). With the retrieve-then-read setting, a re-trieval model first retrieves k text units given thequery. The k retrieved text units are then used asinput along with the query to a reader model toderive the final answer. Typically, the choice ofk is subject to the reader model's maximum inputlength constraint, or the limit of compute budget,which scales with the number of input tokens.For this reason, we follow an evaluation setupwhere the maximum number of retrieved wordsis capped at l = 100 or 500, i.e. only the top lwords from passage, sentence, or proposition levelretrieval are feed into the reader model as input. Weevaluate the percentage of questions for which thepredicted answer exactly matches (EM) the groundtruth. We denote our metric as EM @ l. For ourevaluation, we use T5-large size UnifiedQA-v2 asthe reader model (Khashabi et al., 2022).# 5 Results: Passage RetrievalIn this section, we report and discuss the retrievaltasks performance. Our results show that despitenone of the models being trained with proposition-level data, all the retrieval models demonstratedon-par or superior performance when the corpus isindexed at the proposition level.## 5.1 Passage Retrieval PerformanceWe report our evaluation results in Table 2. Weobserve that retrieval by propositions outperformsretrieval by sentence or passage on most tasks forboth unsupervised and supervised retrievers.With all dense retrievers tested, proposition-level retrieval consistently outperforms sentenceand passage-level retrieval on average across thefive datasets. With the unsupervised retrievers, i.e.SimCSE and Contriever, we see an averaged Re-call@5 improvement of +12.0 and +9.3 (35.0%
Metadata 4: {'page': 4}
Page Content 5: RetrieverGranularityNOTỌAWebQSQUADEQAvg.R@5 R@20R@5 R@20R@5 R@20R@5 R@20R@5 R@20R@5 R@20Unsupervised Dense RetrieversSimCSEPassage28.844.344.959.439.856.029.545.528.440.334.349.1Sentence35.553.150.564.345.364.137.152.336.350.140.956.8Proposition41.158.952.466.550.066.838.753.949.562.246.361.7ContrieverPassage42.563.858.173.737.160.640.859.836.356.343.062.8Sentence46.466.860.675.741.763.145.163.542.761.347.366.1Proposition50.170.065.177.945.966.850.767.751.770.152.770.5Supervised Dense RetrieversDPRPassage66.078.071.680.262.974.938.353.947.560.457.369.5Sentence66.078.071.880.564.174.440.355.953.766.059.271.0Proposition65.477.770.779.662.875.141.457.259.471.359.972.2ANCEPassage70.781.473.981.465.777.243.358.657.069.162.173.5Sentence70.381.673.981.565.277.445.860.761.472.863.374.8Proposition69.981.172.880.665.177.146.261.966.776.664.175.5TAS-BPassage64.277.970.479.365.177.054.369.272.281.365.276.9Sentence64.078.471.480.263.976.758.972.372.782.066.277.9Proposition63.878.671.480.063.876.859.873.475.183.366.878.4GTRPassage66.378.470.179.463.376.554.468.171.780.565.276.6Sentence66.479.471.680.962.276.860.973.472.581.366.778.4Proposition66.579.672.280.963.277.463.375.074.983.068.079.2Table 2: Passage retrieval performance (Recall@k = 5, 20) on five different open-domain QA datasets whenpre-trained dense retrievers work with the three different granularity from the retrieval corpus. Underline denotescases where the training split of the target dataset was included in the training data of the dense retriever.<!-- FigureContent="\- Passage-- Sentence-- PropositionSimCSEContrieverDPRANCETAS-B60GTR60708080Recall@5Recall@5Recall@560Recall@5Recall@5Recall@5406070704050506060202040405050101 102 103101 102 103101 102 103101 102 103101 102 103101 102 103PopularityPopularityPopularityPopularityPopularityPopularity" -->Figure 3: Document retrieval recall vs. the popularity of the target entity in each question from the EntityQuestionsdataset. The popularity of each entity (i.e. smaller value = less common entities, and vice versa) is estimated bythe occurrence of the entity in its top-1000 passage retrieved by BM25. On queries with less common entities, weobserve that retrieving by proposition shows a larger advantage over retrieval by proposition.and 22.5% relative improvement) respectively overfive datasets.With the supervised retrievers, proposition-levelretrieval still shows an advantage on average, yetthe sizes of improvements are smaller. We hypothe-size that this is due to these retrievers being trainedon query-passage pairs. For instance, with DPRand ANCE, which have been trained on NQ, TQA,WebQ, and SQUAD, we observe that propositionand sentence level retrieval perform slightly worsecompared to passage level on three out of the fourdatasets, with the exception of SQUAD. As shownin Table 2, all supervised retrievers demonstratecomparable performance across three levels of re-trieval granularity in NQ, TQA, and WebQ.However, on datasets that the retriever model hasnot seen during training, we observe that retrievalby proposition demonstrates a clear advantage. Forinstance, most notably on SQUAD or EntityQues-tions, we observe that proposition-based retrievalsignificantly outperforms the other two granulari-ties. We see 17-25% Recall@5 relative improve-ment on EntityQuestions with relatively weak re-trievers like DPR and ANCE. Furthermore, theRecall@5 of retrieval by proposition on SQUADimproved most on TAS-B and GTR, with 10-16%relative improvements.## 5.2 Retrieval by Proposition => BetterCross-Task GeneralizationOur results indicate that the advantage of retrievalby proposition becomes most visible in cross-task generalization settings. We observe that onSQUAD and EntityQuestions, retrieval by proposi-
Metadata 5: {'page': 5}
Page Content 6: RetrieverGranularityNQTQAWebQSQUADEQAvg.EMEMEMEMEMEM@100 @500@100@500@100@500@100@500@100@500@100 @500Unsupervised Dense RetrieversSimCSEPassage8.116.322.633.77.714.99.817.810.917.511.820.0Sentence10.118.027.237.29.615.617.324.813.019.815.423.1Proposition12.720.228.437.711.217.218.025.118.325.017.725.0ContrieverPassage11.122.425.741.46.814.915.627.710.921.514.025.6Sentence13.823.930.544.29.117.222.632.812.222.217.628.1Proposition16.526.137.748.713.319.925.634.416.127.321.831.3Supervised Dense RetrieversDPRPassage24.836.140.351.014.022.212.421.718.625.922.031.4Sentence27.635.944.652.816.323.718.626.121.828.225.833.3Proposition28.334.345.751.919.023.819.826.326.331.927.833.6ANCEPassage27.138.343.153.115.223.015.326.023.431.124.834.3Sentence30.137.347.054.716.623.822.930.525.932.028.535.7Proposition29.837.047.453.519.324.122.930.129.133.729.735.7TAS-BPassage21.133.939.350.513.120.723.934.630.937.325.735.4Sentence24.633.943.652.314.421.433.840.531.436.129.636.8Proposition26.634.044.951.818.123.734.238.934.237.831.637.2GTRPassage23.434.538.749.313.120.123.933.831.336.726.134.9Sentence26.835.143.952.215.921.635.641.331.335.130.737.1Proposition29.534.445.952.618.723.837.040.434.137.133.037.7Table 3: Open-domain QA performance (EM = Exact Match) under retrieve-then-read setting where the number ofretrieved words to the reader QA model is limited at l = 100 or 500. We use UnifedQA V2 (Khashabi et al., 2022)as the reader model. The first l words from the concatenated top retrieved text unit are feed as input to the readermodel. Underline denotes cases where the training split of the target dataset was included in the training data of thedense retriever. In most cases, we see better QA performance with smaller retrieval units.tion brings more performance gain over retrievalby passage.To better understand where the improvementscan be attributed, we conduct an additional analysison EntityQuestions. As EntityQuestions featuresquestions targeting the properties of longer-tail enti-ties, we study how the retrieval performance underthree different granularities is affected by the popu-larity of the target entity in question, i.e. whetherthe entity appears frequently in Wikipedia or not.We estimate the popularity of each entity with thefollowing method. Given the surface form of an en-tity, we use BM25 to retrieve the top-1000 relevantpassages from Wikipedia. We use the number ofoccurrences of the entity in its top-1000 passagesas an estimate of its popularity. With the 20,000test queries in EntityQuestion, around 25% of thetarget entities have a popularity value of less orequal to 3.Figure 3 shows the passage retrieval perfor-mance vs. the popularity of the target entity ineach question. Across all 6 dense retrievers, we ob-serve that retrieving by proposition shows a muchlarger advantage over retrieving by passage withquestions targeting less common entities. As thepopularity of entities increases, the performancegap decreases. Our findings indicate that the per-formance gain from retrieval by proposition canmostly be attributed to queries for long-tailed infor-mation. This echoes our observation that retrievalby proposition improves the cross-task generaliza-tion performance of dense retrievers.# 6 Results: Open-Domain QAIn this section, we study how the choice of retrievalgranularity affects downstream open-domain QAtasks. We show that retrieval by proposition leadsto strong downstream QA performance in theretrieve-then-read setting, where the number of re-trieved tokens for input to the reader QA model iscapped at l = 100 or 500 words.## 6.1 QA PerformanceTable 3 shows the evaluation results. Across dif-ferent retrievers, we observe higher QA perfor-mance in terms of the EM@l metric on averagewhen using propositions as the retrieval unit. Theunsupervised retrievers, SimCSE and Contriever,demonstrate improvements of +5.9 and +7.8 inthe EM@100 score (50% and 55% relative im-provement), respectively. The supervised retriev-ers, DPR, ANCE, TAS-B, and GTR, improve +5.8,
Metadata 6: {'page': 6}
Page Content 7: PassageSentence\-Proposition<!-- FigureContent="GTR / NQGTR / TQAGTR / WebQGTR / SQUADGTR / EQ707080Recall (%)Recall (%)70Recall (%)Recall (%)60Recall (%)60607060505050406002004000200400020040002004000200400#Words#Words#Words#Words#Words" -->Figure 4: Recall of the gold answer in the retrieved text limited to first k words for the GTR retriever. Finer-grainedretrieval has a higher recall across all numbers of words.\+4.9, +5.9, and +6.9 EM@100 (26%, 19%, 22%,26% relative improvement), respectively. Similarto our observations from passage retrieval evalu-ations, we find retrieval by proposition becomesmore beneficial to downstream QA performancewhen the retriever has not been trained on the targetdataset. In other cases, retrieval by proposition stillholds an advantage, but with a smaller margin onaverage.## 6.2 Propositions => Higher Density ofQuestion-Related InformationIntuitively, compared to sentences or passages asretrieval units, the advantage of propositions is thatthe retrieved propositions have a higher densityof relevant information to the query. With finer-grained retrieval units, the correct answer to thequery would more likely appear in the top-l re-trieved words by a dense retriever.We illustrate this phenomenon by an analysisshown in Figure 4. Here, we investigate the posi-tion at which the ground truth answer appears inthe top-l retrieved words. Specifically, we calcu-late the recall of the gold answer within the initial lretrieved words with GTR working with Wikipediaindexed in three different granularities.We show the results in Figure 4 and Figure 7 withl ranging from 0 to 500 across all five datasets. Fora fixed retrieved word budget, proposition retrievaldemonstrates a higher success rate compared tosentence and passage retrieval methods. The mostsignificant improvement of proposition retrievalover passage retrieval occurs within the range of100-200 words, which corresponds to roughly 10propositions, 5 sentences, or 2 passages. As theword count further increases, the recall rates ofthe three granularity converge since all question-relevant information is included in the retrievedtext.## 6.3 Error Case StudyTo understand the source of errors from each typeof retrieval granularity, we present and discuss fourtypical examples of mistakes in Table 4. With eachexample, we show the question and its correspond-ing top-1 retrieved text unit by the GTR retrieveracross the three granularities.We observe that with passage-level retrieval, theambiguity of an entity or its references presents achallenge for dense retrievers, which echoes find-ings from (Min et al., 2020). For instance, in exam-ple Q1, the question asks for "Super Bowl 50", butthe retrieved passage and sentence refers to "SuperBowl 5". In Example Q2, passage retrieval failsto identify the part referring to the correct "atomicnumber". Instead, the top-1 retrieved passage men-tions "atomic number" in a different and irrelevantcontext to the question. Retrieval by sentences canalso have a similar problem as retrieval by passageslike Example Q1. Also, retrieval by sentences facesanother challenge of lacking context. In ExampleQ3, sentence-based retrieval fails as the correct sen-tence in the retrieved passage uses "it" to refer tothe pericardial sac.Retrieval by propositions tackles the aforemen-tioned problems by ensuring each retrieval unitcontains one piece of fact only and necessary con-text is incorporated in the propositions. However,proposition-based retrieval faces challenges withquestions that involve multi-hop reasoning overlong-range textual analysis. In Example Q4, theretrieved passage separately describes the actor'sname and the character they portray. There is nota single proposition that entails both the questionand the answer.# 7 Related WorkRecent works on dense retrievers typically adopta dual-encoder architecture (Yih et al., 2011;Reimers and Gurevych, 2019; Karpukhin et al.,2020; Ni et al., 2022). With dual-encoders,
Metadata 7: {'page': 7}
Page Content 8: Passage RetrievalSentence RetrievalProposition RetrievalQ1: What was the theme of Super Bowl 50?Title: Super Bowl X✗The overall theme of the Super Bowl enter-tainment was to celebrate the United StatesBicentennial. Each Cowboys and Steelersplayer wore a special patch with the Bicen-tennial logo on their jerseys ...Title: Super Bowl X✗The overall theme of the Super Bowlentertainment was to celebrate theUnited States Bicentennial.Title: Super Bowl XLV✓... As this was the 50th Super Bowl game,the league [Super Bowl 50] emphasizedthe "golden anniversary" with various gold-themed initiatives during the 2015 season, aswell as ...Q2: The atomic number of indium which belongs to 5th period is?Title: Period 5 element✗The periodic table is laid out in rows to illus-trate recurring (periodic) trends in the chemi-cal behaviour of the elements as their atomicnumber increases: ...Title: Period 5 element✓Indium is a chemical element with thesymbol In and atomic number 49.Title: Period 5 element✓Indium is a chemical element with the sym-bol In and [Indium has a] atomic number 49.This rare, very soft, malleable ...Q3: What is the function of the pericardial sac?Title: Pericardium✓The pericardium, also called pericardial sac... It separates the heart from interference ofother structures, protects it against infectionTitle: Pericardium✗The pericardium, also called pericar-dial sac, is a double-walled sac con-taining the heart and the roots of thegreat vessels.Title: Cardiac muscle✓On the outer aspect of the myocardium is theepicardium which forms part of the pericar-dial sac that surrounds, protects, and lubri-and blunt trauma, and lubricates the heart'scates the heart.movements.Q4: What is the main character's name in layer cake?Title: Layer Cake (film)✓... The film's plot revolves around a London-based criminal, played by Daniel Craig, ...Craig's character is unnamed in the film andis listed in the credits as "XXXX".Title: Angelic Layer✗The primary protagonist is MisakiSuzuhara.Title: Plot twist✗Sometimes the audience may discover thatthe true identity of a character is , in fact,unknown [in Layer Cake] , as in Layer Cakeor the eponymous assassins in V for Vendettaand The Day of the Jackal.Table 4: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correctanswer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustrationpurpose only and not provided to the retrievers and downstream QA models.each query and document is encoded into a low-dimensional feature vector respectively, and theirrelevance is measured by a non-parametric similar-ity function between the embedding vectors (Muss-mann and Ermon, 2016). Due to the limited expres-sivity from the similarity function, dual encodermodels often generalize poorly to new tasks withscarce training data (Thakur et al., 2021). To thisend, previous studies use techniques such as dataaugmentation (Wang et al., 2022; Yu et al., 2023a;Izacard et al., 2022; Gao and Callan, 2022; Linet al., 2023; Dai et al., 2023), continual pre-training(Chang et al., 2020; Sachan et al., 2021; Oguz et al.,2022), task-aware training (Xin et al., 2022; Chenget al., 2023), hybrid sparse-dense retrieval (Luanet al., 2021; Chen et al., 2022) or mixed strategy re-trieval (Ma et al., 2022, 2023) and so on to improvecross-task generalization performance of dense re-trievers.The motivation of our work echoes in part withmulti-vector retrieval, e.g. ColBERT (Khattab andZaharia, 2020), DensePhrase (Lee et al., 2021a,b),ME-BERT (Luan et al., 2021), MVR (Zhang et al.,2022), where the retrieval model learns to encodea candidate retrieval unit into multiple vectors toincrease model expressivity and improve retrievalgranularity (Seo et al., 2019; Humeau et al., 2019).Our work instead focuses on the setting where wedo not update the dense retriever model or its pa-rameters. We show that segmenting the retrievalcorpus into finer-grained units of proposition canbe a simple and orthogonal strategy for improvingthe generalization of dual encoders dense retrieversat inference time.The idea of using propositions as a unit of textrepresentation dates back to the Pyramid methodin summarization evaluation (Nenkova and Passon-neau, 2004), where model generated summary isevaluated by each proposition. Proposition extrac-tion from text has been a long-standing task in NLP,with earlier formulations focusing on a structuredrepresentation of propositions, e.g. Open Infor-mation Extraction (Etzioni et al., 2008), SemanticRole Labeling (Gildea and Jurafsky, 2000). Morerecent studies have found success in extractingpropositions in natural language form via few-shotprompting with large language models (Min et al.,2023; Kamoi et al., 2023), or finetuning smaller
Metadata 8: {'page': 8}
Page Content 9: compact-sized models (Chen et al., 2023b).Retrieve-then-read, or more broadly - retrievalaugmented generation, has recently merged as apopular paradigm for open-domain question an-swering (Lewis et al., 2021; Jiang et al., 2023;Asai et al., 2023). While earlier works provideup to the top 100 retrieved passages for the down-stream reader (Izacard and Grave, 2021; Kediaet al., 2022), the amount of allowed context issignificantly reduced when using recent large lan-guage models (e.g. top 10) (Touvron et al., 2023;Yu et al., 2023b), due to their limited context win-dow length and inability to reason over long con-text (Liu et al., 2023). Recent efforts have tried toimprove the quality of the reader context by filter-ing or compressing the retrieved documents (Wanget al., 2023; Xu et al., 2023). Our work offers anew perspective by leveraging a new retrieval unit,the proposition that not only reduces the contextlength but also offers greater information density,effectively addressing the issue.# 8 ConclusionWe propose the use of propositions as retrieval unitsfor indexing corpus to improve dense retrieval per-formance at inference time. Through our experi-ments on five open-domain QA datasets with sixdifferent dense retrievers, we discovered that re-trieval by proposition outperforms passage or sen-tence in both passage retrieval accuracy and down-stream QA performance with a fixed retrieved wordbudget. We introduce FACTOIDWIKI, an indexedversion of the English Wikipedia dump, where textfrom 6 million pages is segmented into 250 millionpropositions. We hope that FACTOIDWIKI, alongwith our findings in the paper, will facilitate futureresearch on information retrieval.# LimitationsThe scope of our current study on the granular-ity of retrieval corpus has the following limita-tions. (1) Retrieval Corpus - Our study only focuson Wikipedia as the retrieval corpus, due to thefact that most open-domain QA datasets adoptsWikipedia as the retrieval corpus. (2) Types ofdense retrievers evaluated - In the current versionof the paper, we only evaluate on 6 types of populardense retrievers, most of which follow bi- or dual-encoder architecture. In future versions, we willinclude and discuss results on a broader range ofdense retrievers. (3) Language - Our current studyis limited to English Wikipedia only. We leave theexploration on other languages to future work.# ReferencesZeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang,and Jimmy Lin. 2019. Cross-domain modeling ofsentence-level evidence for document retrieval. InProceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 3490-3496, Hong Kong, China. Association for Computa-tional Linguistics.Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, andHannaneh Hajishirzi. 2023. Self-rag: Learning to re-trieve, generate, and critique through self-reflection.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang. 2013. Semantic parsing on freebase fromquestion-answer pairs. In Proceedings of the 2013conference on empirical methods in natural languageprocessing, pages 1533-1544.Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,Daniel Andor, Livio Baldini Soares, Jacob Eisenstein,Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al.2022. Attributed question answering: Evaluation andmodeling for attributed large language models. arXivpreprint arXiv:2212.08037.Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yim-ing Yang, and Sanjiv Kumar. 2020. Pre-training tasksfor embedding-based large-scale retrieval. In Inter-national Conference on Learning Representations.Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, DanRoth, and Tal Schuster. 2023a. PropSegmEnt: Alarge-scale corpus for proposition-level segmentationand entailment recognition. In Findings of the As-sociation for Computational Linguistics: ACL 2023,pages 8874-8893, Toronto, Canada. Association forComputational Linguistics.Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou,Wenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang,Dan Roth, and Dong Yu. 2023b. Sub-sentence en-coder: Contrastive learning of propositional semanticrepresentations. arXiv preprint arXiv:2311.04335.Xilun Chen, Kushal Lakhotia, Barlas Oguz, AnchitGupta, Patrick Lewis, Stan Peshterliev, YasharMehdad, Sonal Gupta, and Wen-tau Yih. 2022.Salient phrase aware dense retrieval: Can a denseretriever imitate a sparse one? In Findings of theAssociation for Computational Linguistics: EMNLP2022, pages 250-262, Abu Dhabi, United Arab Emi-rates. Association for Computational Linguistics.Hao Cheng, Hao Fang, Xiaodong Liu, and JianfengGao. 2023. Task-aware specialization for efficientand robust dense retrieval for open-domain question
Metadata 9: {'page': 9}
Page Content 10: answering. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 2: Short Papers), pages 1864-1875, Toronto,Canada. Association for Computational Linguistics.Eunsol Choi, Jennimaria Palomaki, Matthew Lamm,Tom Kwiatkowski, Dipanjan Das, and MichaelCollins. 2021. Decontextualization: Making sen-tences stand-alone. Transactions of the Associationfor Computational Linguistics, 9:447-461.Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.2022. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416.Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, JianmoNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall,and Ming-Wei Chang. 2023. Promptagator: Few-shot dense retrieval from 8 examples. In The EleventhInternational Conference on Learning Representa-tions.Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages4171-4186, Minneapolis, Minnesota. Association forComputational Linguistics.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S Weld. 2008. Open information extrac-tion from the web. Communications of the ACM,51(12):68-74.Luyu Gao and Jamie Callan. 2022. Unsupervised cor-pus aware language model pre-training for dense pas-sage retrieval. In Proceedings of the 60th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 2843-2853,Dublin, Ireland. Association for Computational Lin-guistics.Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.Simcse: Simple contrastive learning of sentence em-beddings. arXiv preprint arXiv:2104.08821.Daniel Gildea and Daniel Jurafsky. 2000. Automaticlabeling of semantic roles. In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics, pages 512-520, Hong Kong. Associationfor Computational Linguistics.Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-HongYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-ficiently teaching an effective dense retriever withbalanced topic aware sampling. In Proceedings ofthe 44th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 113-122.Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,and Jason Weston. 2019. Poly-encoders: Trans-former architectures and pre-training strategies forfast and accurate multi-sentence scoring. arXivpreprint arXiv: 1905.01969.Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-tian Riedel, Piotr Bojanowski, Armand Joulin, andEdouard Grave. 2022. Unsupervised dense informa-tion retrieval with contrastive learning. Transactionson Machine Learning Research.Gautier Izacard and Edouard Grave. 2021. Leveragingpassage retrieval with generative models for open do-main question answering. In Proceedings of the 16thConference of the European Chapter of the Associ-ation for Computational Linguistics: Main Volume,pages 874-880, Online. Association for Computa-tional Linguistics.Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,Qian Liu, Jane Dwivedi-Yu, Yiming Yang, JamieCallan, and Graham Neubig. 2023. Active retrievalaugmented generation.Mandar Joshi, Eunsol Choi, Daniel S Weld, and LukeZettlemoyer. 2017. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehen-sion. arXiv preprint arXiv: 1705.03551.Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, andGreg Durrett. 2023. Wice: Real-world entailment forclaims in wikipedia. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing.Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 6769-6781,Online. Association for Computational Linguistics.Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee. 2022.FiE: Building a global probability space by leverag-ing early fusion in encoder for open-domain questionanswering. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Process-ing, pages 4246-4260, Abu Dhabi, United Arab Emi-rates. Association for Computational Linguistics.Daniel Khashabi, Yeganeh Kordi, and Hannaneh Ha-jishirzi. 2022. Unifiedqa-v2: Stronger generalizationvia broader cross-format training. arXiv preprintarXiv:2202.12359.Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-cient and effective passage search via contextualizedlate interaction over bert. In Proceedings of the 43rdInternational ACM SIGIR conference on researchand development in Information Retrieval, pages 39-48.Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti,
Metadata 10: {'page': 10}
Page Content 11: Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-ton Lee, et al. 2019. Natural questions: a benchmarkfor question answering research. Transactions of theAssociation for Computational Linguistics, 7:453-466.Jaewoong Lee, Heejoon Lee, Hwanhee Lee, and Ky-omin Jung. 2021a. Learning to select question-relevant relations for visual question answering. InProceedings of the Third Workshop on MultimodalArtificial Intelligence, pages 87-96, Mexico City,Mexico. Association for Computational Linguistics.Jinhyuk Lee, Alexander Wettig, and Danqi Chen. 2021b.Phrase retrieval learns passage retrieval, too. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 3661-3672, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics.Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-täschel, et al. 2020. Retrieval-augmented generationfor knowledge-intensive nlp tasks. Advances in Neu-ral Information Processing Systems, 33:9459-9474.Patrick Lewis, Ethan Perez, Aleksandra Piktus, FabioPetroni, Vladimir Karpukhin, Naman Goyal, Hein-rich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-täschel, Sebastian Riedel, and Douwe Kiela. 2021.Retrieval-augmented generation for knowledge-intensive nlp tasks.Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and XilunChen. 2023. How to Train Your DRAGON: Di-verse Augmentation Towards Generalizable DenseRetrieval. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing.Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and PercyLiang. 2023. Lost in the middle: How languagemodels use long contexts.Yi Luan, Jacob Eisenstein, Kristina Toutanova, andMichael Collins. 2021. Sparse, dense, and attentionalrepresentations for text retrieval. Transactions of theAssociation for Computational Linguistics, 9:329-345.Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg,and Jianfeng Gao. 2022. Open-domain question an-swering via chain of reasoning over heterogeneousknowledge. In Findings of the Association for Com-putational Linguistics: EMNLP 2022, pages 5360-5374, Abu Dhabi, United Arab Emirates. Associationfor Computational Linguistics.Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, EricNyberg, and Jianfeng Gao. 2023. Chain-of-skills:A configurable model for open-domain question an-swering. In Proceedings of the 61st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 1599-1618, Toronto,Canada. Association for Computational Linguistics.Sewon Min, Kalpesh Krishna, Xinxi Lyu, MikeLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.FActScore: Fine-grained atomic evaluation of factualprecision in long form text generation. In Proceed-ings of the 2023 Conference on Empirical Methodsin Natural Language Processing.Sewon Min, Julian Michael, Hannaneh Hajishirzi, andLuke Zettlemoyer. 2020. AmbigQA: Answering am-biguous open-domain questions. In Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 5783-5797, Online. Association for Computational Lin-guistics.Stephen Mussmann and Stefano Ermon. 2016. Learningand inference via maximum inner product search.In International Conference on Machine Learning,pages 2587-2596. PMLR.Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-ing content selection in summarization: The pyramidmethod. In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics:HLT-NAACL 2004, pages 145-152, Boston, Mas-sachusetts, USA. Association for Computational Lin-guistics.Tri Nguyen, Mir Rosenberg, Xia Song, JianfengGao, Saurabh Tiwary, Rangan Majumder, andLi Deng. 2016. MS MARCO: A human gener-ated machine reading comprehension dataset. CoRR,abs/1611.09268.Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, GustavoHernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.Large dual encoders are generalizable retrievers. InProceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, pages9844-9855, Abu Dhabi, United Arab Emirates. As-sociation for Computational Linguistics.Barlas Oguz, Kushal Lakhotia, Anchit Gupta, PatrickLewis, Vladimir Karpukhin, Aleksandra Piktus,Xilun Chen, Sebastian Riedel, Scott Yih, SonalGupta, and Yashar Mehdad. 2022. Domain-matchedpre-training tasks for dense retrieval. In Findingsof the Association for Computational Linguistics:NAACL 2022, pages 1524-1534, Seattle, UnitedStates. Association for Computational Linguistics.OpenAI. 2023.Gpt-4 technical report.ArXiv,abs/2303.08774.Fabio Petroni, Aleksandra Piktus, Angela Fan, PatrickLewis, Majid Yazdani, Nicola De Cao, James Thorne,Yacine Jernite, Vladimir Karpukhin, Jean Maillard,Vassilis Plachouras, Tim Rocktäschel, and SebastianRiedel. 2021. KILT: a benchmark for knowledgeintensive language tasks. In Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: Human
Metadata 11: {'page': 11}
Page Content 12: Language Technologies, pages 2523-2544, Online.Association for Computational Linguistics.Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text trans-former. The Journal of Machine Learning Research,21(1):5485-5551.Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, andPercy Liang. 2016. Squad: 100,000+ questionsfor machine comprehension of text. arXiv preprintarXiv: 1606.05250.Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages3982-3992, Hong Kong, China. Association for Com-putational Linguistics.Devendra Sachan, Mostofa Patwary, MohammadShoeybi, Neel Kant, Wei Ping, William L. Hamil-ton, and Bryan Catanzaro. 2021. End-to-end trainingof neural retrievers for open-domain question answer-ing. In Proceedings of the 59th Annual Meeting of theAssociation for Computational Linguistics and the11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages6648-6662, Online. Association for ComputationalLinguistics.Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,Christopher Potts, and Matei Zaharia. 2022. Col-BERTv2: Effective and efficient retrieval vialightweight late interaction. In Proceedings of the2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 3715-3734, Seat-tle, United States. Association for ComputationalLinguistics.Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,and Danqi Chen. 2021. Simple entity-centric ques-tions challenge dense retrievers. arXiv preprintarXiv:2109.08535.Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, AnkurParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.Real-time open-domain question answering withdense-sparse phrase index. In Proceedings of the57th Annual Meeting of the Association for Computa-tional Linguistics, pages 4430-4441, Florence, Italy.Association for Computational Linguistics.Freda Shi, Xinyun Chen, Kanishka Misra, NathanScales, David Dohan, Ed H Chi, Nathanael Schärli,and Denny Zhou. 2023. Large language models canbe easily distracted by irrelevant context. In Inter-national Conference on Machine Learning, pages31210-31227. PMLR.Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-hishek Srivastava, and Iryna Gurevych. 2021. Beir:A heterogeneous benchmark for zero-shot evaluationof information retrieval models. In Thirty-fifth Con-ference on Neural Information Processing SystemsDatasets and Benchmarks Track (Round 2).Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu,Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023. Llama 2: Open foundation and fine-tuned chat models.Kexin Wang, Nandan Thakur, Nils Reimers, and IrynaGurevych. 2022. GPL: Generative pseudo labelingfor unsupervised domain adaptation of dense retrieval.In Proceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 2345-2360, Seattle, United States. Associationfor Computational Linguistics.Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md RizwanParvez, and Graham Neubig. 2023. Learning to filtercontext for retrieval-augmented generation.Ji Xin, Chenyan Xiong, Ashwin Srinivasan, AnkitaSharma, Damien Jose, and Paul Bennett. 2022. Zero-shot dense retrieval with momentum adversarial do-main invariant representations. In Findings of the As-sociation for Computational Linguistics: ACL 2022,pages 4008-4020, Dublin, Ireland. Association forComputational Linguistics.Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul Bennett, Junaid Ahmed, and ArnoldOverwijk. 2020. Approximate nearest neighbor neg-ative contrastive learning for dense text retrieval.arXiv preprint arXiv:2007.00808.Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Re-comp: Improving retrieval-augmented lms with com-pression and selective augmentation.Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,Jax Law, Noah Constant, Gustavo Hernandez Abrego,Steve Yuan, Chris Tar, Yun-Hsuan Sung, et al. 2020.
Metadata 12: {'page': 12}
Page Content 13: Multilingual universal sentence encoder for semanticretrieval. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations, pages 87-94.Wen-tau Yih, Kristina Toutanova, John C. Platt, andChristopher Meek. 2011. Learning discriminativeprojections for text similarity measures. In Proceed-ings of the Fifteenth Conference on ComputationalNatural Language Learning, pages 247-256, Port-land, Oregon, USA. Association for ComputationalLinguistics.Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,Michael Zeng, and Meng Jiang. 2023a. Generaterather than retrieve: Large language models arestrong context generators. In The Eleventh Inter-national Conference on Learning Representations.Wenhao Yu, Hongming Zhang, Xiaoman Pan, KaixinMa, Hongwei Wang, and Dong Yu. 2023b. Chain-of-note: Enhancing robustness in retrieval-augmentedlanguage models. arXiv preprint arXiv:2311.09210.Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang,and Nan Duan. 2022. Multi-view document repre-sentation learning for open-domain dense retrieval.In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 5990-6000, Dublin, Ireland.Association for Computational Linguistics.# A Retrieval Corpus ProcessingThe English Wikipedia dump used in this study,released by Bohnet et al., 2022, was selected be-cause it has been filtered to remove figures, tables,and lists, and is organized into paragraphs. Thedump dates back to October 13, 2021. We havesegmented Wikipedia into three retrieval units forthis study: 100-word passage chunks, sentences,and propositions. Paragraphs are divided into 100-word passage chunks using a greedy method. Wedivide only at the end of sentences to ensure eachpassage chunk contains complete sentences. Aswe process the paragraph, we add sentences oneby one. If including the next sentence causes thepassage chunk to exceed 100 words, we start anew passage chunk with that sentence. However,if the final passage chunk is shorter than 50 words,we merge it with the previous one to avoid overlysmall segments. Each passage is further segmentedinto sentences using the widely used Python SpaCyen\_core\_web\_lg model. Additionally, eachpassage is decomposed into propositions by ourPropositionizer model. We decomposed 6 millionpages into 41 million passages, 114 million sen-tences, and 257 million propositions. On average,a passage contains 6.3 propositions, and a sentencecontains 2.3 propositions.# B Training the PropositionizerWe generated a list of propositions from a givenparagraph using GPT-4 with a prompt, as shown inFigure 8. After filtering, 42,857 pairs were used tofine-tune a Flan-T5-Large model. We named themodel Propositionizer. The AdamW optimizer wasused with a batch size of 64, learning rate of 1e-4,weight decay of 1e-4, and 3 epochs.To compare the proposition generation perfor-mance of different models, we set up a developmentset and an evaluation metric. The development setcontains an additional 1,000 pairs collected by GPT-4 using the same approach as the training set. Weevaluated the quality of the predicted propositionsby the F1 score of two sets of propositions. Mo-tivated by the F1 score of two sets of tokens inBertScore, we designed the F1 score for two sets ofpropositions. Let P = {p1, ... , Pn } denote the setof labeled propositions and P = {p1, ... , pm} theset of predicted propositions. We use sim(pi, pj)to represent the similarity between two proposi-tions. Theoretically, any text similarity metric canbe used. We chose BertScore with roberta-large
Metadata 13: {'page': 13}
Page Content 14: configuration as our sim function since we wantedour metric to reflect the semantic difference be-tween propositions. We defineRecall =P1PiEPP¡EPmax sim (pi, Pj)Precision =1max sim (pi, pj)PiEPPrecision . RecallF1 =2.Precision + RecallHere is a figurative explanation of the F1 score:Recall represents the percentage of propositionsin the labeled set that are similar to those in thegenerated set, Precision represents the percentageof propositions in the generated set that are similarto the labeled set, and F1 is the harmonic mean ofRecall and Precision. F1 is 1 if the two sets areexactly the same, and 0 if any two propositions aresemantically different.We conducted a comparative analysis of base-size and large-size Flan-T5 models, which weretrained using varying amounts of data (shown inFigure 5). Our findings suggest that larger models,coupled with extensive training data, yield betterresults. The Propositionizer presented in this paperattained an F1 score of 0.822. Upon manuallyreviewing the generated propositions, we foundthem to be satisfactory.<!-- FigureContent="\-- flan-t5-base-0- flan-t5-large818079F178777650007500 10000 12500 15000 17500Number of training samples" -->Figure 5: Performance of proposition-level decompo-sition by models with different sizes and number oftraining data.# C Offline IndexingWe used the pyserini and faiss packagesto encode retrieval units into embeddings. Weexploited multiple GPUs to encode each textunit in groups of 1M units with a batch sizeof 64. After preprocessing the embeddings,we used an exact search for the inner product(faiss . IndexFlat IP) in all experiments. Theplain index of FACTOIDWIKIis approximately768GB in size. To reduce memory pressure, theembeddings are split into 8 shards. An approximatenearest neighbor search is conducted per shard be-fore aggregating all results.Although the number of propositions is six timesthat of passages, using efficient indexing tech-niques can enable sub-linear search times relativeto the total count of vectors. Moreover, utilizingGPU parallelism and distributed indexes signifi-cantly decreases the online search time. As a result,with proper implementation, we can make propo-sition retrieval a practically viable and efficientoption.# D Retrievers ModelsWe used transformers and sentence-transformers packages for the model implementa-tion. We used the following checkpoints releasedon HuggingFace: SimCSE (princeton-nlp/unsup-simcse-bert-base-uncased), Con-triever (facebook/contriever), DPR (facebook/dpr-ctx\_encoder-multiset-base, facebook/dpr-question\_encoder-multiset-base), ANCE (castorini/ance-dpr-context-multi, castorini/ance-dpr-question-multi, ), TAS-B (sentence-transformers/msmarco-distilbert-base-tas-b), and GTR (sentence-transformers/gtr-t5-base).# E Additional ResultsIn Section 5.2, we demonstrated the advantageof retrieval by proposition over retrieval by sen-tence, particularly as the population of the entitydecreases in EQ. We used the occurrence in thetop-1000 paragraphs retrieved by BM25 as a proxyfor popularity, rather than counting the number ofhyperlinks to the entity used in Sciavolino et al.,2021. Therefore, the trend in the performance ver-sus popularity plot shows some differences (Fig-ure 6) between our results and those in Sciavolinoet al., 2021. For example, some entities are am-biguous (e.g., 1992, a TV series). In such cases,the occurrence of the surface form of the entity islarge. Simultaneously, questions related to ambigu-ous entities are challenging to answer, leading tolower recall.In Section 6.2, we discussed the recall of an-swers in the retrieved text with respect to the con-
Metadata 14: {'page': 14}
Page Content 15: text length. We further illustrate the performancetrends of six dense retrievers, as detailed in Fig-ure 7. The results indicate that the recall rate ofpropositions consistently outperforms that of sen-tences and passages. Our findings lead to the con-clusion that question-related density is greater inproposition units compared to sentences and pas-sages.
Metadata 15: {'page': 15}
Page Content 16: <!-- FigureContent="\-- Passage-- Sentence-\*- PropositionSimCSEContrieverDPRANCETAS-BGTR60Recall@560Recall@550Recall@560Recall@570Recall@5Recall@580404060804070305020.701011021010ª1011021011021011021010ªPopularityPopularityPopularityPopularityPopularityPopularity(a) Where was [X] born?-\*- Passage-- Sentence-- PropositionSimCSEContrieverDPRANCETAS-BGTR608090Recall@5Recall@560Recall@5Recall@5Le.80Recall@5Recall@5904060804080206020707040101102101102101102101102101102101102PopularityPopularityPopularityPopularityPopularityPopularity(b) Who was [X] created by?" -->Figure 6: Document retrieval recall vs. the popularity of the target entity in each question from the EntityQuestionsdataset. We display the performance of two relations.
Metadata 16: {'page': 16}
Page Content 17: <!-- FigureContent="PassageSentencePropositionSimCSE / NQSimCSE / TQASimCSE / WebQSimCSE / SQUADSimCSE / EQ4050Recall (%)Recall (%)504050Recall (%)Recall (%)Recall (%)30404030402030: 30302002004000200400020040002004000200400#Words#Words#Words#Words#Words-PassageSentence-PropositionContriever / NQContriever / TQAContriever / WebQContriever / SQUADContriever / EQ7050Recall (%)50Recall (%)Recall (%)50Recall (%)50Recall (%)604040404030503030302040202002004000200400020040002004000200400#Words#Words#Words#Words#WordsPassageSentencePropositionDPR / NQDPR / TQADPR / WebQDPR / SQUADDPR / EQ7070Recall (%)Recall (%)70Recall (%)Recall (%)Recall (%)6060604050506050304002004000200400020040002004000200400#Words#Words#Words#Words#Words-PassageSentence-PropositionANCE / NQANCE / TQAANCE / WebQANCE / SQUADANCE / EQ70707050Recall (%)Recall (%)Recall (%)Recall (%)Recall (%)7060604060506050305002004000200400020040002004000200400#Words#Words#Words#Words#Words-PassageSentence-PropositionTAS-B / NQTAS-B / TQATAS-B / WebQTAS-B / SQUADTAS-B / EQ707080Recall (%)706060Recall (%)Recall (%)Recall (%)Recall (%)60705050605040406002004000200400020040002004000200400#Words#Words#Words#Words#Words-PassageSentencePropositionGTR / NQGTR / TQAGTR / WebQGTR / SQUADGTR / EQ70Recall (%)7080Recall (%)Recall (%)70Recall (%)60Recall (%)60607050605050406002004000200400020040002004000200400#Words#Words#Words#Words#Words" -->Figure 7: Recall of the gold answer in the retrieved text limited to first k words. Finer-grained retrieval has a higherrecall across all numbers of words.
Metadata 17: {'page': 17}
Page Content 18: # Passage = PropositionsDecompose the "Content" into clear and simple propositions, ensuring they are interpretable out ofcontext.1\. Split compound sentence into simple sentences. Maintain the original phrasing from the inputwhenever possible.2\. For any named entity that is accompanied by additional descriptive information, separate thisinformation into its own distinct proposition.3\. Decontextualize the proposition by adding necessary modifier to nouns or entire sentencesand replacing pronouns (e.g., "it", "he", "she", "they", "this", "that") with the full name of theentities they refer to.4\. Present the results as a list of strings, formatted in JSON.Input: Title: Eostre. Section: Theories and interpretations, Connection to Easter Hares. Content:The earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown inother parts of Germany until the 18th century. Scholar Richard Sermon writes that "hares werefrequently seen in gardens in spring, and thus may have served as a convenient explanation for theorigin of the colored eggs hidden there for children. Alternatively, there is a European traditionthat hares laid eggs, since a hare's scratch or form and a lapwing's nest look very similar, andboth occur on grassland and are first seen in the spring. In the nineteenth century the influenceof Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.German immigrants then exported the custom to Britain and America where it evolved into theEaster Bunny."Output: [ "The earliest evidence for the Easter Hare was recorded in south-west Germany in1678 by Georg Franck von Franckenau.", "Georg Franck von Franckenau was a professor ofmedicine.", "The evidence for the Easter Hare remained unknown in other parts of Germany untilthe 18th century.", "Richard Sermon was a scholar.", "Richard Sermon writes a hypothesis aboutthe possible explanation for the connection between hares and the tradition during Easter", "Hareswere frequently seen in gardens in spring.", "Hares may have served as a convenient explanationfor the origin of the colored eggs hidden in gardens for children.", "There is a European traditionthat hares laid eggs.", "A hare's scratch or form and a lapwing's nest look very similar.", "Bothhares and lapwing's nests occur on grassland and are first seen in the spring.", "In the nineteenthcentury the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popularthroughout Europe.", "German immigrants exported the custom of the Easter Hare/Rabbit toBritain and America.", "The custom of the Easter Hare/Rabbit evolved into the Easter Bunny inBritain and America." ]Input: < a new passage>Output:<!-- PageFooter="Figure 8: Prompt for generating propositions from a passage using GPT-4." -->
Metadata 18: {'page': 18}